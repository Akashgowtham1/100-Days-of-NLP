{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "General Utterance Generation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOaDb2k71vm7K/e2jg/eKMs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/100-Days-of-NLP/blob/applications%2Fgeneration/applications/generation/utterance_generation/General%20Utterance%20Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQyXEMiSqGDk",
        "colab_type": "text"
      },
      "source": [
        "# Utterance Generation\n",
        "\n",
        "Utterance generation is an important problem in NLP, especially in question answering, information retrieval, information extraction, conversation systems, to name a few. It could also be used to create synthentic training data for many NLP problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zr5lRx9XJTQQ",
        "colab_type": "text"
      },
      "source": [
        "## MS COCO Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAwkaojCJQbf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6cba88e7-bc82-48b3-ed12-ebe2fff04775"
      },
      "source": [
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-28 15:41:36--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.10.35\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.10.35|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2017.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  32.5MB/s    in 8.1s    \n",
            "\n",
            "2020-06-28 15:41:45 (29.8 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS_-F8HGJaVO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9ec52680-9804-46b1-9c8a-b6d7cd55a9a8"
      },
      "source": [
        "!unzip annotations_trainval2017.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  annotations_trainval2017.zip\n",
            "  inflating: annotations/instances_train2017.json  \n",
            "  inflating: annotations/instances_val2017.json  \n",
            "  inflating: annotations/captions_train2017.json  \n",
            "  inflating: annotations/captions_val2017.json  \n",
            "  inflating: annotations/person_keypoints_train2017.json  \n",
            "  inflating: annotations/person_keypoints_val2017.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRrL88vT1lYI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "f95fc56c-ac33-4ae7-bdd7-150343f3c954"
      },
      "source": [
        "!ls -lah annotations"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 796M\n",
            "drwxr-xr-x 2 root root 4.0K Jun 28 15:41 .\n",
            "drwxr-xr-x 1 root root 4.0K Jun 28 15:41 ..\n",
            "-rw-rw-r-- 1 root root  88M Sep  1  2017 captions_train2017.json\n",
            "-rw-rw-r-- 1 root root 3.7M Sep  1  2017 captions_val2017.json\n",
            "-rw-rw-r-- 1 root root 449M Sep  1  2017 instances_train2017.json\n",
            "-rw-rw-r-- 1 root root  20M Sep  1  2017 instances_val2017.json\n",
            "-rw-rw-r-- 1 root root 228M Sep  1  2017 person_keypoints_train2017.json\n",
            "-rw-rw-r-- 1 root root 9.6M Sep  1  2017 person_keypoints_val2017.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDJYtQupLE0V",
        "colab_type": "text"
      },
      "source": [
        "## Required Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKsk0jqW74Ex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "051a6851-b813-43c5-d3d7-540a1d3436c6"
      },
      "source": [
        "!pip install youtokentome"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting youtokentome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/4a86cf99da3f680497ae132329025b291e2fda22327e8da6a9476e51acb1/youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\r\u001b[K     |▏                               | 10kB 19.4MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 5.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 6.8MB/s eta 0:00:01\r\u001b[K     |▊                               | 40kB 7.7MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 6.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61kB 7.4MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71kB 7.8MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81kB 8.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 92kB 7.5MB/s eta 0:00:01\r\u001b[K     |██                              | 102kB 7.7MB/s eta 0:00:01\r\u001b[K     |██                              | 112kB 7.7MB/s eta 0:00:01\r\u001b[K     |██▎                             | 122kB 7.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 133kB 7.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143kB 7.7MB/s eta 0:00:01\r\u001b[K     |██▉                             | 153kB 7.7MB/s eta 0:00:01\r\u001b[K     |███                             | 163kB 7.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 174kB 7.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 184kB 7.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 194kB 7.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 204kB 7.7MB/s eta 0:00:01\r\u001b[K     |████                            | 215kB 7.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 225kB 7.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 235kB 7.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 245kB 7.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 256kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 266kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 276kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 286kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 296kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 307kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 317kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 327kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 337kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 348kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 358kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 368kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 378kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 389kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 399kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 409kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 419kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 430kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 440kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 450kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 460kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 471kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 481kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 491kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 501kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 512kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 522kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 532kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 542kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 552kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 563kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 573kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 583kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 593kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 604kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 614kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 624kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 634kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 645kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 655kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 665kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 675kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 686kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 696kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 706kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 716kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 727kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 737kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 747kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 757kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 768kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 778kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 788kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 798kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 808kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 819kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 829kB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 839kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 849kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 860kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 870kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 880kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 890kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 901kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 911kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 921kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 931kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 942kB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 952kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 962kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 972kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 983kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 993kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.0MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.0MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.0MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.0MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.1MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.2MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.2MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.2MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.2MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.3MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.3MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.3MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.4MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.4MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.4MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.4MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.4MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.4MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.4MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.4MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.5MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.5MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.5MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.5MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.5MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.5MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.5MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.5MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.6MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6MB 7.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.6MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.6MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.6MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.6MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.6MB 7.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.6MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.7MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.7MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.7MB 7.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.7MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7MB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7MB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome) (7.1.2)\n",
            "Installing collected packages: youtokentome\n",
            "Successfully installed youtokentome-1.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlgec3YXLOvW",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCAkehv1btSs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "96bc818e-fcb4-42c0-d647-5fbb20bbb3b7"
      },
      "source": [
        "import json\n",
        "import time\n",
        "import codecs\n",
        "import random\n",
        "import math\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import youtokentome\n",
        "from torchtext import data, vocab\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import seaborn as sns"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IsArf_BcbJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMt1wbI3drfi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ee8f1f9-a02e-4b78-f9d4-5ff5cd648e5f"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuD5_N2tLThp",
        "colab_type": "text"
      },
      "source": [
        "## Data Exploration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsnh3e8wPixa",
        "colab_type": "text"
      },
      "source": [
        "MS COCO dataset contains various types of annotations designed for different tasks. Since we only need utterances which are similar to each other, we consider only caption files. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVpHjrjZQAp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "caption_files = ['annotations/captions_train2017.json', 'annotations/captions_val2017.json']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxXVw3VrRBlI",
        "colab_type": "text"
      },
      "source": [
        "Let's group the captions by `image_id`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzjrgzmjQgms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "captions = {}\n",
        "\n",
        "for each_file in caption_files:\n",
        "    with open(each_file, 'r') as f:\n",
        "        content = json.load(f)\n",
        "\n",
        "    for i in content['annotations']:\n",
        "        if i['image_id'] in captions:\n",
        "            captions[i['image_id']].append(i['caption'])\n",
        "        else:\n",
        "            captions[i['image_id']] = [i['caption']]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBfBiX9qRJi7",
        "colab_type": "text"
      },
      "source": [
        "Sample check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr1EGjemQs-i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "34a0048f-2cc3-4055-fa6d-fc8a55de07bd"
      },
      "source": [
        "captions[203564]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A bicycle replica with a clock as the front wheel.',\n",
              " 'The bike has a clock as a tire.',\n",
              " 'A black metal bicycle with a clock inside the front wheel.',\n",
              " 'A bicycle figurine in which the front wheel is replaced with a clock\\n',\n",
              " 'A clock with the appearance of the wheel of a bicycle ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-09Kf0ERMND",
        "colab_type": "text"
      },
      "source": [
        "Since each image has 5 captions, we randomly drop 1 out of 5 and create 2 (sentence, utterance) pairs out of the remaining 4 captions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VyC7vnpSWGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coco_data = []\n",
        "\n",
        "for _, caption in captions.items():\n",
        "    \n",
        "    if len(caption) == 5:\n",
        "        # randomly pick an id\n",
        "        random_id = random.choice(range(len(caption)))\n",
        "        \n",
        "        # remove the caption corresponding to that id\n",
        "        _ = caption.pop(random_id)\n",
        "    if len(caption) < 4 and len(caption) > 2:\n",
        "        coco_data.append((caption[0], caption[1]))\n",
        "        continue\n",
        "\n",
        "    # add the remaining four captions as (sentence, utterance) pairs\n",
        "    coco_data.append((caption[0], caption[1]))\n",
        "    coco_data.append((caption[2], caption[3]))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EF-E9VRFX_wC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d4917eba-db98-4576-89cc-ea055e7bb49e"
      },
      "source": [
        "len(coco_data)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "246574"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgAd8tcmaGuQ",
        "colab_type": "text"
      },
      "source": [
        "Let's use `90%` of the data to train the model, `5%` as validation data and `5%` as test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62oNUxItiOD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_length = len(coco_data)\n",
        "train_data_size = int(0.9 * total_length)\n",
        "valid_data_size = int(0.05 * total_length)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCQDcju1Ztz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = coco_data[:train_data_size]\n",
        "valid_data = coco_data[train_data_size:train_data_size + valid_data_size]\n",
        "test_data = coco_data[train_data_size + valid_data_size:]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yi-B7ntaQyc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b787d0b-40aa-42de-fac2-8ba36989fe20"
      },
      "source": [
        "len(train_data), len(valid_data), len(test_data)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(221916, 12328, 12330)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTCp9BeZbCOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.DataFrame(train_data, columns=['sentence', 'utterance'])\n",
        "valid_df = pd.DataFrame(valid_data, columns=['sentence', 'utterance'])\n",
        "test_df = pd.DataFrame(test_data, columns=['sentence', 'utterance'])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XDT3XWHoCZf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "cf5329ec-77b8-4360-f723-d4922afb3f29"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>utterance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The bike has a clock as a tire.</td>\n",
              "      <td>A black metal bicycle with a clock inside the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A bicycle figurine in which the front wheel is...</td>\n",
              "      <td>A clock with the appearance of the wheel of a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Blue and white color scheme in a small bathroom.</td>\n",
              "      <td>This is a blue and white bathroom with a wall ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A blue boat themed bathroom with a life preser...</td>\n",
              "      <td>A bathroom with walls that are painted baby blue.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A car that seems to be parked illegally behind...</td>\n",
              "      <td>two cars parked on the sidewalk on the street</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence                                          utterance\n",
              "0                    The bike has a clock as a tire.  A black metal bicycle with a clock inside the ...\n",
              "1  A bicycle figurine in which the front wheel is...  A clock with the appearance of the wheel of a ...\n",
              "2   Blue and white color scheme in a small bathroom.  This is a blue and white bathroom with a wall ...\n",
              "3  A blue boat themed bathroom with a life preser...  A bathroom with walls that are painted baby blue.\n",
              "4  A car that seems to be parked illegally behind...      two cars parked on the sidewalk on the street"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCWxVukgbeDs",
        "colab_type": "text"
      },
      "source": [
        "Save the data into files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iph-nS94iP95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.to_csv('train_ds.csv')\n",
        "valid_df.to_csv('valid_ds.csv')\n",
        "test_df.to_csv('test_ds.csv')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe1SsyEniR9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "5b9bc2aa-bb98-45f2-c08d-6e650be399fb"
      },
      "source": [
        "!ls -lah"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 268M\n",
            "drwxr-xr-x 1 root root 4.0K Jun 28 15:44 .\n",
            "drwxr-xr-x 1 root root 4.0K Jun 28 15:40 ..\n",
            "drwxr-xr-x 2 root root 4.0K Jun 28 15:41 annotations\n",
            "-rw-r--r-- 1 root root 242M Jul 10  2018 annotations_trainval2017.zip\n",
            "drwxr-xr-x 1 root root 4.0K Jun 25 17:02 .config\n",
            "drwxr-xr-x 1 root root 4.0K Jun 17 16:18 sample_data\n",
            "-rw-r--r-- 1 root root 1.4M Jun 28 15:44 test_ds.csv\n",
            "-rw-r--r-- 1 root root  25M Jun 28 15:44 train_ds.csv\n",
            "-rw-r--r-- 1 root root 1.4M Jun 28 15:44 valid_ds.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq7Bwq8OblUR",
        "colab_type": "text"
      },
      "source": [
        "Create a file which contains all the training data, so that it can be used for training the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNkx98nRLeH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using only training data to train the BPE tokenizer\n",
        "all_data = list(train_df['sentence'].str.lower().values) + list(train_df['utterance'].str.lower().values)\n",
        "\n",
        "with codecs.open(\"all_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(all_data))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mPUkWmEMoAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# free some ram\n",
        "del all_data"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5VMifpeb1MQ",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDWJspRPb9ic",
        "colab_type": "text"
      },
      "source": [
        "Using the `youtokentome` library to train the BPE (Byte-Pair Encoding) tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qE-ivyaMDS_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "199dcde4-72e8-4b5a-e3f0-d6593cb61f4d"
      },
      "source": [
        "# Perform BPE\n",
        "print(\"\\nLearning BPE...\")\n",
        "youtokentome.BPE.train(data=\"all_data.txt\", vocab_size=20000, model=\"bpe.model\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Learning BPE...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<youtokentome.youtokentome.BPE at 0x7f3527e3e5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Oh-1x80VDsA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "cb0fdfc8-b86c-4276-ec84-b8fafdfb8af6"
      },
      "source": [
        "!ls -lah"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 291M\n",
            "drwxr-xr-x 1 root root 4.0K Jun 28 15:44 .\n",
            "drwxr-xr-x 1 root root 4.0K Jun 28 15:40 ..\n",
            "-rw-r--r-- 1 root root  23M Jun 28 15:44 all_data.txt\n",
            "drwxr-xr-x 2 root root 4.0K Jun 28 15:41 annotations\n",
            "-rw-r--r-- 1 root root 242M Jul 10  2018 annotations_trainval2017.zip\n",
            "-rw-r--r-- 1 root root 271K Jun 28 15:44 bpe.model\n",
            "drwxr-xr-x 1 root root 4.0K Jun 25 17:02 .config\n",
            "drwxr-xr-x 1 root root 4.0K Jun 17 16:18 sample_data\n",
            "-rw-r--r-- 1 root root 1.4M Jun 28 15:44 test_ds.csv\n",
            "-rw-r--r-- 1 root root  25M Jun 28 15:44 train_ds.csv\n",
            "-rw-r--r-- 1 root root 1.4M Jun 28 15:44 valid_ds.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXbbGZFAcIw6",
        "colab_type": "text"
      },
      "source": [
        "Load the trained `BPE` tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SYi5Pv1MXLo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "91bda952-0073-4a01-e7d1-859dd352bed7"
      },
      "source": [
        "# Load BPE model\n",
        "print(\"\\nLoading BPE model...\")\n",
        "bpe_model = youtokentome.BPE(model=\"bpe.model\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading BPE model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtqgyN4_N_W8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c70cf837-42b2-4a5b-fefa-c63e1aba2096"
      },
      "source": [
        "# Special Tokens\n",
        "print(f\"<BOS>: {bpe_model.subword_to_id('<BOS>')}\")    # Begining of the sentence token\n",
        "print(f\"<EOS>: {bpe_model.subword_to_id('<EOS>')}\")    # End of the sentence token\n",
        "print(f\"<UNK>: {bpe_model.subword_to_id('<UNK>')}\")    # Unknown token\n",
        "print(f\"<PAD>: {bpe_model.subword_to_id('<PAD>')}\")    # Pad token"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BOS>: 2\n",
            "<EOS>: 3\n",
            "<UNK>: 1\n",
            "<PAD>: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoafiSZrjDxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_index = bpe_model.subword_to_id('<PAD>')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEYH9D2AUYft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "973dc6c1-46c9-4f06-f9f9-aac292d70718"
      },
      "source": [
        "sentence = \"This is a sample sentence\"\n",
        "encoded_ids = bpe_model.encode(sentence.lower(), output_type=youtokentome.OutputType.ID, bos=True, eos=True)\n",
        "encoded_text = bpe_model.encode(sentence.lower(), output_type=youtokentome.OutputType.SUBWORD, bos=True, eos=True)\n",
        "decoded_text = bpe_model.decode(encoded_ids, ignore_ids=[2, 3])\n",
        "\n",
        "print(encoded_ids)\n",
        "print(encoded_text)\n",
        "print(decoded_text)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 547, 112, 67, 10274, 70, 416, 9110, 3]\n",
            "['<BOS>', '▁this', '▁is', '▁a', '▁sample', '▁s', 'ent', 'ence', '<EOS>']\n",
            "['this is a sample sentence']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExVJzF0APBo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a tokenizer method which takes in a sentence and returns ids\n",
        "# by defining this, we can configure the tokenizer to torchtext Field\n",
        "def bpe_tokenizer(sentence):\n",
        "    encoded_ids = bpe_model.encode(sentence.lower(), output_type=youtokentome.OutputType.ID, bos=True, eos=True)\n",
        "    return encoded_ids"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE6AfGCrRyMU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ecc03f0f-3c00-4ce6-ca9e-846fce3f0334"
      },
      "source": [
        "bpe_tokenizer(\"This is a sample sentence\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 547, 112, 67, 10274, 70, 416, 9110, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fN60xqHdgBv",
        "colab_type": "text"
      },
      "source": [
        "## Read the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuB53_DbiTPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = data.get_tokenizer(bpe_tokenizer)\n",
        "TEXT = data.Field(tokenize=tokenizer, batch_first=True, use_vocab=False, pad_token=pad_index)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RGUVl9ReSuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fields = [(None, None), (\"source\", TEXT), (\"target\", TEXT)]\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = data.TabularDataset.splits(path='.',\n",
        "                                     train='train_ds.csv', validation='valid_ds.csv', test='test_ds.csv',\n",
        "                                     format='csv', skip_header=True, fields=fields)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ6NELb0ibX4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5e63d149-cb7b-4ac6-c3fc-2ed394127802"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation examples: {len(valid_dataset)}\")\n",
        "print(f\"Number of testing examples: {len(test_dataset)}\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 221916\n",
            "Number of validation examples: 12328\n",
            "Number of testing examples: 12330\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nFRDyZsic9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0fc73bfb-7cb8-4c9e-aaf9-514e7f1b570c"
      },
      "source": [
        "print(vars(train_dataset.examples[0]))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'source': [2, 82, 816, 318, 67, 528, 453, 67, 6360, 3], 'target': [2, 67, 257, 1036, 850, 95, 67, 528, 638, 82, 264, 5596, 3]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzOFppxuWY3s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b048b2b9-6126-48ba-c84b-77cb5444bcf9"
      },
      "source": [
        "# Building vocabulary is not required as the BPE tokenizer already convert the sentence into ids\n",
        "\n",
        "# let's check the vocab size\n",
        "print(f\"Vocab size: {bpe_model.vocab_size()}\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahRmAn_fdmOP",
        "colab_type": "text"
      },
      "source": [
        "## Data Iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNpR4NNtejTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_dataset, valid_dataset, test_dataset),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_key=lambda x: len(x.source),\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZYV_W_igUTt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4fb645c-88d7-4c94-a82c-31c43732db8b"
      },
      "source": [
        "temp = next(iter(train_iterator))\n",
        "temp.source.shape, temp.target.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 26]), torch.Size([64, 21]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRey1FMXdsUR",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1thdYY7rQFr",
        "colab_type": "text"
      },
      "source": [
        "The model we will be using of type Sequence-to-Sequence, which takes in a sequence(sentence) and outputs a sequence(utterance).\n",
        "\n",
        "![seq-to-seq](https://drive.google.com/uc?id=1TkFl1a68iOCfRaW6QGRBs5Jf4SHLqk9j)\n",
        "\n",
        "In particular, we will be using Transformer Model.\n",
        "\n",
        "![transformer](https://drive.google.com/uc?id=1Bg_PrLjFXmmfXqktCSJI_UisyH121oqS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ctkLmztsnWe",
        "colab_type": "text"
      },
      "source": [
        "### Multi-Head Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeJnl6cue6rJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout, pad_idx, device):\n",
        "        super().__init__()\n",
        "\n",
        "        assert d_model % n_heads == 0, \"n_heads must be a factor of d_model\"\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "\n",
        "        self.q = nn.Linear(d_model, d_model)\n",
        "        self.k = nn.Linear(d_model, d_model)\n",
        "        self.v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.pad_idx = pad_idx\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # query => [batch_size, seq_len, d_model] \n",
        "        # key => [batch_size, seq_len, d_model]\n",
        "        # value => [batch_size, seq_len, d_model]\n",
        "\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        Q = self.q(query)\n",
        "        K = self.k(key)\n",
        "        V = self.v(value)\n",
        "        # Q, K, V => [batch_size, seq_len, d_model]\n",
        "\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # Q, K, V => [batch_size, n_heads, seq_len, head_dim]\n",
        "\n",
        "        energy = torch.matmul(Q, K.permute(0 ,1, 3, 2))\n",
        "        energy = energy / self.scale\n",
        "        # energy => [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "        # attention => [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        weighted = torch.matmul(attention, V)\n",
        "        # weighted => [batch_size, n_heads, query_len, head_dim]\n",
        "\n",
        "        weighted = weighted.permute(0, 2, 1, 3).contiguous()\n",
        "        # weighted => [batch_size, query_len, n_heads, head_dim]\n",
        "\n",
        "        x = weighted.view(batch_size, -1, self.d_model)\n",
        "        # x => [batch_size, query_len, d_model]\n",
        "\n",
        "        x = self.fc(x)\n",
        "        # x => [batch_size, query_len, d_model]\n",
        "        # attention => [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        return x, attention\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbpeeOJksrLv",
        "colab_type": "text"
      },
      "source": [
        "### Feed Forward "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vz1NP3akIUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, pff_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(d_model, pff_dim)\n",
        "        self.fc2 = nn.Linear(pff_dim, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        # input => [batch_size, seq_len, d_model]\n",
        "\n",
        "        x = self.dropout(torch.relu(self.fc1(input)))\n",
        "        # x => [batch_size, seq_len, pff_dim]\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        # x => [batch_size, seq_len, d_model]\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybU9LOxpsvZV",
        "colab_type": "text"
      },
      "source": [
        "### Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vfRvJLgkMis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, pff_dim, dropout, pad_idx, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attention = SelfAttention(d_model, n_heads, dropout, pad_idx, device)\n",
        "        self.pff = PositionWiseFeedForward(d_model, pff_dim, dropout)\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.pff_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        # src => [batch_size, src_len, d_model]\n",
        "\n",
        "        # self attention on src\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        # _src => [batch_size, src_len, d_model]\n",
        "\n",
        "        # residual connection and layer normalization\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        # src => [batch_size, src_len, d_model]\n",
        "\n",
        "        # position wise feed forward\n",
        "        _src = self.pff(src)\n",
        "        # _src => [batch_size, src_len, d_model]\n",
        "\n",
        "        # residual connection and layer normalization\n",
        "        src = self.pff_layer_norm(src + self.dropout(_src))\n",
        "        # src => [batch_size, src_len, d_model]\n",
        "\n",
        "        return src"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgivyB7Qs0J4",
        "colab_type": "text"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApyJtPvdkQCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, n_layers, n_heads, pff_dim, dropout, pad_idx, device, max_len=500):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(input_dim, d_model)\n",
        "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, pff_dim, dropout, pad_idx, device) for _ in range(n_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
        "    \n",
        "    def forward(self, src, src_mask=None):\n",
        "        # src => [batch_size, src_len]\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        # pos => [batch_size, src_len]\n",
        "\n",
        "        word_embed = self.word_embedding(src)\n",
        "        word_embed = word_embed * self.scale\n",
        "        # word_embed => [batch_size, src_len, d_model]\n",
        "\n",
        "        pos_embed = self.pos_embedding(pos)\n",
        "        # pos_embed => [batch_size, src_len, d_model]\n",
        "\n",
        "        src = self.dropout(word_embed + pos_embed)\n",
        "        # src => [batch_size, src_len, d_model]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        \n",
        "        # src => [batch_size, src_len, d_model]\n",
        "        return src\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTJ6UGG1s2xL",
        "colab_type": "text"
      },
      "source": [
        "### Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJc6NfMDkWpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, pff_dim, dropout, pad_idx, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attention = SelfAttention(d_model, n_heads, dropout, pad_idx, device)\n",
        "        self.enc_attention = SelfAttention(d_model, n_heads, dropout, pad_idx, device)\n",
        "        self.pff = PositionWiseFeedForward(d_model, pff_dim, dropout)\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.pff_layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        # self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        # _trg => [batch_size, trg_len, d_model]\n",
        "\n",
        "        # residual connection and layer normalization\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "        # trg => [batch_size, trg_len, d_model]\n",
        "\n",
        "        # enc_attention\n",
        "        _trg, attention = self.enc_attention(trg, enc_src, enc_src, src_mask)\n",
        "        # _trg => [batch_size, trg_len, d_model]\n",
        "        # attention => [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "        # residual connection and layer normalization\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "        # trg => [batch_size, trg_len, d_model]\n",
        "\n",
        "        # positionwise feed forward\n",
        "        _trg = self.pff(trg)\n",
        "        # _trg => [batch_size, trg_len, d_model]\n",
        "\n",
        "        # residual connection and layer normalization\n",
        "        trg = self.pff_layer_norm(trg + self.dropout(_trg))\n",
        "        # trg => [batch_size, trg_len, d_model]\n",
        "\n",
        "        return trg, attention\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsqK0kmgs58m",
        "colab_type": "text"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3w0Ui47ka8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, d_model, n_layers, n_heads, pff_dim, dropout, pad_idx, device, max_len=500):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(output_dim, d_model)\n",
        "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, pff_dim, dropout, pad_idx, device) for _ in range(n_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, output_dim)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
        "    \n",
        "    def forward(self, trg, enc_src, trg_mask=None, src_mask=None):\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        # pos => [batch_size, trg_len]\n",
        "\n",
        "        word_embedding = self.word_embedding(trg)\n",
        "        word_embedding = word_embedding * self.scale\n",
        "        # word_embedding => [batch_size, trg_len, d_model]\n",
        "\n",
        "        pos_embedding = self.pos_embedding(pos)\n",
        "        # pos_embedding => [batch_size, trg_len, d_model]\n",
        "\n",
        "        trg = self.dropout(word_embedding + pos_embedding)\n",
        "        # trg => [batch_size, trg_len, d_model]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        logits = self.fc_out(trg)\n",
        "        # logits => [batch_size, trg_len, output_dim]\n",
        "        # attention => [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "        return logits, attention\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNvFDrjxs8V6",
        "colab_type": "text"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2di5Xr1keFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, pad_idx, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.pad_idx = pad_idx\n",
        "        self.device = device\n",
        "    \n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2).to(self.device)\n",
        "        # src_mask => [batch_size, 1, 1, src_len]\n",
        "        \n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        trg_pad_mask = (trg != self.pad_idx).unsqueeze(1).unsqueeze(2).to(self.device)\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
        "\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        # trg_mask => [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "        return trg_mask\n",
        " \n",
        "    def forward(self, src, trg):\n",
        "        # src => [batch_size, src_len]\n",
        "        # trg => [batch_size, trg_len]\n",
        "\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        return output, attention"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubb6qw1ls_M8",
        "colab_type": "text"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClzGYGQVoV6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD_IDX = pad_index\n",
        "INPUT_DIM = bpe_model.vocab_size()\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.3\n",
        "DEC_DROPOUT = 0.3\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT,\n",
        "              PAD_IDX,\n",
        "              device)\n",
        "\n",
        "dec = Decoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              PAD_IDX,\n",
        "              device)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lbnRuYrodmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Transformer(enc, dec, PAD_IDX, device).to(device)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ap9CEFjtFE4",
        "colab_type": "text"
      },
      "source": [
        "### Initialize weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1RdG5lhogrl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e3be755b-c419-41a6-cd45-b7ef74e28c0f"
      },
      "source": [
        "def init_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (word_embedding): Embedding(20000, 256)\n",
              "    (pos_embedding): Embedding(500, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (pff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (pff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (pff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (pff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (pff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (pff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (word_embedding): Embedding(20000, 256)\n",
              "    (pos_embedding): Embedding(500, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (enc_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (pff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (pff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (enc_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (pff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (pff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (enc_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (pff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (pff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "    (fc_out): Linear(in_features=256, out_features=20000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPZoHaR8oeH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83251937-1ce9-48a2-ea84-cd464c4d9659"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 19,589,664 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJxy6qOBtK9Z",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer & Criterion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEVpPmmbopYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REt8C1RotRXf",
        "colab_type": "text"
      },
      "source": [
        "### Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPuPSJwOotC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, criterion, optimizer, clip):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.source\n",
        "        trg = batch.target\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output, _ = model(src, trg[:, :-1])\n",
        "        # output => [batch_size, trg_len - 1, output_dim]\n",
        "        # trg => [batch_size, trg_len]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    return epoch_loss / len(iterator)\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30EMhGD9tXWw",
        "colab_type": "text"
      },
      "source": [
        "### Validation Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y8YEYgN_H1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.source\n",
        "            trg = batch.target\n",
        "\n",
        "            output, _ = model(src, trg[:, :-1])\n",
        "            # output => [batch_size, trg_len - 1, output_dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZzKy8VuAAD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = elapsed_time - (elapsed_mins * 60)\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lKCZrymtcVg",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKYvMZdxAXdA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "a33eebd7-5980-4b6c-dd4e-21d5d878d73a"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iterator, criterion, optimizer, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss): 7.3f}\")\n",
        "    print(f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss): 7.3f}\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 4m 25.250166416168213s\n",
            "\tTrain Loss: 3.563 | Train PPL:  35.271\n",
            "\tValid Loss: 3.128 | Valid PPL:  22.820\n",
            "Epoch: 02 | Time: 4m 24.61474061012268s\n",
            "\tTrain Loss: 2.927 | Train PPL:  18.675\n",
            "\tValid Loss: 2.956 | Valid PPL:  19.212\n",
            "Epoch: 03 | Time: 4m 24.679898738861084s\n",
            "\tTrain Loss: 2.757 | Train PPL:  15.750\n",
            "\tValid Loss: 2.882 | Valid PPL:  17.856\n",
            "Epoch: 04 | Time: 4m 25.98631000518799s\n",
            "\tTrain Loss: 2.651 | Train PPL:  14.162\n",
            "\tValid Loss: 2.837 | Valid PPL:  17.072\n",
            "Epoch: 05 | Time: 4m 25.264737367630005s\n",
            "\tTrain Loss: 2.574 | Train PPL:  13.114\n",
            "\tValid Loss: 2.814 | Valid PPL:  16.676\n",
            "Epoch: 06 | Time: 4m 25.38588285446167s\n",
            "\tTrain Loss: 2.514 | Train PPL:  12.352\n",
            "\tValid Loss: 2.799 | Valid PPL:  16.431\n",
            "Epoch: 07 | Time: 4m 26.530824184417725s\n",
            "\tTrain Loss: 2.466 | Train PPL:  11.781\n",
            "\tValid Loss: 2.808 | Valid PPL:  16.584\n",
            "Epoch: 08 | Time: 4m 26.672357320785522s\n",
            "\tTrain Loss: 2.427 | Train PPL:  11.320\n",
            "\tValid Loss: 2.803 | Valid PPL:  16.498\n",
            "Epoch: 09 | Time: 4m 26.779080390930176s\n",
            "\tTrain Loss: 2.394 | Train PPL:  10.962\n",
            "\tValid Loss: 2.796 | Valid PPL:  16.385\n",
            "Epoch: 10 | Time: 4m 26.84247899055481s\n",
            "\tTrain Loss: 2.365 | Train PPL:  10.649\n",
            "\tValid Loss: 2.802 | Valid PPL:  16.470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQv-Da10fcT4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a676e268-dae6-4fe7-8656-874d03f2eb85"
      },
      "source": [
        "# Load the trained model\n",
        "model.load_state_dict(torch.load('model.pt'))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzemT2IYqDh2",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM5yGk4wfkID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d41d1f5-43bf-4c57-b383-5384a7d8c0a2"
      },
      "source": [
        "model.eval()\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "print(f\"\\tTest Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss): 7.3f}\")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTest Loss: 2.738 | Test PPL:  15.451\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3cwhSRmtMZ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "ae67bdd6-5647-405d-b2a1-967422c7297d"
      },
      "source": [
        "!ls -lah"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 366M\n",
            "drwxr-xr-x 1 root root 4.0K Jun 28 15:52 .\n",
            "drwxr-xr-x 1 root root 4.0K Jun 28 15:40 ..\n",
            "-rw-r--r-- 1 root root  23M Jun 28 15:44 all_data.txt\n",
            "drwxr-xr-x 2 root root 4.0K Jun 28 15:41 annotations\n",
            "-rw-r--r-- 1 root root 242M Jul 10  2018 annotations_trainval2017.zip\n",
            "-rw-r--r-- 1 root root 271K Jun 28 15:44 bpe.model\n",
            "drwxr-xr-x 1 root root 4.0K Jun 25 17:02 .config\n",
            "-rw-r--r-- 1 root root  75M Jun 28 16:27 model.pt\n",
            "drwxr-xr-x 1 root root 4.0K Jun 17 16:18 sample_data\n",
            "-rw-r--r-- 1 root root 1.4M Jun 28 15:44 test_ds.csv\n",
            "-rw-r--r-- 1 root root  25M Jun 28 15:44 train_ds.csv\n",
            "-rw-r--r-- 1 root root 1.4M Jun 28 15:44 valid_ds.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuZYZErmtl27",
        "colab_type": "text"
      },
      "source": [
        "## Utterance generation with Greedy Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS8U79sLeUZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_utterance_greedy(sentence, bpe_model, model, device, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "        tokens = bpe_tokenizer(sentence)\n",
        "    else:\n",
        "        tokens = [int(token) for token in sentence]\n",
        "\n",
        "    src_indexes = tokens\n",
        " \n",
        "    # convert to tensor format\n",
        "    # since the inference is done on single sentence, batch size is 1\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    # src_tensor => [1, seq_len]\n",
        "\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "    \n",
        "    # the starting input to decoder is always <bos>\n",
        "    trg_indexes = [bpe_model.subword_to_id('<BOS>')]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:, -1].item()\n",
        "\n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        # if the predicted token is <eos> means stop the decoding\n",
        "        if pred_token == bpe_model.subword_to_id('<EOS>'):\n",
        "            break\n",
        "    \n",
        "    # convert the predicted token ids to words\n",
        "    trg_tokens = bpe_model.decode(trg_indexes, ignore_ids=[2,3])[0] # ignore <bos>, <eos>\n",
        "\n",
        "    return tokens, trg_tokens, attention\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PWvhIvstvjs",
        "colab_type": "text"
      },
      "source": [
        "## Utterane Generation with Beam Search\n",
        "\n",
        "One of the ways to mitigate the repetition in the generation of utterances is to use Beam Search. By choosing the top-scored word at each step (greedy) may lead to a sub-optimal solution but by choosing a lower scored word that may reach an optimal solution.\n",
        "\n",
        "Instead of greedily choosing the most likely next step as the sequence is constructed, the beam search expands all possible next steps and keeps the k most likely, where k is a user-specified parameter and controls the number of beams or parallel searches through the sequence of probabilities.\n",
        "\n",
        "![beam](https://drive.google.com/uc?id=1lzTlU3Ui4V_qwc3bDEXkPK41vOhMOAI9)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX--xsAkdn3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_utterance_beam(sentence, bpe_model, model, device, max_len=50, beam_size=10, length_norm_coefficient=0.6):\n",
        "    with torch.no_grad():\n",
        "        k = beam_size\n",
        "\n",
        "        # minimum number of hypotheses to complete\n",
        "        n_completed_hypotheses = min(k, 10)\n",
        "\n",
        "        # vocab size\n",
        "        vocab_size = bpe_model.vocab_size()\n",
        "\n",
        "        if isinstance(sentence, str):\n",
        "            tokens = bpe_tokenizer(sentence)\n",
        "        else:\n",
        "            tokens = [int(token) for token in sentence]\n",
        "\n",
        "        src_indexes = tokens\n",
        "        \n",
        "        # convert to tensor format\n",
        "        # since the inference is done on single sentence, batch size is 1\n",
        "        src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "        # src_tensor => [1, seq_len]\n",
        "\n",
        "        # encode\n",
        "        enc_src = model.encoder(src_tensor)\n",
        "        # enc_src => [1, src_len, d_model]\n",
        "\n",
        "        # Our hypothesis to begin with is just <bos>\n",
        "        hypotheses = torch.LongTensor([[bpe_model.subword_to_id('<BOS>')]]).to(device)  # (1, 1)\n",
        "\n",
        "        # Tensor to store hypotheses' scores; now it's just 0\n",
        "        hypotheses_scores = torch.zeros(1).to(device)  # (1)\n",
        "\n",
        "        # Lists to store completed hypotheses and their scores\n",
        "        completed_hypotheses = list()\n",
        "        completed_hypotheses_scores = list()\n",
        "\n",
        "        # Start decoding\n",
        "        step = 1\n",
        "\n",
        "        # Assume \"s\" is the number of incomplete hypotheses currently in the bag; a number less than or equal to \"k\"\n",
        "        # At this point, s is 1, because we only have 1 hypothesis to work with, i.e. \"<sos>\"\n",
        "        while True:\n",
        "            s = hypotheses.size(0)\n",
        "            trg_mask = model.make_trg_mask(hypotheses)\n",
        "            decoder_sequences, _ = model.decoder(hypotheses, enc_src.repeat(s, 1, 1), trg_mask)\n",
        "            # decoder_sequences => [s, step_size, vocab_size]\n",
        "\n",
        "            # Scores at this step\n",
        "            scores = decoder_sequences[:, -1, :]  # (s, vocab_size)\n",
        "            scores = F.log_softmax(scores, dim=-1)  # (s, vocab_size)\n",
        "\n",
        "            # Add hypotheses' scores from last step to scores at this step to get scores for all possible new hypotheses\n",
        "            scores = hypotheses_scores.unsqueeze(1) + scores  # (s, vocab_size)\n",
        "\n",
        "            # Unroll and find top k scores, and their unrolled indices\n",
        "            top_k_hypotheses_scores, unrolled_indices = scores.view(-1).topk(k, 0, True, True)  # (k)\n",
        "\n",
        "            # Convert unrolled indices to actual indices of the scores tensor which yielded the best scores\n",
        "            prev_word_indices = unrolled_indices // vocab_size  # (k)\n",
        "            next_word_indices = unrolled_indices % vocab_size  # (k)\n",
        "\n",
        "            # Construct the the new top k hypotheses from these indices\n",
        "            top_k_hypotheses = torch.cat([hypotheses[prev_word_indices], next_word_indices.unsqueeze(1)],\n",
        "                                         dim=1)  # (k, step + 1)\n",
        "            \n",
        "            # Which of these new hypotheses are complete (reached <eos>)?\n",
        "            complete = next_word_indices == bpe_model.subword_to_id('<EOS>')  # (k), bool\n",
        "\n",
        "            # Set aside completed hypotheses and their scores normalized by their lengths\n",
        "            # For the length normalization formula, see\n",
        "            # \"Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\"\n",
        "            completed_hypotheses.extend(top_k_hypotheses[complete].tolist())\n",
        "            norm = math.pow(((5 + step) / (5 + 1)), length_norm_coefficient)\n",
        "            completed_hypotheses_scores.extend((top_k_hypotheses_scores[complete] / norm).tolist())\n",
        "\n",
        "            # Stop if we have completed enough hypotheses\n",
        "            if len(completed_hypotheses) >= n_completed_hypotheses:\n",
        "                break\n",
        "\n",
        "            # Else, continue with incomplete hypotheses\n",
        "            hypotheses = top_k_hypotheses[~complete]  # (s, step + 1)\n",
        "            hypotheses_scores = top_k_hypotheses_scores[~complete]  # (s)\n",
        "            hypotheses_lengths = torch.LongTensor(hypotheses.size(0) * [hypotheses.size(1)]).to(device)  # (s)\n",
        "\n",
        "            # Stop if things have been going on for too long\n",
        "            if step > 100:\n",
        "                break\n",
        "            step += 1\n",
        "        \n",
        "        # If there is not a single completed hypothesis, use partial hypotheses\n",
        "        if len(completed_hypotheses) == 0:\n",
        "            completed_hypotheses = hypotheses.tolist()\n",
        "            completed_hypotheses_scores = hypotheses_scores.tolist()\n",
        "        \n",
        "        # Decode the hypotheses\n",
        "        all_hypotheses = list()\n",
        "        for i, hypo in enumerate(completed_hypotheses):\n",
        "            h = bpe_model.decode(hypo, ignore_ids=[2, 3])[0]    # ignore <bos>, <eos>\n",
        "            all_hypotheses.append({\"hypothesis\": h, \"score\": completed_hypotheses_scores[i]})\n",
        "        \n",
        "        # Find the best scoring completed hypothesis\n",
        "        i = completed_hypotheses_scores.index(max(completed_hypotheses_scores))\n",
        "        best_hypothesis = all_hypotheses[i][\"hypothesis\"]\n",
        "\n",
        "        return tokens, best_hypothesis, all_hypotheses"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LtM0qYKt18N",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcOx1gdgiZOh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "d41418bf-625b-4553-da5b-9d6fca911697"
      },
      "source": [
        "example_idx = 55\n",
        "\n",
        "src = vars(valid_dataset.examples[example_idx])['source']\n",
        "trg = vars(valid_dataset.examples[example_idx])['target']\n",
        "\n",
        "print(f'src = {bpe_model.decode(src, ignore_ids=[2, 3])[0]}')\n",
        "print(f'trg = {bpe_model.decode(trg, ignore_ids=[2,3])[0]}\\n')\n",
        "\n",
        "\n",
        "_, utterance, attention = generate_utterance_greedy(src, bpe_model, model, device)\n",
        "_, best_one, all_utterances = generate_utterance_beam(src, bpe_model, model, device)\n",
        "\n",
        "print(f'greedy generated utterance = {utterance}\\n')\n",
        "print(\"All beam generated utterances:\")\n",
        "print(\"------------------------------\")\n",
        "for i in all_utterances:\n",
        "    print(f'{i[\"hypothesis\"]}')\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = an athlete swinging a tennis racket at a tennis ball\n",
            "trg = a man swinging a tennis racket backhanded at a tennis ball.\n",
            "\n",
            "greedy generated utterance = a tennis player is hitting a ball with a racket\n",
            "\n",
            "All beam generated utterances:\n",
            "------------------------------\n",
            "a tennis player hitting a ball with a racquet.\n",
            "a tennis player hitting a ball with his racket.\n",
            "a tennis player hitting a ball with a racket.\n",
            "a tennis player hitting a ball with a racket\n",
            "a tennis player hitting a tennis ball with his racket.\n",
            "a tennis player hitting a tennis ball with a racquet.\n",
            "a man holding a tennis racquet on a tennis court.\n",
            "a tennis player hitting a tennis ball with a racket.\n",
            "a tennis player hitting a tennis ball with a racket\n",
            "a tennis player is hitting a tennis ball with a racquet.\n",
            "a tennis player is hitting a tennis ball with a racket.\n",
            "a tennis player is hitting a tennis ball with a racket\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwBtUydakMe_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "674288b9-f158-4981-8c42-4c7aaa1af304"
      },
      "source": [
        "src = \"A car is parked on the side of a road\"\n",
        "\n",
        "tokens, utterance, attention = generate_utterance_greedy(src, bpe_model, model, device)\n",
        "_, _, all_utterances = generate_utterance_beam(src, bpe_model, model, device)\n",
        "print(f'src = {src}\\n')\n",
        "print(f'Greedy generated utterance = {utterance}\\n')\n",
        "print(\"Beam generated utterances:\")\n",
        "print(\"------------------------------\")\n",
        "for i in all_utterances:\n",
        "    print(f'{i[\"hypothesis\"]}')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = A car is parked on the side of a road\n",
            "\n",
            "Greedy generated utterance = a car driving down a street with a lot of traffic.\n",
            "\n",
            "Beam generated utterances:\n",
            "------------------------------\n",
            "a car parked on the side of a road.\n",
            "a car parked on the side of the road.\n",
            "a car parked on the side of a street.\n",
            "a truck driving down a street next to houses.\n",
            "a car driving down a street next to a building.\n",
            "a truck driving down a street next to a building.\n",
            "a car driving down a street next to a parking meter.\n",
            "a car driving down a street next to a parking lot.\n",
            "a car driving down a street next to a parking lot\n",
            "a car driving down a street next to a parking meter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpwB9Mn68sv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "2d245ff6-da8b-4acf-d658-204352ef349b"
      },
      "source": [
        "src = \"A plane is flying high in the sky\"\n",
        "\n",
        "tokens, utterance, attention = generate_utterance_greedy(src, bpe_model, model, device)\n",
        "_, _, all_utterances = generate_utterance_beam(src, bpe_model, model, device)\n",
        "print(f'src = {src}\\n')\n",
        "print(f'Greedy generated utterance = {utterance}\\n')\n",
        "print(\"Beam generated utterances:\")\n",
        "print(\"------------------------------\")\n",
        "for i in all_utterances:\n",
        "    print(f'{i[\"hypothesis\"]}')\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = A plane is flying high in the sky\n",
            "\n",
            "Greedy generated utterance = a plane flying in the sky with a sky background\n",
            "\n",
            "Beam generated utterances:\n",
            "------------------------------\n",
            "a plane that is flying in the sky.\n",
            "a plane that is flying in the air.\n",
            "a plane is flying high in the sky.\n",
            "a plane that is flying in the sky\n",
            "a plane is flying high in the sky\n",
            "a plane flying in the sky above a mountain.\n",
            "a plane flying in the air above a mountain.\n",
            "a plane flying in the sky with a sky background\n",
            "a plane flying high in the sky above a mountain.\n",
            "a plane flying high in the sky above a city.\n",
            "a plane flying high in the sky above a field.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKfbslnE8Xpy",
        "colab_type": "text"
      },
      "source": [
        "## Further Improvements\n",
        "\n",
        "With this I am ending the utterance generation series. However, following enhancements could be done.\n",
        "\n",
        "*   Evaluation metrics like BLEU, Rouge \n",
        "*   Combined training / finetuning on Quora dataset, so that model can generate utterances for general sentences and question type utterances\n",
        "*   Explore the [Paraphrase Dataset](http://paraphrase.org/) and train the model on that data\n",
        "*   Using pretrained models like GPT-2/T5/BART model\n",
        "\n",
        "*Note: Raise an issue [here](https://github.com/graviraja/100-Days-of-NLP/issues) in case of any issues/modifications*\n"
      ]
    }
  ]
}