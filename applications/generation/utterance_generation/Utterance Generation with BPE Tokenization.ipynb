{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Utterance Generation with BPE Tokenization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMoA2NRqvlcSEIyRwr2wixe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/100-Days-of-NLP/blob/applications%2Fgeneration/applications/generation/utterance_generation/Utterance%20Generation%20with%20BPE%20Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKsk0jqW74Ex",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "a854731e-c42f-49e0-8c58-d3cf74c20fcf"
      },
      "source": [
        "!pip install youtokentome"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting youtokentome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/4a86cf99da3f680497ae132329025b291e2fda22327e8da6a9476e51acb1/youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\r\u001b[K     |▏                               | 10kB 12.0MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 1.6MB/s eta 0:00:02\r\u001b[K     |▋                               | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |▊                               | 40kB 2.4MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 61kB 2.1MB/s eta 0:00:01\r\u001b[K     |█▍                              | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |█▌                              | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |█▊                              | 92kB 2.7MB/s eta 0:00:01\r\u001b[K     |██                              | 102kB 2.6MB/s eta 0:00:01\r\u001b[K     |██                              | 112kB 2.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 122kB 2.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 133kB 2.6MB/s eta 0:00:01\r\u001b[K     |██▊                             | 143kB 2.6MB/s eta 0:00:01\r\u001b[K     |██▉                             | 153kB 2.6MB/s eta 0:00:01\r\u001b[K     |███                             | 163kB 2.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 174kB 2.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 184kB 2.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 194kB 2.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 204kB 2.6MB/s eta 0:00:01\r\u001b[K     |████                            | 215kB 2.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 225kB 2.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 235kB 2.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 245kB 2.6MB/s eta 0:00:01\r\u001b[K     |████▉                           | 256kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 266kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 276kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 286kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 296kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 307kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 317kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 327kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 337kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 348kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 358kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 368kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 378kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 389kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 399kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 409kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 419kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 430kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 440kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 450kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 460kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 471kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 481kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 491kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 501kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 512kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 522kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 532kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 542kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 552kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 563kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 573kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 583kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 593kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 604kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 614kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 624kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 634kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 645kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 655kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 665kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 675kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 686kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 696kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 706kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 716kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 727kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 737kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 747kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 757kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 768kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 778kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 788kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 798kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 808kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 819kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 829kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 839kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 849kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 860kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 870kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 880kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 890kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 901kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 911kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 921kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 931kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 942kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 952kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 962kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 972kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 983kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 993kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.0MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.0MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 1.0MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.0MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.0MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.1MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.1MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.1MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.1MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.1MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.1MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.1MB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.1MB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.1MB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.1MB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.2MB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.2MB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.2MB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.2MB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.2MB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.2MB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.2MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.2MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.2MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.2MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.3MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.3MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.3MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.3MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.3MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.3MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.3MB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3MB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.3MB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.4MB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.4MB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.4MB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.4MB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.4MB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.4MB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.4MB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.4MB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.4MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.4MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.5MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.5MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.5MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.5MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.5MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.5MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.5MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.5MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.5MB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.5MB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.6MB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.6MB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.6MB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.6MB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.6MB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.6MB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.6MB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.6MB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.6MB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.6MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.7MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.7MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.7MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.7MB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.7MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7MB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.7MB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome) (7.1.2)\n",
            "Installing collected packages: youtokentome\n",
            "Successfully installed youtokentome-1.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sza_abchtrF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "ad89eca9-f7a4-4057-f0a7-c7ffa3f46811"
      },
      "source": [
        "TASK_DATA_DIR = 'glue_data/QQP'\n",
        "!test -d glue_data || git clone https://gist.github.com/60c2bdb54d156a41194446737ce03e2e.git glue_data\n",
        "!test -d $TASK_DATA_DIR || python glue_data/download_glue_data.py --data_dir glue_data --tasks=QQP\n",
        "!ls -alh $TASK_DATA_DIR"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'glue_data'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Total 21 (delta 0), reused 0 (delta 0), pack-reused 21\u001b[K\n",
            "Unpacking objects:   4% (1/21)   \rUnpacking objects:   9% (2/21)   \rUnpacking objects:  14% (3/21)   \rUnpacking objects:  19% (4/21)   \rUnpacking objects:  23% (5/21)   \rUnpacking objects:  28% (6/21)   \rUnpacking objects:  33% (7/21)   \rUnpacking objects:  38% (8/21)   \rUnpacking objects:  42% (9/21)   \rUnpacking objects:  47% (10/21)   \rUnpacking objects:  52% (11/21)   \rUnpacking objects:  57% (12/21)   \rUnpacking objects:  61% (13/21)   \rUnpacking objects:  66% (14/21)   \rUnpacking objects:  71% (15/21)   \rUnpacking objects:  76% (16/21)   \rUnpacking objects:  80% (17/21)   \rUnpacking objects:  85% (18/21)   \rUnpacking objects:  90% (19/21)   \rUnpacking objects:  95% (20/21)   \rUnpacking objects: 100% (21/21)   \rUnpacking objects: 100% (21/21), done.\n",
            "Downloading and extracting QQP...\n",
            "\tCompleted!\n",
            "total 104M\n",
            "drwxr-xr-x 3 root root 4.0K Jun 26 15:45 .\n",
            "drwxr-xr-x 4 root root 4.0K Jun 26 15:45 ..\n",
            "-rw-r--r-- 1 root root 5.6M Jun 26 15:45 dev.tsv\n",
            "drwxr-xr-x 2 root root 4.0K Jun 26 15:45 original\n",
            "-rw-r--r-- 1 root root  49M Jun 26 15:45 test.tsv\n",
            "-rw-r--r-- 1 root root  50M Jun 26 15:45 train.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCAkehv1btSs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "0bcfd7bf-a9c9-4368-f55c-36ca176f45cd"
      },
      "source": [
        "import time\n",
        "import codecs\n",
        "import random\n",
        "import math\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import youtokentome\n",
        "from torchtext import data, vocab\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import seaborn as sns"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IsArf_BcbJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMt1wbI3drfi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef1b4aed-9c1e-47be-cf23-ee916cb7b9d4"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNJ4w3ksdxbk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "8327eab0-ce93-47db-89d9-f6ac41f7addb"
      },
      "source": [
        "train_df = pd.read_csv(TASK_DATA_DIR + '/train.tsv', sep='\\t', error_bad_lines=False)\n",
        "valid_df = pd.read_csv(TASK_DATA_DIR + '/dev.tsv', sep='\\t', error_bad_lines=False)\n",
        "train_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 83032: expected 6 fields, saw 7\\n'\n",
            "b'Skipping line 154657: expected 6 fields, saw 7\\n'\n",
            "b'Skipping line 323916: expected 6 fields, saw 7\\n'\n",
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "      <th>is_duplicate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>133273</td>\n",
              "      <td>213221</td>\n",
              "      <td>213222.0</td>\n",
              "      <td>How is the life of a math student? Could you d...</td>\n",
              "      <td>Which level of prepration is enough for the ex...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>402555</td>\n",
              "      <td>536040</td>\n",
              "      <td>536041.0</td>\n",
              "      <td>How do I control my horny emotions?</td>\n",
              "      <td>How do you control your horniness?</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>360472</td>\n",
              "      <td>364011</td>\n",
              "      <td>490273.0</td>\n",
              "      <td>What causes stool color to change to yellow?</td>\n",
              "      <td>What can cause stool to come out as little balls?</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>150662</td>\n",
              "      <td>155721</td>\n",
              "      <td>7256.0</td>\n",
              "      <td>What can one do after MBBS?</td>\n",
              "      <td>What do i do after my MBBS ?</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>183004</td>\n",
              "      <td>279958</td>\n",
              "      <td>279959.0</td>\n",
              "      <td>Where can I find a power outlet for my laptop ...</td>\n",
              "      <td>Would a second airport in Sydney, Australia be...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ... is_duplicate\n",
              "0  133273  ...          0.0\n",
              "1  402555  ...          1.0\n",
              "2  360472  ...          0.0\n",
              "3  150662  ...          1.0\n",
              "4  183004  ...          0.0\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoON5vy6h__i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15b00c92-71aa-4fa4-8b1f-6d59385087ff"
      },
      "source": [
        "len(train_df), len(valid_df)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(363192, 40372)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L58WhvBid8G8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "3c3c2a99-c8a4-4be4-f0d7-55a8a28844e0"
      },
      "source": [
        "sns.countplot(train_df['is_duplicate'])\n",
        "plt.xlabel('Train data distribution')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train data distribution')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owbcmh4kiDB6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "e8fa3f11-2882-4c4a-e464-eb3657798246"
      },
      "source": [
        "sns.countplot(valid_df['is_duplicate'])\n",
        "plt.xlabel('Valid data distribution')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Valid data distribution')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VcVceuyiGC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = train_df[train_df['is_duplicate'] == 1]\n",
        "valid_data = valid_df[valid_df['is_duplicate'] == 1]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kpuSDDkiIgT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ec68cf8-b184-4d07-cf7a-c01492c839ee"
      },
      "source": [
        "len(train_data), len(valid_data)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(134141, 14857)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqIbATTEiKbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = train_data[['question1', 'question2']]\n",
        "valid_data = valid_data[['question1', 'question2']]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cxeFuvkiMQk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d071c824-f3f2-4580-cb77-063a8a037457"
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How do I control my horny emotions?</td>\n",
              "      <td>How do you control your horniness?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What can one do after MBBS?</td>\n",
              "      <td>What do i do after my MBBS ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What is the best self help book you have read?...</td>\n",
              "      <td>What are the top self help books I should read?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What will be Hillary Clinton's policy towards ...</td>\n",
              "      <td>What will be Hilary Clinton's policy towards I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Which is the best book to study TENSOR for gen...</td>\n",
              "      <td>Which is the best book for tensor calculus?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question1                                          question2\n",
              "1                 How do I control my horny emotions?                 How do you control your horniness?\n",
              "3                         What can one do after MBBS?                       What do i do after my MBBS ?\n",
              "7   What is the best self help book you have read?...    What are the top self help books I should read?\n",
              "11  What will be Hillary Clinton's policy towards ...  What will be Hilary Clinton's policy towards I...\n",
              "13  Which is the best book to study TENSOR for gen...        Which is the best book for tensor calculus?"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62oNUxItiOD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_train_data = train_data\n",
        "sample_valid_data = valid_data"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iph-nS94iP95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_train_data.to_csv('train_ds.csv')\n",
        "sample_valid_data.to_csv('valid_ds.csv')"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe1SsyEniR9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "72ad8028-fb00-4388-dbc2-c0015bd2da7e"
      },
      "source": [
        "!ls -lah"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 62M\n",
            "drwxr-xr-x 1 root root 4.0K Jun 26 16:19 .\n",
            "drwxr-xr-x 1 root root 4.0K Jun 26 15:40 ..\n",
            "-rw-r--r-- 1 root root 5.2M Jun 26 16:15 all_data.txt\n",
            "-rw-r--r-- 1 root root 101K Jun 26 16:15 bpe.model\n",
            "drwxr-xr-x 1 root root 4.0K Jun 19 16:15 .config\n",
            "drwxr-xr-x 4 root root 4.0K Jun 26 15:45 glue_data\n",
            "-rw-r--r-- 1 root root  40M Jun 26 16:33 model.pt\n",
            "drwxr-xr-x 1 root root 4.0K Jun 17 16:18 sample_data\n",
            "-rw-r--r-- 1 root root  15M Jun 26 16:48 train_ds.csv\n",
            "-rw-r--r-- 1 root root 1.7M Jun 26 16:48 valid_ds.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNkx98nRLeH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using only training data to train the BPE tokenizer\n",
        "all_data = list(sample_train_data['question1'].str.lower().values) + list(sample_train_data['question2'].str.lower().values)\n",
        "\n",
        "with codecs.open(\"all_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(all_data))"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mPUkWmEMoAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# free some ram\n",
        "del all_data"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qE-ivyaMDS_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cdd89173-b5b8-406c-ec8d-edf043aa90d4"
      },
      "source": [
        "# Perform BPE\n",
        "print(\"\\nLearning BPE...\")\n",
        "youtokentome.BPE.train(data=\"all_data.txt\", vocab_size=15000, model=\"bpe.model\")"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Learning BPE...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<youtokentome.youtokentome.BPE at 0x7fbe2a366eb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Oh-1x80VDsA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "5c65654a-c2a1-4d97-e45f-08a221e84cd6"
      },
      "source": [
        "!ls -lah"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 70M\n",
            "drwxr-xr-x 1 root root 4.0K Jun 26 16:19 .\n",
            "drwxr-xr-x 1 root root 4.0K Jun 26 15:40 ..\n",
            "-rw-r--r-- 1 root root  14M Jun 26 16:48 all_data.txt\n",
            "-rw-r--r-- 1 root root 200K Jun 26 16:48 bpe.model\n",
            "drwxr-xr-x 1 root root 4.0K Jun 19 16:15 .config\n",
            "drwxr-xr-x 4 root root 4.0K Jun 26 15:45 glue_data\n",
            "-rw-r--r-- 1 root root  40M Jun 26 16:33 model.pt\n",
            "drwxr-xr-x 1 root root 4.0K Jun 17 16:18 sample_data\n",
            "-rw-r--r-- 1 root root  15M Jun 26 16:48 train_ds.csv\n",
            "-rw-r--r-- 1 root root 1.7M Jun 26 16:48 valid_ds.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SYi5Pv1MXLo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5a31d8f0-5542-4fdf-fb1b-a9f343c7de68"
      },
      "source": [
        "# Load BPE model\n",
        "print(\"\\nLoading BPE model...\")\n",
        "bpe_model = youtokentome.BPE(model=\"bpe.model\")"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading BPE model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtqgyN4_N_W8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9105018d-eb00-4dc5-ac5f-0fc50ede78e0"
      },
      "source": [
        "# Special Tokens\n",
        "print(f\"<BOS>: {bpe_model.subword_to_id('<BOS>')}\")    # Begining of the sentence token\n",
        "print(f\"<EOS>: {bpe_model.subword_to_id('<EOS>')}\")    # End of the sentence token\n",
        "print(f\"<UNK>: {bpe_model.subword_to_id('<UNK>')}\")    # Unknown token\n",
        "print(f\"<PAD>: {bpe_model.subword_to_id('<PAD>')}\")    # Pad token"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BOS>: 2\n",
            "<EOS>: 3\n",
            "<UNK>: 1\n",
            "<PAD>: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JoafiSZrjDxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_index = bpe_model.subword_to_id('<PAD>')"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEYH9D2AUYft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5664d1ee-a580-4e1b-b113-3317dd68be46"
      },
      "source": [
        "sentence = \"This is a sample sentence\"\n",
        "encoded_ids = bpe_model.encode(sentence.lower(), output_type=youtokentome.OutputType.ID, bos=True, eos=True)\n",
        "encoded_text = bpe_model.encode(sentence.lower(), output_type=youtokentome.OutputType.SUBWORD, bos=True, eos=True)\n",
        "decoded_text = bpe_model.decode(encoded_ids, ignore_ids=[2, 3])\n",
        "\n",
        "print(encoded_ids)\n",
        "print(encoded_text)\n",
        "print(decoded_text)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 732, 250, 225, 8472, 271, 6710, 3]\n",
            "['<BOS>', '▁this', '▁is', '▁a', '▁samp', 'le', '▁sentence', '<EOS>']\n",
            "['this is a sample sentence']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExVJzF0APBo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a tokenizer method which takes in a sentence and returns ids\n",
        "# by defining this, we can configure the tokenizer to torchtext Field\n",
        "def bpe_tokenizer(sentence):\n",
        "    encoded_ids = bpe_model.encode(sentence.lower(), output_type=youtokentome.OutputType.ID, bos=True, eos=True)\n",
        "    return encoded_ids"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE6AfGCrRyMU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3cd5c8ba-15d9-4baf-f18a-9c08cb24f47c"
      },
      "source": [
        "bpe_tokenizer(\"This is a sample sentence\")"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 732, 250, 225, 8472, 271, 6710, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuB53_DbiTPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = data.get_tokenizer(bpe_tokenizer)\n",
        "TEXT = data.Field(tokenize=tokenizer, batch_first=True, use_vocab=False, pad_token=pad_index)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RGUVl9ReSuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fields = [(None, None), (\"source\", TEXT), (\"target\", TEXT)]\n",
        "\n",
        "train_dataset, valid_dataset = data.TabularDataset.splits(path='.',\n",
        "                                     train='train_ds.csv', validation='valid_ds.csv',\n",
        "                                     format='csv', skip_header=True, fields=fields)"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ6NELb0ibX4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a66c8d87-aa69-411b-f066-647d4acd497f"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation examples: {len(valid_dataset)}\")"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 134141\n",
            "Number of validation examples: 14857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nFRDyZsic9B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8e9ec04-c470-490c-c858-88c27fa2312c"
      },
      "source": [
        "print(vars(train_dataset.examples[1]))"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'source': [2, 243, 272, 476, 252, 539, 5008, 3], 'target': [2, 243, 252, 222, 252, 539, 301, 5360, 1223, 3]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzOFppxuWY3s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f98660fd-977a-4938-c7c4-2d53a18b7c77"
      },
      "source": [
        "# Building vocabulary is not required as the BPE tokenizer already convert the sentence into ids\n",
        "\n",
        "# let's check the vocab size\n",
        "print(f\"Vocab size: {bpe_model.vocab_size()}\")"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNpR4NNtejTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
        "    (train_dataset, valid_dataset),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_key=lambda x: len(x.source),\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZYV_W_igUTt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50fbb128-337c-4bc5-8fcb-c538aff8bee8"
      },
      "source": [
        "temp = next(iter(train_iterator))\n",
        "temp.source.shape, temp.target.shape"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 27]), torch.Size([64, 25]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeJnl6cue6rJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout, pad_idx, device):\n",
        "        super().__init__()\n",
        "\n",
        "        assert d_model % n_heads == 0, \"n_heads must be a factor of d_model\"\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "\n",
        "        self.q = nn.Linear(d_model, d_model)\n",
        "        self.k = nn.Linear(d_model, d_model)\n",
        "        self.v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.pad_idx = pad_idx\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # query => [batch_size, seq_len, d_model] \n",
        "        # key => [batch_size, seq_len, d_model]\n",
        "        # value => [batch_size, seq_len, d_model]\n",
        "\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        Q = self.q(query)\n",
        "        K = self.k(key)\n",
        "        V = self.v(value)\n",
        "        # Q, K, V => [batch_size, seq_len, d_model]\n",
        "\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # Q, K, V => [batch_size, n_heads, seq_len, head_dim]\n",
        "\n",
        "        energy = torch.matmul(Q, K.permute(0 ,1, 3, 2))\n",
        "        energy = energy / self.scale\n",
        "        # energy => [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "        # attention => [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        weighted = torch.matmul(attention, V)\n",
        "        # weighted => [batch_size, n_heads, query_len, head_dim]\n",
        "\n",
        "        weighted = weighted.permute(0, 2, 1, 3).contiguous()\n",
        "        # weighted => [batch_size, query_len, n_heads, head_dim]\n",
        "\n",
        "        x = weighted.view(batch_size, -1, self.d_model)\n",
        "        # x => [batch_size, query_len, d_model]\n",
        "\n",
        "        x = self.fc(x)\n",
        "        # x => [batch_size, query_len, d_model]\n",
        "        # attention => [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        return x, attention\n"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vz1NP3akIUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, pff_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(d_model, pff_dim)\n",
        "        self.fc2 = nn.Linear(pff_dim, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        # input => [batch_size, seq_len, d_model]\n",
        "\n",
        "        x = self.dropout(torch.relu(self.fc1(input)))\n",
        "        # x => [batch_size, seq_len, pff_dim]\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        # x => [batch_size, seq_len, d_model]\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vfRvJLgkMis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, pff_dim, dropout, pad_idx, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attention = SelfAttention(d_model, n_heads, dropout, pad_idx, device)\n",
        "        self.pff = PositionWiseFeedForward(d_model, pff_dim, dropout)\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.pff_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        # src => [batch_size, src_len, d_model]\n",
        "\n",
        "        # self attention on src\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        # _src => [batch_size, src_len, d_model]\n",
        "\n",
        "        # residual connection and layer normalization\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        # src => [batch_size, src_len, d_model]\n",
        "\n",
        "        # position wise feed forward\n",
        "        _src = self.pff(src)\n",
        "        # _src => [batch_size, src_len, d_model]\n",
        "\n",
        "        # residual connection and layer normalization\n",
        "        src = self.pff_layer_norm(src + self.dropout(_src))\n",
        "        # src => [batch_size, src_len, d_model]\n",
        "\n",
        "        return src"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApyJtPvdkQCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, n_layers, n_heads, pff_dim, dropout, pad_idx, device, max_len=500):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(input_dim, d_model)\n",
        "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, pff_dim, dropout, pad_idx, device) for _ in range(n_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
        "    \n",
        "    def forward(self, src, src_mask=None):\n",
        "        # src => [batch_size, src_len]\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        # pos => [batch_size, src_len]\n",
        "\n",
        "        word_embed = self.word_embedding(src)\n",
        "        word_embed = word_embed * self.scale\n",
        "        # word_embed => [batch_size, src_len, d_model]\n",
        "\n",
        "        pos_embed = self.pos_embedding(pos)\n",
        "        # pos_embed => [batch_size, src_len, d_model]\n",
        "\n",
        "        src = self.dropout(word_embed + pos_embed)\n",
        "        # src => [batch_size, src_len, d_model]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        \n",
        "        # src => [batch_size, src_len, d_model]\n",
        "        return src\n"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJc6NfMDkWpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, pff_dim, dropout, pad_idx, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attention = SelfAttention(d_model, n_heads, dropout, pad_idx, device)\n",
        "        self.enc_attention = SelfAttention(d_model, n_heads, dropout, pad_idx, device)\n",
        "        self.pff = PositionWiseFeedForward(d_model, pff_dim, dropout)\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(d_model)\n",
        "        self.pff_layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        \n",
        "        # self attention\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        # _trg => [batch_size, trg_len, d_model]\n",
        "\n",
        "        # residual connection and layer normalization\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "        # trg => [batch_size, trg_len, d_model]\n",
        "\n",
        "        # enc_attention\n",
        "        _trg, attention = self.enc_attention(trg, enc_src, enc_src, src_mask)\n",
        "        # _trg => [batch_size, trg_len, d_model]\n",
        "        # attention => [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "        # residual connection and layer normalization\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "        # trg => [batch_size, trg_len, d_model]\n",
        "\n",
        "        # positionwise feed forward\n",
        "        _trg = self.pff(trg)\n",
        "        # _trg => [batch_size, trg_len, d_model]\n",
        "\n",
        "        # residual connection and layer normalization\n",
        "        trg = self.pff_layer_norm(trg + self.dropout(_trg))\n",
        "        # trg => [batch_size, trg_len, d_model]\n",
        "\n",
        "        return trg, attention\n"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3w0Ui47ka8N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, d_model, n_layers, n_heads, pff_dim, dropout, pad_idx, device, max_len=500):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(output_dim, d_model)\n",
        "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, pff_dim, dropout, pad_idx, device) for _ in range(n_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, output_dim)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([d_model])).to(device)\n",
        "    \n",
        "    def forward(self, trg, enc_src, trg_mask=None, src_mask=None):\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        \n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        # pos => [batch_size, trg_len]\n",
        "\n",
        "        word_embedding = self.word_embedding(trg)\n",
        "        word_embedding = word_embedding * self.scale\n",
        "        # word_embedding => [batch_size, trg_len, d_model]\n",
        "\n",
        "        pos_embedding = self.pos_embedding(pos)\n",
        "        # pos_embedding => [batch_size, trg_len, d_model]\n",
        "\n",
        "        trg = self.dropout(word_embedding + pos_embedding)\n",
        "        # trg => [batch_size, trg_len, d_model]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        logits = self.fc_out(trg)\n",
        "        # logits => [batch_size, trg_len, output_dim]\n",
        "        # attention => [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "        return logits, attention\n"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2di5Xr1keFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, pad_idx, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.pad_idx = pad_idx\n",
        "        self.device = device\n",
        "    \n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2).to(self.device)\n",
        "        # src_mask => [batch_size, 1, 1, src_len]\n",
        "        \n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        trg_pad_mask = (trg != self.pad_idx).unsqueeze(1).unsqueeze(2).to(self.device)\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
        "\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        # trg_mask => [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "        return trg_mask\n",
        " \n",
        "    def forward(self, src, trg):\n",
        "        # src => [batch_size, src_len]\n",
        "        # trg => [batch_size, trg_len]\n",
        "\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        return output, attention"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClzGYGQVoV6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PAD_IDX = pad_index\n",
        "INPUT_DIM = bpe_model.vocab_size()\n",
        "HID_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.3\n",
        "DEC_DROPOUT = 0.3\n",
        "\n",
        "enc = Encoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              ENC_LAYERS, \n",
        "              ENC_HEADS, \n",
        "              ENC_PF_DIM, \n",
        "              ENC_DROPOUT,\n",
        "              PAD_IDX,\n",
        "              device)\n",
        "\n",
        "dec = Decoder(INPUT_DIM, \n",
        "              HID_DIM, \n",
        "              DEC_LAYERS, \n",
        "              DEC_HEADS, \n",
        "              DEC_PF_DIM, \n",
        "              DEC_DROPOUT, \n",
        "              PAD_IDX,\n",
        "              device)"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lbnRuYrodmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Transformer(enc, dec, PAD_IDX, device).to(device)"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1RdG5lhogrl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2816be48-a443-432f-ec45-ff4e48029253"
      },
      "source": [
        "def init_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (word_embedding): Embedding(15000, 256)\n",
              "    (pos_embedding): Embedding(500, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (pff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (pff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (pff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (pff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (pff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (pff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (word_embedding): Embedding(15000, 256)\n",
              "    (pos_embedding): Embedding(500, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (enc_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (pff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (pff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (enc_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (pff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (pff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (enc_attention): SelfAttention(\n",
              "          (q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (pff): PositionWiseFeedForward(\n",
              "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (pff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "    (fc_out): Linear(in_features=256, out_features=15000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPZoHaR8oeH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2be1f2b-b9b3-4110-a145-38bc76e15e00"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 15,744,664 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEVpPmmbopYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPuPSJwOotC7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, criterion, optimizer, clip):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.source\n",
        "        trg = batch.target\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output, _ = model(src, trg[:, :-1])\n",
        "        # output => [batch_size, trg_len - 1, output_dim]\n",
        "        # trg => [batch_size, trg_len]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    return epoch_loss / len(iterator)\n"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y8YEYgN_H1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.source\n",
        "            trg = batch.target\n",
        "\n",
        "            output, _ = model(src, trg[:, :-1])\n",
        "            # output => [batch_size, trg_len - 1, output_dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZzKy8VuAAD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = elapsed_time - (elapsed_mins * 60)\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKYvMZdxAXdA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "24e7f786-15a6-4876-d91a-a4888322dad7"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iterator, criterion, optimizer, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "\n",
        "    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss): 7.3f}\")\n",
        "    print(f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss): 7.3f}\")"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 4m 28.46742033958435s\n",
            "\tTrain Loss: 4.209 | Train PPL:  67.300\n",
            "\tValid Loss: 3.193 | Valid PPL:  24.368\n",
            "Epoch: 02 | Time: 4m 27.564754009246826s\n",
            "\tTrain Loss: 2.986 | Train PPL:  19.812\n",
            "\tValid Loss: 2.674 | Valid PPL:  14.497\n",
            "Epoch: 03 | Time: 4m 27.924962520599365s\n",
            "\tTrain Loss: 2.516 | Train PPL:  12.378\n",
            "\tValid Loss: 2.437 | Valid PPL:  11.442\n",
            "Epoch: 04 | Time: 4m 28.077025175094604s\n",
            "\tTrain Loss: 2.245 | Train PPL:   9.438\n",
            "\tValid Loss: 2.326 | Valid PPL:  10.232\n",
            "Epoch: 05 | Time: 4m 29.57198929786682s\n",
            "\tTrain Loss: 2.068 | Train PPL:   7.910\n",
            "\tValid Loss: 2.259 | Valid PPL:   9.575\n",
            "Epoch: 06 | Time: 4m 30.706633806228638s\n",
            "\tTrain Loss: 1.941 | Train PPL:   6.963\n",
            "\tValid Loss: 2.246 | Valid PPL:   9.448\n",
            "Epoch: 07 | Time: 4m 30.72706151008606s\n",
            "\tTrain Loss: 1.839 | Train PPL:   6.292\n",
            "\tValid Loss: 2.221 | Valid PPL:   9.215\n",
            "Epoch: 08 | Time: 4m 28.905417919158936s\n",
            "\tTrain Loss: 1.759 | Train PPL:   5.807\n",
            "\tValid Loss: 2.209 | Valid PPL:   9.104\n",
            "Epoch: 09 | Time: 4m 27.779645919799805s\n",
            "\tTrain Loss: 1.693 | Train PPL:   5.435\n",
            "\tValid Loss: 2.197 | Valid PPL:   8.995\n",
            "Epoch: 10 | Time: 4m 26.150173902511597s\n",
            "\tTrain Loss: 1.636 | Train PPL:   5.137\n",
            "\tValid Loss: 2.188 | Valid PPL:   8.918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhU0D8FUm8bE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e5b3dc1-fd56-4099-fc7e-d0ebf4dbfdb4"
      },
      "source": [
        "model.load_state_dict(torch.load('model.pt'))"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3cwhSRmtMZ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2380ba33-b5fb-487b-88c1-dc062325b13b"
      },
      "source": [
        "!ls -lah"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 91M\n",
            "drwxr-xr-x 1 root root 4.0K Jun 26 16:19 .\n",
            "drwxr-xr-x 1 root root 4.0K Jun 26 15:40 ..\n",
            "-rw-r--r-- 1 root root  14M Jun 26 16:48 all_data.txt\n",
            "-rw-r--r-- 1 root root 200K Jun 26 16:48 bpe.model\n",
            "drwxr-xr-x 1 root root 4.0K Jun 19 16:15 .config\n",
            "drwxr-xr-x 4 root root 4.0K Jun 26 15:45 glue_data\n",
            "-rw-r--r-- 1 root root  61M Jun 26 17:34 model.pt\n",
            "drwxr-xr-x 1 root root 4.0K Jun 17 16:18 sample_data\n",
            "-rw-r--r-- 1 root root  15M Jun 26 16:48 train_ds.csv\n",
            "-rw-r--r-- 1 root root 1.7M Jun 26 16:48 valid_ds.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lS8U79sLeUZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_utterance_greedy(sentence, bpe_model, model, device, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "        tokens = bpe_tokenizer(sentence)\n",
        "    else:\n",
        "        tokens = [int(token) for token in sentence]\n",
        "\n",
        "    src_indexes = tokens\n",
        " \n",
        "    # convert to tensor format\n",
        "    # since the inference is done on single sentence, batch size is 1\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    # src_tensor => [1, seq_len]\n",
        "\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "    \n",
        "    # the starting input to decoder is always <bos>\n",
        "    trg_indexes = [bpe_model.subword_to_id('<BOS>')]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        \n",
        "        pred_token = output.argmax(2)[:, -1].item()\n",
        "\n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        # if the predicted token is <eos> means stop the decoding\n",
        "        if pred_token == bpe_model.subword_to_id('<EOS>'):\n",
        "            break\n",
        "    \n",
        "    # convert the predicted token ids to words\n",
        "    trg_tokens = bpe_model.decode(trg_indexes, ignore_ids=[2,3])[0] # ignore <bos>, <eos>\n",
        "\n",
        "    return tokens, trg_tokens, attention\n"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PX--xsAkdn3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_utterance_beam(sentence, bpe_model, model, device, max_len=50, beam_size=10, length_norm_coefficient=0.6):\n",
        "    with torch.no_grad():\n",
        "        k = beam_size\n",
        "\n",
        "        # minimum number of hypotheses to complete\n",
        "        n_completed_hypotheses = min(k, 10)\n",
        "\n",
        "        # vocab size\n",
        "        vocab_size = bpe_model.vocab_size()\n",
        "\n",
        "        if isinstance(sentence, str):\n",
        "            tokens = bpe_tokenizer(sentence)\n",
        "        else:\n",
        "            tokens = [int(token) for token in sentence]\n",
        "\n",
        "        src_indexes = tokens\n",
        "        \n",
        "        # convert to tensor format\n",
        "        # since the inference is done on single sentence, batch size is 1\n",
        "        src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "        # src_tensor => [1, seq_len]\n",
        "\n",
        "        # encode\n",
        "        enc_src = model.encoder(src_tensor)\n",
        "        # enc_src => [1, src_len, d_model]\n",
        "\n",
        "        # Our hypothesis to begin with is just <bos>\n",
        "        hypotheses = torch.LongTensor([[bpe_model.subword_to_id('<BOS>')]]).to(device)  # (1, 1)\n",
        "\n",
        "        # Tensor to store hypotheses' scores; now it's just 0\n",
        "        hypotheses_scores = torch.zeros(1).to(device)  # (1)\n",
        "\n",
        "        # Lists to store completed hypotheses and their scores\n",
        "        completed_hypotheses = list()\n",
        "        completed_hypotheses_scores = list()\n",
        "\n",
        "        # Start decoding\n",
        "        step = 1\n",
        "\n",
        "        # Assume \"s\" is the number of incomplete hypotheses currently in the bag; a number less than or equal to \"k\"\n",
        "        # At this point, s is 1, because we only have 1 hypothesis to work with, i.e. \"<sos>\"\n",
        "        while True:\n",
        "            s = hypotheses.size(0)\n",
        "            trg_mask = model.make_trg_mask(hypotheses)\n",
        "            decoder_sequences, _ = model.decoder(hypotheses, enc_src.repeat(s, 1, 1), trg_mask)\n",
        "            # decoder_sequences => [s, step_size, vocab_size]\n",
        "\n",
        "            # Scores at this step\n",
        "            scores = decoder_sequences[:, -1, :]  # (s, vocab_size)\n",
        "            scores = F.log_softmax(scores, dim=-1)  # (s, vocab_size)\n",
        "\n",
        "            # Add hypotheses' scores from last step to scores at this step to get scores for all possible new hypotheses\n",
        "            scores = hypotheses_scores.unsqueeze(1) + scores  # (s, vocab_size)\n",
        "\n",
        "            # Unroll and find top k scores, and their unrolled indices\n",
        "            top_k_hypotheses_scores, unrolled_indices = scores.view(-1).topk(k, 0, True, True)  # (k)\n",
        "\n",
        "            # Convert unrolled indices to actual indices of the scores tensor which yielded the best scores\n",
        "            prev_word_indices = unrolled_indices // vocab_size  # (k)\n",
        "            next_word_indices = unrolled_indices % vocab_size  # (k)\n",
        "\n",
        "            # Construct the the new top k hypotheses from these indices\n",
        "            top_k_hypotheses = torch.cat([hypotheses[prev_word_indices], next_word_indices.unsqueeze(1)],\n",
        "                                         dim=1)  # (k, step + 1)\n",
        "            \n",
        "            # Which of these new hypotheses are complete (reached <eos>)?\n",
        "            complete = next_word_indices == bpe_model.subword_to_id('<EOS>')  # (k), bool\n",
        "\n",
        "            # Set aside completed hypotheses and their scores normalized by their lengths\n",
        "            # For the length normalization formula, see\n",
        "            # \"Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\"\n",
        "            completed_hypotheses.extend(top_k_hypotheses[complete].tolist())\n",
        "            norm = math.pow(((5 + step) / (5 + 1)), length_norm_coefficient)\n",
        "            completed_hypotheses_scores.extend((top_k_hypotheses_scores[complete] / norm).tolist())\n",
        "\n",
        "            # Stop if we have completed enough hypotheses\n",
        "            if len(completed_hypotheses) >= n_completed_hypotheses:\n",
        "                break\n",
        "\n",
        "            # Else, continue with incomplete hypotheses\n",
        "            hypotheses = top_k_hypotheses[~complete]  # (s, step + 1)\n",
        "            hypotheses_scores = top_k_hypotheses_scores[~complete]  # (s)\n",
        "            hypotheses_lengths = torch.LongTensor(hypotheses.size(0) * [hypotheses.size(1)]).to(device)  # (s)\n",
        "\n",
        "            # Stop if things have been going on for too long\n",
        "            if step > 100:\n",
        "                break\n",
        "            step += 1\n",
        "        \n",
        "        # If there is not a single completed hypothesis, use partial hypotheses\n",
        "        if len(completed_hypotheses) == 0:\n",
        "            completed_hypotheses = hypotheses.tolist()\n",
        "            completed_hypotheses_scores = hypotheses_scores.tolist()\n",
        "        \n",
        "        # Decode the hypotheses\n",
        "        all_hypotheses = list()\n",
        "        for i, hypo in enumerate(completed_hypotheses):\n",
        "            h = bpe_model.decode(hypo, ignore_ids=[2, 3])[0]    # ignore <bos>, <eos>\n",
        "            all_hypotheses.append({\"hypothesis\": h, \"score\": completed_hypotheses_scores[i]})\n",
        "        \n",
        "        # Find the best scoring completed hypothesis\n",
        "        i = completed_hypotheses_scores.index(max(completed_hypotheses_scores))\n",
        "        best_hypothesis = all_hypotheses[i][\"hypothesis\"]\n",
        "\n",
        "        return tokens, best_hypothesis, all_hypotheses"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcOx1gdgiZOh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "784d6d5a-5615-4aba-88ed-f1899ad97de2"
      },
      "source": [
        "example_idx = 55\n",
        "\n",
        "src = vars(valid_dataset.examples[example_idx])['source']\n",
        "trg = vars(valid_dataset.examples[example_idx])['target']\n",
        "\n",
        "print(f'src = {bpe_model.decode(src, ignore_ids=[2, 3])[0]}')\n",
        "print(f'trg = {bpe_model.decode(trg, ignore_ids=[2,3])[0]}\\n')\n",
        "\n",
        "\n",
        "_, utterance, attention = generate_utterance_greedy(src, bpe_model, model, device)\n",
        "_, best_one, all_utterances = generate_utterance_beam(src, bpe_model, model, device)\n",
        "\n",
        "print(f'greedy generated utterance = {utterance}\\n')\n",
        "print(\"All beam generated utterances:\")\n",
        "print(\"------------------------------\")\n",
        "for i in all_utterances:\n",
        "    print(f'{i[\"hypothesis\"]}')\n"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = how do i find my own gmail accounts list?\n",
            "trg = how can you find all of your gmail accounts?\n",
            "\n",
            "greedy generated utterance = how do i find out my gmail account password online?\n",
            "\n",
            "All beam generated utterances:\n",
            "------------------------------\n",
            "how do i find my old gmail account back?\n",
            "how can i find my old gmail account back?\n",
            "how do i get a list of my gmail addresses?\n",
            "how do i find a list of my gmail addresses?\n",
            "how can i get a list of my gmail addresses?\n",
            "how do i get a complete list of my gmail addresses?\n",
            "what is the best way to get a person's gmail account?\n",
            "how do i find out my old gmail account password online?\n",
            "how do i get a complete list of my gmail account number?\n",
            "how can i find out my old gmail account password or my account?\n",
            "how do i find out my old gmail account password or my account?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwBtUydakMe_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "5fba4dda-e76e-400a-fe9c-48b12e89d807"
      },
      "source": [
        "src = \"What can I do after engineering?\"\n",
        "\n",
        "tokens, utterance, attention = generate_utterance_greedy(src, bpe_model, model, device)\n",
        "_, _, all_utterances = generate_utterance_beam(src, bpe_model, model, device)\n",
        "print(f'src = {src}\\n')\n",
        "print(f'Greedy generated utterance = {utterance}\\n')\n",
        "print(\"Beam generated utterances:\")\n",
        "print(\"------------------------------\")\n",
        "for i in all_utterances:\n",
        "    print(f'{i[\"hypothesis\"]}')"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = What can I do after engineering?\n",
            "\n",
            "Greedy generated utterance = what should i do after my btech?\n",
            "\n",
            "Beam generated utterances:\n",
            "------------------------------\n",
            "what should i do after engineering?\n",
            "what should i do after btech?\n",
            "what should i do after i die?\n",
            "what should i do after my btech?\n",
            "what is the best way to get into engineering?\n",
            "how do i know what i want to do?\n",
            "what are some of the best things to do after engineering?\n",
            "how do i know what i want to do after engineering?\n",
            "what is the best way to get a job after engineering?\n",
            "what should i do after i want to do after engineering?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpwB9Mn68sv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "f8cedae9-a24d-40ed-953c-f9caec71f9ed"
      },
      "source": [
        "src = \"How do I motivate myself?\"\n",
        "\n",
        "tokens, utterance, attention = generate_utterance_greedy(src, bpe_model, model, device)\n",
        "_, _, all_utterances = generate_utterance_beam(src, bpe_model, model, device)\n",
        "print(f'src = {src}\\n')\n",
        "print(f'Greedy generated utterance = {utterance}\\n')\n",
        "print(\"Beam generated utterances:\")\n",
        "print(\"------------------------------\")\n",
        "for i in all_utterances:\n",
        "    print(f'{i[\"hypothesis\"]}')\n"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = How do I motivate myself?\n",
            "\n",
            "Greedy generated utterance = how do i motivate myself to get into study?\n",
            "\n",
            "Beam generated utterances:\n",
            "------------------------------\n",
            "how can i motivate myself to do anything?\n",
            "how do i motivate myself to do anything?\n",
            "how do i motivate myself to be motivated?\n",
            "how do i motivate myself to get into study?\n",
            "how can i motivate myself to get into study?\n",
            "how can i motivate myself to wake up early?\n",
            "how do i motivate myself to wake up early?\n",
            "how can i motivate myself to keep myself motivated myself?\n",
            "how do i motivate myself to keep myself motivated myself?\n",
            "what is the best way to get yourself to be motivated?\n",
            "how can i motivate myself to keep myself motivated to study?\n",
            "how do i motivate myself to keep myself motivated to study?\n",
            "what is the best way to get yourself to get motivated?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIRDfhZ1uwTi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}