{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic Machine Translation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPnsilYbuCKwouJJbrOC30",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/100-Days-of-NLP/blob/applications%2Fgeneration/applications/generation/Basic%20Machine%20Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75ZzjZ9aYR1Z",
        "colab_type": "text"
      },
      "source": [
        "# Machine Translation\n",
        "\n",
        "Machine Translation (MT) is the task of automatically converting one natural language into another, preserving the meaning of the input text, and producing fluent text in the output language. Ideally, a source language sequence is translated into target language sequence. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVoZ9ithYWwc",
        "colab_type": "text"
      },
      "source": [
        "The most common sequence-to-sequence (seq2seq) models are encoder-decoder models, which commonly use a recurrent neural network (RNN) to encode the source (input) sentence into a single vector.\n",
        "\n",
        "![overview](https://drive.google.com/uc?id=1TkFl1a68iOCfRaW6QGRBs5Jf4SHLqk9j)\n",
        "\n",
        "\n",
        "In this notebook, we'll refer to this single vector as a context vector. We can think of the context vector as being an abstract representation of the entire input sentence. This vector is then decoded by a second RNN which learns to output the target (output) sentence by generating it one word at a time.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1AZ8D6qSu_rMWOHa4STtLaPM4cKvTdZvL)\n",
        "\n",
        "We will be working on German to English Translation. The dataset can be found [here](https://pytorch.org/text/datasets.html#multi30k)\n",
        "\n",
        "\n",
        "### Resources\n",
        "\n",
        "- [Unreasonable effectiveness of RNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "- [Ben Trevett Seq2Seq](https://github.com/bentrevett/pytorch-seq2seq)\n",
        "- [Sequence to Sequence Learning with Neural Networks paper](https://arxiv.org/abs/1409.3215)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ZEQwWZyQJE",
        "colab_type": "text"
      },
      "source": [
        "## Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh4vEg955kcp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import time\n",
        "import spacy\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi5fkhrD6Vuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 64\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEWR3Z7YzAz6",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "For tokenizing the english and german sentences, we will be using [`spacy`](https://spacy.io/api/tokenizer/).\n",
        "\n",
        "Download the models corresponding to the language.\n",
        "- German - **`de`**\n",
        "- English - **`en`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk3vKT1D6_xW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "783c27a1-57ad-4d02-9533-d9518d3b3bbd"
      },
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.3.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 827kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (46.3.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907056 sha256=3219c473175965a26972a63f5d2fc6024eeeda5b0f65dd3708cc3b16ea684c9e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-moor_itt/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWZZrP0GzoXK",
        "colab_type": "text"
      },
      "source": [
        "Load the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT6SIIy17UTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_en = spacy.load('en')\n",
        "spacy_de = spacy.load('de')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALrjGvF4zsni",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4Y3tAEB7KpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz3AfDoXz3NM",
        "colab_type": "text"
      },
      "source": [
        "## Field\n",
        "\n",
        "For the basic usage of Field, refer to my other notebooks [here](https://github.com/graviraja/100-Days-of-NLP/blob/master/applications/classification/Simple%20Sentiment%20Analysis.ipynb) and [here](https://github.com/graviraja/100-Days-of-NLP/blob/master/applications/classification/Improved%20Sentiment%20Analysis.ipynb).\n",
        "\n",
        "However, in Machine Translation, the source language and target languages are different.\n",
        "\n",
        "`Field` defines how the data should be processed.\n",
        "\n",
        "Since we are tokenizing using spacy, we can pass our tokenizer method to the argument **`tokenizer`**\n",
        "\n",
        "In order to indicate the starting and ending of a sentence, we can `init_token` as **`<sos>`** and `eos_token` as **`<eos>`**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ccYws17sew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC = Field(\n",
        "        tokenize=tokenize_de,\n",
        "        init_token='<sos>',\n",
        "        eos_token='<eos>',\n",
        "        lower=True)\n",
        "\n",
        "TRG = Field(\n",
        "        tokenize=tokenize_en,\n",
        "        init_token='<sos>',\n",
        "        eos_token='<eos>',\n",
        "        lower=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkGhgHC71ivo",
        "colab_type": "text"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We will be using [Multi30K dataset](https://pytorch.org/text/datasets.html#multi30k). Torchtext provides support for multi open datasets. This is a dataset with ~31,000 parallel English, German and French sentences. \n",
        "\n",
        "**`exts`** specifies which languages to use as the source and target (source goes first) and **`fields`** specifies which field to use for the source and target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQWRqoIS8Uwb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a737844d-fee6-4856-f87c-e460959d3a5a"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:02<00:00, 532kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 171kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 156kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdBLbXi18ffw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "313d3eec-6c72-459e-f50f-74ca98e0d698"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d1rF8IV8h94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "0188e5ab-c4ac-4d27-d92a-4c5dd83fc2ce"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOG7uydt2Y_3",
        "colab_type": "text"
      },
      "source": [
        "## Vocabulary\n",
        "\n",
        "Using the **`min_freq`** argument, we only allow tokens that appear at least 2 times to appear in our vocabulary. Tokens that appear only once are converted into an **`<unk>`** (unknown) token.\n",
        "\n",
        "It is important to note that our vocabulary should only be built from the training set and not the validation/test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LrLIXdV8qhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3Ua6lI39L99",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e1429b92-a0a9-406f-de08-7dfc53c18fe3"
      },
      "source": [
        "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in source (de) vocabulary: 7855\n",
            "Unique tokens in target (en) vocabulary: 5893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sTbic6A9PJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c5c49ee6-42f8-4f1a-91de-7bd33f10aab0"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7npvddQ3M1G",
        "colab_type": "text"
      },
      "source": [
        "## Iterators\n",
        "\n",
        "\n",
        "The final step of preparing the data is to create the iterators. These can be iterated on to return a batch of data which will have a src attribute (the PyTorch tensors containing a batch of numericalized source sentences) and a trg attribute (the PyTorch tensors containing a batch of numericalized target sentences). \n",
        "\n",
        "\n",
        "When we get a batch of examples using an iterator we need to make sure that all of the source sentences are padded to the same length, the same with the target sentences. Luckily, TorchText iterators handle this for us!\n",
        "\n",
        "We use a BucketIterator instead of the standard Iterator as it creates batches in such a way that it minimizes the amount of padding in both the source and target sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaHxxiip9drj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n-32ejuGqYN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9fb7f996-0e97-4a0b-d965-df96be1b53df"
      },
      "source": [
        "# sample checking\n",
        "temp = next(iter(train_iterator))\n",
        "temp.src.shape, temp.trg.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([28, 64]), torch.Size([28, 64]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGSz6S9KZ6zs",
        "colab_type": "text"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "\n",
        "![encoder](https://drive.google.com/uc?id=1-glq5zaySAlL5HpRlA9v3Zj1JXb1kUfu)\n",
        "\n",
        "The encoder we are using is a 2 layer LSTM. The paper we are implementing uses a 4-layer LSTM, but in the interest of training time we cut this down to 2-layers.\n",
        "\n",
        "For more on LSTM and hidden states explaination refer to my post on RNN [here](https://github.com/graviraja/100-Days-of-NLP/blob/applications/generation/architectures/RNN.ipynb)\n",
        "\n",
        "Once the source sentence is passed through the LSTM, the final step's hidden state in all the layers will be used for decoding.\n",
        "\n",
        "This final hidden state is called `context vector`, which represents the encoding of the source sentence. This will be used as the initial hidden state for decoder. \n",
        "\n",
        "> *Note: In order for this to work, the number of layers in encoder and decoder must be same, if not then some extra strategy needs to be applied. For example, if the encoder is 1 layer and decoder is 2 layers means, then encoder final hidden state needs to replicated and sent as initial state to decoder*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHpgQx3j9s8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, num_layers=n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, src):\n",
        "        # src => [seq_len, batch_size]\n",
        "\n",
        "        embedded = self.embedding(src)\n",
        "        embedded = self.dropout(embedded)\n",
        "        # embedded => [seq_len, batch_size, emb_dim]\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        # outputs => [seq_len, batch_size, hid_dim]\n",
        "        # hidden => [num_layers * num_dir, batch_size, hid_dim] => [2, batch_size, hid_dim]\n",
        "        # cell => [num_layers * num_dir, batch_size, hid_dim] => [2, batch_size, hid_dim]\n",
        "\n",
        "        return hidden, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWGxQiEWb5E8",
        "colab_type": "text"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "![](https://drive.google.com/uc?id=1YomvoraK5OLyJ_UO40T8K2Am6OOAaGuo)\n",
        "\n",
        "The decoder we are using is a 2 layer LSTM like encoder. The Decoder class does a single step of decoding, i.e. it ouputs single token per time-step. \n",
        "\n",
        "The initial hidden and cell states to the decoder are context vectors $z^1, z^2$, which are the final hidden and cell states of the encoder from the same layer.\n",
        "\n",
        "\n",
        "As we are only decoding one token at a time, the input tokens will always have a sequence length of 1. We unsqueeze the input tokens to add a sentence length dimension of 1. Then it is passed through the RNN. There is an additional Linear layer, used to make the predictions from the top layer hidden state.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDOtlZxCBn9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, num_layers=n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hidden_dim, input_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, input, hidden, cell):\n",
        "        # input => [batch_size]\n",
        "        # hidden => [num_layers, batch_size, hid_dim]\n",
        "        # cell => [num_layers, batch_size, hid_dim]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "        # input => [1, batch_size]\n",
        "\n",
        "        embedded = self.embedding(input)\n",
        "        embedded = self.dropout(embedded)\n",
        "        # embedded => [1, batch_size, emb_dim]\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        # seq_len and num_directions will always be 1 in decoder\n",
        "        # output => [seq_len, batch_size, hid_dim]\n",
        "        #        => [1, batch_size, hid_dim]\n",
        "        # hidden => [num_layers, batch_size, hid_dim]\n",
        "        # cell   => [num_layers, batch_size, hid_dim]\n",
        "\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        # prediction => [batch_size, input_dim]\n",
        "\n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXobr06Bc2Xh",
        "colab_type": "text"
      },
      "source": [
        "## Seq2Seq\n",
        "\n",
        "For the final part of the implemenetation, we'll implement the seq2seq model. This will handle:\n",
        "\n",
        "- receiving the input/source sentence\n",
        "- using the encoder to produce the context vectors\n",
        "- using the decoder to produce the predicted output/target sentence\n",
        "\n",
        "Our full model will look like this:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1AZ8D6qSu_rMWOHa4STtLaPM4cKvTdZvL)\n",
        "\n",
        "\n",
        "The forward method takes the source sentence, target sentence and a teacher-forcing ratio.\n",
        "\n",
        "The teacher forcing ratio is used when training our model. When decoding, at each time-step we will predict what the next token in the target sequence will be from the previous tokens decoded, $\\hat{y}_{t+1}=f(s_t^L)$. With probability equal to the teaching forcing ratio (teacher_forcing_ratio) we will use the actual ground-truth next token in the sequence as the input to the decoder during the next time-step. \n",
        "\n",
        "However, with probability 1 - teacher_forcing_ratio, we will use the token that the model predicted as the next input to the model, even if it doesn't match the actual next token in the sequence.\n",
        "\n",
        "The first input to the decoder is the start of sequence `<sos>` token. As our trg tensor already has the `<sos>` token appended we get our $y_1$ by slicing into it. We know how long our target sentences should be (max_len), so we loop that many times. The last token input into the decoder is the one before the `<eos>` token "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUu68eFSFOtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "    \n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src => [src_len, batch_size]\n",
        "        # trg => [trg_len, batch_size]\n",
        "\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.input_dim\n",
        "\n",
        "        # outputs: to store the predictions of the decoder\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # send the initial input to decoder as <sos>\n",
        "        dec_inp = trg[0, :]\n",
        "\n",
        "        # loop till the maximum target length\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(dec_inp, hidden, cell)\n",
        "            outputs[t] = output\n",
        "\n",
        "            # to decide whether to use the predicted as input or actual ground truth for the next time step\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            # pick the top one: greedy\n",
        "            # other strategy is to sample from the top_k distribution\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            dec_inp = trg[t] if teacher_force else top1\n",
        "        return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD_W5oZ1HCxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_iVhFJyHsfy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "85bd2c99-156f-4c73-a870-ac450f963ffb"
      },
      "source": [
        "# initial the weights of the model with uniform distribution between -0.08 and 0.08 [stated in paper]\n",
        "def init_weights(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7855, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPJJz6seIdiZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dcbe7036-bf5d-46b8-ec58-ddd07817845b"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model)} trainable parameters')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 13899013 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HwkiehmI6n1",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer & Criterion\n",
        "\n",
        "We use the **`Adam`** optimizer.\n",
        "\n",
        "**`CrossEntropyLoss`**: This criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class. It is useful when training a classification problem with C classes. Ignore the `<pad>` index as it does not contribute to loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad62nFPRIxii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXvqt3ukI2xm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvGr2_qdJK6d",
        "colab_type": "text"
      },
      "source": [
        "## Train\n",
        "\n",
        "At each iteration:\n",
        "\n",
        "- get the source and target sentences from the batch, $X$ and $Y$\n",
        "- zero the gradients calculated from the last batch\n",
        "- feed the source and target into the model to get the output, $\\hat{Y}$\n",
        "- as the loss function only works on 2d inputs with 1d targets we need to flatten each of them with .view\n",
        "- we slice off the first column of the output and target tensors (`<sos>` not used)\n",
        "- calculate the gradients with loss.backward()\n",
        "- clip the gradients to prevent them from exploding\n",
        "- update the parameters of our model by doing an optimizer step\n",
        "- update the loss\n",
        "\n",
        "Finally, we return the loss that is averaged over all batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_OZsBOtJHdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, criterion, optimizer, clip):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # keep the model in train mode\n",
        "    model.train()\n",
        "\n",
        "    # iterate over train data\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        # src => [seq_len, batch_size]\n",
        "        # trg => [seq_len, batch_size]\n",
        "\n",
        "        # zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        output = model(src, trg)\n",
        "\n",
        "        # reshaping the output to make it compatible to cal. loss\n",
        "        # can also do without reshaping\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient clipping \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        # update the parameters of the model\n",
        "        optimizer.step()\n",
        "\n",
        "        # update the loss\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    return epoch_loss / len(iterator)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWkd_fLIJMmk",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "Similar to training loop, without backward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwzaxGLtMeuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    # keep the model in eval mode\n",
        "    model.eval()\n",
        "    \n",
        "    # do not calculate gradients\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # iterate over the data\n",
        "        for batch in iterator:\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            # src => [seq_len, batch_size]\n",
        "            # trg => [seq_len, batch_size]\n",
        "\n",
        "            # forward pass\n",
        "            # make sure the teacher_forcing_ratio is 0 in eval\n",
        "            output = model(src, trg, 0)\n",
        "\n",
        "            # reshaping for loss calculation\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            # loss\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            # update loss\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNOuCC56NFuE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = elapsed_time - (elapsed_mins * 60)\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDGBkqkgJPWJ",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zBFCJJ8NYCn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "97b77ae5-ec26-40fc-a4b9-d5d1cc455660"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, criterion, optimizer, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "    \n",
        "    print(f\"Epoch {epoch + 1} | Time: {epoch_mins}m {epoch_secs}s\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} |\")\n",
        "    print(f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f} |\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 | Time: 0m 37.81912350654602s\n",
            "\tTrain Loss: 4.821 | Train PPL: 124.027 |\n",
            "\tValid Loss: 4.775 | Valid PPL: 118.487 |\n",
            "Epoch 2 | Time: 0m 37.0620002746582s\n",
            "\tTrain Loss: 4.154 | Train PPL:  63.693 |\n",
            "\tValid Loss: 4.468 | Valid PPL:  87.161 |\n",
            "Epoch 3 | Time: 0m 37.08384919166565s\n",
            "\tTrain Loss: 3.860 | Train PPL:  47.464 |\n",
            "\tValid Loss: 4.252 | Valid PPL:  70.266 |\n",
            "Epoch 4 | Time: 0m 37.10718059539795s\n",
            "\tTrain Loss: 3.643 | Train PPL:  38.214 |\n",
            "\tValid Loss: 4.111 | Valid PPL:  61.026 |\n",
            "Epoch 5 | Time: 0m 37.17174744606018s\n",
            "\tTrain Loss: 3.420 | Train PPL:  30.576 |\n",
            "\tValid Loss: 3.987 | Valid PPL:  53.877 |\n",
            "Epoch 6 | Time: 0m 37.099289417266846s\n",
            "\tTrain Loss: 3.232 | Train PPL:  25.334 |\n",
            "\tValid Loss: 3.862 | Valid PPL:  47.541 |\n",
            "Epoch 7 | Time: 0m 37.21440601348877s\n",
            "\tTrain Loss: 3.057 | Train PPL:  21.260 |\n",
            "\tValid Loss: 3.701 | Valid PPL:  40.488 |\n",
            "Epoch 8 | Time: 0m 37.18330144882202s\n",
            "\tTrain Loss: 2.901 | Train PPL:  18.190 |\n",
            "\tValid Loss: 3.773 | Valid PPL:  43.524 |\n",
            "Epoch 9 | Time: 0m 36.97871255874634s\n",
            "\tTrain Loss: 2.781 | Train PPL:  16.134 |\n",
            "\tValid Loss: 3.611 | Valid PPL:  36.998 |\n",
            "Epoch 10 | Time: 0m 37.153504371643066s\n",
            "\tTrain Loss: 2.653 | Train PPL:  14.192 |\n",
            "\tValid Loss: 3.623 | Valid PPL:  37.456 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu5ztMNuJRPF",
        "colab_type": "text"
      },
      "source": [
        "## Testing\n",
        "\n",
        "Load the pre-trained model and evaluate on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWXE6ZpWOraz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d0517e81-29d5-4d89-d6f6-23d80a2440e6"
      },
      "source": [
        "model.load_state_dict(torch.load('model.pt'))\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "print(f\"\\tTest Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTest Loss: 3.602 | Test PPL:  36.658 |\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}