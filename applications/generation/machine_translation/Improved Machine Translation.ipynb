{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Improved Machine Translation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKcp/pXMwE0DkfR9D9rds9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/100-Days-of-NLP/blob/applications%2Fgeneration/applications/generation/Improved%20Machine%20Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RQ1D3-NiwmD",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we will implement the model from [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf) paper. This model will have better test perplexity than the [Basic MT model](https://github.com/graviraja/100-Days-of-NLP/blob/master/applications/generation/Basic%20Machine%20Translation.ipynb) whilst using only single layer RNN in both encoder and decoder.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1zPgyT1xZ0g37OAbFM6HWSI1mxfOUxTbJ)\n",
        "\n",
        "### Resources\n",
        "- [Unreasonable effectiveness of RNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "- [Ben Trevett Seq2Seq](https://github.com/bentrevett/pytorch-seq2seq)\n",
        "- [Multi30K dataset](https://pytorch.org/text/datasets.html#multi30k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtRvakhckP05",
        "colab_type": "text"
      },
      "source": [
        "## Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d3vqgVcXffr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.datasets import Multi30k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFScJRfWkXQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGSVzSi3kYIX",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "For tokenizing the english and german sentences, we will be using spacy.\n",
        "\n",
        "Download the models corresponding to the language.\n",
        "\n",
        "- German - **`de`**\n",
        "- English - **`en`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc_9ARCtaMno",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc63730c-bebf-4840-ca36-647fbe060bcb"
      },
      "source": [
        "!python -m spacy download de\n",
        "!python -m spacy download en"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 506kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (46.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907056 sha256=60e3728a80341be9b598fd61d6a40c4bcd343028dba35e416623cea0735b3cb8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ypt1etfx/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.3.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAA7F8pVkjSN",
        "colab_type": "text"
      },
      "source": [
        "Load the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-S1ct8MaS1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenize_en = spacy.load('en')\n",
        "tokenize_de = spacy.load('de')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUQGpIEkkkYi",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GA3aAxYaZpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizer_en(text):\n",
        "    return [tok.text for tok in tokenize_en.tokenizer(text)]\n",
        "\n",
        "def tokenizer_de(text):\n",
        "    return [tok.text for tok in tokenize_de.tokenizer(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmPc-xTMkoTc",
        "colab_type": "text"
      },
      "source": [
        "## Field\n",
        "\n",
        "Field defines how the data should be processed.\n",
        "\n",
        "Since we are tokenizing using spacy, we can pass our tokenizer method to the argument **tokenizer**\n",
        "\n",
        "In order to indicate the starting and ending of a sentence, we can init_token as **`<sos>`** and eos_token as **`<eos>`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbrNGCi0a4or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC = Field(\n",
        "        tokenize=tokenizer_de,\n",
        "        init_token='<sos>',\n",
        "        eos_token='<eos>',\n",
        "        lower=True\n",
        ")\n",
        "\n",
        "TRG = Field(\n",
        "        tokenize=tokenizer_en,\n",
        "        init_token='<sos>',\n",
        "        eos_token='<eos>',\n",
        "        lower=True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7azLa7OAk7_C",
        "colab_type": "text"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We will be using Multi30K dataset. Torchtext provides support for multi open datasets. This is a dataset with ~31,000 parallel English, German and French sentences.\n",
        "\n",
        "exts specifies which languages to use as the source and target (source goes first) and fields specifies which field to use for the source and target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU1lIjaFbM-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "0e0ff218-a42c-4403-c7c4-7c1187102767"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:04<00:00, 248kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 80.0kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 73.4kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EjGN3xobY0S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4ad7850c-9569-4ad7-89db-a50ef870c548"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 29000\n",
            "Number of validation examples: 1014\n",
            "Number of testing examples: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apqxzi2tbluY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "dae7d02b-ae3f-4be4-e2e5-ffe6a6fc878e"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'src': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PalT71m9ouSO",
        "colab_type": "text"
      },
      "source": [
        "## Vocabulary\n",
        "\n",
        "Using the **`min_freq argument`**, we only allow tokens that appear at least 2 times to appear in our vocabulary. Tokens that appear only once are converted into an **`<unk>`** (unknown) token.\n",
        "\n",
        "It is important to note that our vocabulary should only be built from the training set and not the validation/test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBDHpHGkbo52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq=2)\n",
        "TRG.build_vocab(valid_data, min_freq=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIdHK6D0b8Hj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3b56645a-a60e-4677-86cc-3c2b43653cb9"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCqiyFmzcEhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkPK0hIuo4cL",
        "colab_type": "text"
      },
      "source": [
        "## Iterators\n",
        "\n",
        "The final step of preparing the data is to create the iterators. These can be iterated on to return a batch of data which will have a src attribute (the PyTorch tensors containing a batch of numericalized source sentences) and a trg attribute (the PyTorch tensors containing a batch of numericalized target sentences).\n",
        "\n",
        "When we get a batch of examples using an iterator we need to make sure that all of the source sentences are padded to the same length, the same with the target sentences. Luckily, TorchText iterators handle this for us!\n",
        "\n",
        "We use a BucketIterator instead of the standard Iterator as it creates batches in such a way that it minimizes the amount of padding in both the source and target sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhpnRDGYb0un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FkhyYoscS72",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6ccf3897-a926-404a-e6c3-df1c14fe3351"
      },
      "source": [
        "# sample checking\n",
        "temp = next(iter(train_iterator))\n",
        "temp.src.shape, temp.trg.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([25, 64]), torch.Size([29, 64]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-55G0i9o_Ik",
        "colab_type": "text"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "![](https://drive.google.com/uc?id=1FjGJ0Fdya_Eq2HkSS27sgyG-n_mJFrQI)\n",
        "\n",
        "The encoder we are using is a 1 layer GRU.\n",
        "\n",
        "For more on GRU and hidden states explaination refer to my post on RNN [here](https://github.com/graviraja/100-Days-of-NLP/blob/applications/generation/architectures/RNN.ipynb)\n",
        "\n",
        "Once the source sentence is passed through the GRU, the final step's hidden state in all the layers will be used for decoding.\n",
        "\n",
        "This final hidden state is called context vector, which represents the encoding of the source sentence. This will be used as the initial hidden state for decoder and also as input to decoder rnn along with decoder embedded input. This context vector will also be provided as input to Classifier along with decoder rnn output. \n",
        "\n",
        "> Note: In order for this to work, the number of layers in encoder and decoder must be same. Here we will be using 1 layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNFgdV0YcbWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, emb_size, hidden_size, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, emb_size)\n",
        "        self.rnn = nn.GRU(emb_size, hidden_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        # input => [seq_len, batch_size]\n",
        "\n",
        "        embedded = self.embedding(input)\n",
        "        embedded = self.dropout(embedded)\n",
        "        # embedded => [seq_len, batch_size, emb_dim]\n",
        "\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        # output => [seq_len, batch_size, hidden_dim]\n",
        "        # hidden => [num_layers * num_dir, batch_size, hidden_dim] => [1, batch_size, hidden_dim]\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XYDheScpBWN",
        "colab_type": "text"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "![](https://drive.google.com/uc?id=1b4aQXhMU11bE3hvLyCHZUFViqNI_6aQO)\n",
        "\n",
        "The decoder is where the implementation differs significantly from the previous model and we alleviate some of the information compression.\n",
        "\n",
        "In the [previous implementation](https://github.com/graviraja/100-Days-of-NLP/blob/master/applications/generation/Basic%20Machine%20Translation.ipynb), the context vector is used only as the initial hidden state to decoder. i.e Only during the decoding of the first token in the target, the source information is available. For the rest of the tokens, the propagated states will be used.\n",
        "\n",
        "However, here we provide the context vector for every decoding step, as an input to decoder and as an input to classifier. This helps the decoder to have more information about the tokens decoded so far and also the source sentence information. \n",
        "\n",
        "The addition of $z^1$ to the classifier also means this layer can directly see what the token is, without having to get this information from the hidden state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTcfl_O_dVc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim + hidden_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(emb_dim + (hidden_dim * 2), input_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, input, hidden, context):\n",
        "        # input => [batch_size]\n",
        "        # hidden => [1, batch_size, hidden_dim]\n",
        "        # context => [1, batch_size, hidden_dim]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "        # input => [1, batch_size]\n",
        "\n",
        "        embedded = self.embedding(input)\n",
        "        embedded = self.dropout(embedded)\n",
        "        # embedded => [1, batch_size, emb_dim]\n",
        "\n",
        "        combined = torch.cat((embedded, context), dim=-1)\n",
        "        # combined => [1, batch_size, emb_dim + hid_dim]\n",
        "\n",
        "        output, hidden = self.rnn(combined, hidden)\n",
        "        # output => [1, batch_size, hid_dim]\n",
        "        # hidden => [1, batch_size, hid_dim]\n",
        "\n",
        "        out_combined = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim=-1)\n",
        "        # out_combined => [batch_size, emb_dim + 2 * hid_dim ]\n",
        "\n",
        "        logits = self.fc(self.dropout(out_combined))\n",
        "        # logits => [batch_size, input_dim]\n",
        "\n",
        "        return logits, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW7jpd7-pDZ1",
        "colab_type": "text"
      },
      "source": [
        "## Seq2Seq\n",
        "\n",
        "For the final part of the implemenetation, we'll implement the seq2seq model. This will handle:\n",
        "\n",
        "- receiving the input/source sentence\n",
        "- using the encoder to produce the context vectors\n",
        "- using the decoder to produce the predicted output/target sentence\n",
        "\n",
        "Our full model will look like this:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1zPgyT1xZ0g37OAbFM6HWSI1mxfOUxTbJ)\n",
        "\n",
        "\n",
        "The forward method takes the source sentence, target sentence and a teacher-forcing ratio.\n",
        "\n",
        "The teacher forcing ratio is used when training our model. \n",
        "\n",
        "When decoding, at each time-step we will predict what the next token in the target sequence will be from the previous tokens decoded. With probability equal to the teaching forcing ratio (teacher_forcing_ratio) we will use the actual ground-truth next token in the sequence as the input to the decoder during the next time-step.\n",
        "\n",
        "However, with probability 1 - teacher_forcing_ratio, we will use the token that the model predicted as the next input to the model, even if it doesn't match the actual next token in the sequence.\n",
        "\n",
        "The first input to the decoder is the start of sequence `<sos>` token. As our trg tensor already has the `<sos>` token appended we get our $y_1$ by slicing into it. We know how long our target sentences should be (max_len), so we loop that many times. The last token input into the decoder is the one before the `<eos>` token"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5w3bV1xqIq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "    \n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        # src => [seq_len, batch_size]\n",
        "        # trg => [seq_len, batch_size]\n",
        "\n",
        "        batch_size = src.shape[-1]\n",
        "        max_trg_length = trg.shape[0]\n",
        "        output_dim = self.decoder.input_dim\n",
        "\n",
        "        # outputs: to store the predictions of the decoder\n",
        "        outputs = torch.zeros(max_trg_length, batch_size, output_dim).to(self.device)\n",
        "\n",
        "        context = self.encoder(src)\n",
        "        hidden = context\n",
        "\n",
        "        # send the initial input to decoder as <sos>\n",
        "        dec_inp = trg[0, :]\n",
        "\n",
        "        # loop till the maximum target length\n",
        "        for i in range(1, max_trg_length):\n",
        "            output, hidden = self.decoder(dec_inp, hidden, context)\n",
        "            outputs[i] = output\n",
        "            # to decide whether to use the predicted as input or actual ground truth for the next time step\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            # pick the top one: greedy\n",
        "            # other strategy is to sample from the top_k distribution\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            dec_inp = trg[i] if teacher_force else top1\n",
        "        return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C46_GfQ-WtTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HIDDEN_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HIDDEN_DIM, DEC_DROPOUT)\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovU9wWE2ZG3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "d0677d9a-fa46-4b29-af17-0a0af6ad677d"
      },
      "source": [
        "def init_weights(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7855, 256)\n",
              "    (rnn): GRU(256, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(844, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc): Linear(in_features=1280, out_features=844, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBIp_Y5MZZT4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "99e40210-cf56-417a-f4fc-8fc5e90c70d0"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"The model has {count_parameters(model)} trainable parameters\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 6459980 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbkx17BrpKlk",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer & Loss Criterion\n",
        "\n",
        "We use the Adam optimizer.\n",
        "\n",
        "**`CrossEntropyLoss`**: This criterion combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class. It is useful when training a classification problem with C classes. Ignore the **`<pad>`** index as it does not contribute to loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8bWnIl5XYi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG.vocab.stoi[TRG.pad_token])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwEFN9tPpXeZ",
        "colab_type": "text"
      },
      "source": [
        "## Train\n",
        "\n",
        "At each iteration:\n",
        "\n",
        "- get the source and target sentences from the batch, $X$ and $Y$\n",
        "- zero the gradients calculated from the last batch\n",
        "- feed the source and target into the model to get the output, $\\hat{Y}$\n",
        "- as the loss function only works on 2d inputs with 1d targets we need to flatten each of them with .view\n",
        "we slice off the first column of the output and target tensors (`<sos>` not used)\n",
        "- calculate the gradients with loss.backward()\n",
        "- clip the gradients to prevent them from exploding\n",
        "- update the parameters of our model by doing an optimizer step\n",
        "- update the loss\n",
        "\n",
        "Finally, we return the loss that is averaged over all batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfyucHVDZs8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, criterion, optimizer, clip):\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for batch in iterator:\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        loss = criterion(output[1:].view(-1, output_dim), trg[1:].view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jfy4nakEpl5-",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate\n",
        "\n",
        "Similar to training loop, without backward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iUXtbDqb5Du",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg)\n",
        "            output_dim = output.shape[-1]\n",
        "            loss = criterion(output[1:].view(-1, output_dim), trg[1:].view(-1))\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynVh6W0AdDiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = elapsed_time - elapsed_mins\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8n5njCippbA",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWQX7VfFc2VL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "d87b2dad-2589-450d-caed-477983fb00a8"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_iterator, criterion, optimizer, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "    \n",
        "    print(f\"Epoch: {epoch + 1} | Time: {epoch_mins}m {epoch_secs}s\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
        "    print(f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}\")\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | Time: 0m 27.00088930130005s\n",
            "\tTrain Loss: 3.897 | Train PPL:  49.267\n",
            "\tValid Loss: 3.737 | Valid PPL:  41.985\n",
            "Epoch: 2 | Time: 0m 26.967023611068726s\n",
            "\tTrain Loss: 3.196 | Train PPL:  24.427\n",
            "\tValid Loss: 3.084 | Valid PPL:  21.835\n",
            "Epoch: 3 | Time: 0m 26.914294719696045s\n",
            "\tTrain Loss: 2.811 | Train PPL:  16.633\n",
            "\tValid Loss: 2.770 | Valid PPL:  15.952\n",
            "Epoch: 4 | Time: 0m 26.982502937316895s\n",
            "\tTrain Loss: 2.553 | Train PPL:  12.848\n",
            "\tValid Loss: 2.489 | Valid PPL:  12.052\n",
            "Epoch: 5 | Time: 0m 26.98589015007019s\n",
            "\tTrain Loss: 2.381 | Train PPL:  10.812\n",
            "\tValid Loss: 2.314 | Valid PPL:  10.111\n",
            "Epoch: 6 | Time: 0m 26.743107557296753s\n",
            "\tTrain Loss: 2.245 | Train PPL:   9.440\n",
            "\tValid Loss: 2.246 | Valid PPL:   9.454\n",
            "Epoch: 7 | Time: 0m 26.547712087631226s\n",
            "\tTrain Loss: 2.145 | Train PPL:   8.538\n",
            "\tValid Loss: 2.182 | Valid PPL:   8.863\n",
            "Epoch: 8 | Time: 0m 26.72164821624756s\n",
            "\tTrain Loss: 2.042 | Train PPL:   7.709\n",
            "\tValid Loss: 2.159 | Valid PPL:   8.667\n",
            "Epoch: 9 | Time: 0m 26.534064292907715s\n",
            "\tTrain Loss: 2.010 | Train PPL:   7.462\n",
            "\tValid Loss: 2.134 | Valid PPL:   8.449\n",
            "Epoch: 10 | Time: 0m 26.487512826919556s\n",
            "\tTrain Loss: 1.927 | Train PPL:   6.872\n",
            "\tValid Loss: 2.051 | Valid PPL:   7.773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqUeIFOYprw_",
        "colab_type": "text"
      },
      "source": [
        "## Testing\n",
        "\n",
        "Load the pre-trained model and evaluate on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jbUjwZpeTjW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f391343d-fe7a-43c9-a284-2781655a8b5b"
      },
      "source": [
        "model.load_state_dict(torch.load('model.pt'))\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "print(f\"Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.952 | Test PPL:   7.041\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}