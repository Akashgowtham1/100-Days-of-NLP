{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Document Classification with HAN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN8b3bLe3+D6GYDCngogjYf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/100-Days-of-NLP/blob/applications%2Fclassification/applications/classification/Document%20Classification%20with%20HAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JVHsKkCTODb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "e601e353-0efd-48e0-f0a6-43bc2a0da6ac"
      },
      "source": [
        "!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-06 17:22:56--  http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Musical_Instruments_5.json.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2460495 (2.3M) [application/x-gzip]\n",
            "Saving to: ‘reviews_Musical_Instruments_5.json.gz’\n",
            "\n",
            "reviews_Musical_Ins 100%[===================>]   2.35M   832KB/s    in 2.9s    \n",
            "\n",
            "2020-06-06 17:22:59 (832 KB/s) - ‘reviews_Musical_Instruments_5.json.gz’ saved [2460495/2460495]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhM4lAUyTTus",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2bd31c2e-d3a9-45a7-9ac7-7e8e7e074b00"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reviews_Musical_Instruments_5.json.gz  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28mb_5jMTcd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gunzip reviews_Musical_Instruments_5.json.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ahn_MYFCTyEI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "146826e3-12e4-487f-c7af-160d4ebfd505"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reviews_Musical_Instruments_5.json  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrUq9qnFcVJP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d8226631-1639-4a0a-911c-98fe094c7b5f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnEnmahjSYmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from fastai.text import Tokenizer, Vocab\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiP09GF-xbJw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "115e737f-e313-4ecf-e70b-3aba17886afe"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "datafile = 'reviews_Musical_Instruments_5.json'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAb8uNyFxhn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(datafile, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for line in lines[:-1]:\n",
        "    data = json.loads(line)\n",
        "    texts.append(data[\"reviewText\"].lower())\n",
        "    labels.append(int(data[\"overall\"]) - 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyce9W30xiyP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7675f8c4-7f5b-469d-d375-2d6f54519d8b"
      },
      "source": [
        "train_data, valid_data, train_label, valid_label = train_test_split(\n",
        "        texts, labels, train_size=0.8, random_state=1\n",
        "    )\n",
        "\n",
        "print(f\"Number of training data examples: {len(train_label)}\")\n",
        "print(f\"Number of validation data examples: {len(valid_label)}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training data examples: 8208\n",
            "Number of validation data examples: 2053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mY_Jqqupxpvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_label = np.array(train_label, dtype=\"int32\")\n",
        "valid_label = np.array(valid_label, dtype=\"int32\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUE3E_Xhxr-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HANPreprocessor:\n",
        "    \"\"\"\n",
        "    Preprocessor to prepare the data for Hierarchical Attention Networks.\n",
        "    It will tokenize a document into sentences and sentences into tokens\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_vocab, min_freq, percentile, tokenizer):\n",
        "        self.max_vocab = max_vocab\n",
        "        self.min_freq = min_freq\n",
        "        self.percentile = percentile\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = None\n",
        "\n",
        "    def _make_sentences(self, texts):\n",
        "        texts_sents = [sent_tokenize(text) for text in texts]\n",
        "        print(f\"Sample sentences: {texts_sents[0]}\")\n",
        "        return texts_sents\n",
        "    \n",
        "    def tokenize(self, texts):\n",
        "        print(f\"Processing {len(texts)} documents\")\n",
        "        texts_sents = self._make_sentences(texts)\n",
        "        all_sents = [s for sent in texts_sents for s in sent]\n",
        "\n",
        "        texts_length = [0] + [len(s) for s in texts_sents]\n",
        "        range_idx = [sum(texts_length[: i + 1]) for i in range(len(texts_length))]\n",
        "\n",
        "        print(f\"Tokenizing {len(all_sents)} sentences\")\n",
        "        sents_tokens = self.tokenizer(all_sents)\n",
        "\n",
        "        # calculating lengths of tokens in each sentence for padding purposes\n",
        "        sents_length = [len(s) for s in sents_tokens]\n",
        "\n",
        "        if self.vocab is None:\n",
        "            self.vocab = Vocab.create(sents_tokens, max_vocab=self.max_vocab, min_freq=self.min_freq)\n",
        "\n",
        "        sents_nums = [self.vocab.numericalize(s) for s in sents_tokens]\n",
        "\n",
        "        # group sentences into documents\n",
        "        texts_nums = [sents_nums[range_idx[i]: range_idx[i + 1]] for i in range(len(range_idx[:-1]))]\n",
        "\n",
        "        # compute max lengths for padding purposes\n",
        "        self.maxlen_sent = int(np.quantile(sents_length, q=self.percentile))\n",
        "        self.maxlen_doc = int(np.quantile(texts_length[1:], q=self.percentile))\n",
        "\n",
        "        print(\"Padding sentences and documents...\")\n",
        "        self.pad_token = self.vocab.stoi['xxpad']\n",
        "\n",
        "        padded_texts = [pad_nested_sequences(r, self.maxlen_sent, self.maxlen_doc, self.pad_token) for r in texts_nums]\n",
        "        return np.stack(padded_texts, axis=0)\n",
        "    \n",
        "    def transform(self, texts):\n",
        "        return self.tokenize(texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPvqbph4xwcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizer(texts):\n",
        "    tokens = Tokenizer().process_all(texts)\n",
        "    print(f\"sentence: {texts[0]}\")\n",
        "    print(f\"tokens: {tokens[0]}\")\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxu3nk_fxy1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sequences(seq, max_len, pad_idx):\n",
        "    if len(seq) > max_len:\n",
        "        return np.array(seq[:max_len]).astype(\"int32\")\n",
        "    else:\n",
        "        res = np.zeros(max_len, dtype=\"int32\") + pad_idx\n",
        "        res[:len(seq)] = seq\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bajOzj6x03C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_nested_sequences(seq, maxlen_sent, maxlen_doc, pad_idx):\n",
        "    if len(seq) == 0:\n",
        "        return np.array([[pad_idx] * maxlen_sent] * maxlen_doc).astype(\"int32\")\n",
        "\n",
        "    # pad the sentences in all docs\n",
        "    seq = [pad_sequences(s, maxlen_sent, pad_idx) for s in seq]\n",
        "\n",
        "    # padding the documents\n",
        "    if len(seq) > maxlen_doc:\n",
        "        return np.array(seq[:maxlen_doc])\n",
        "    else:\n",
        "        res = np.array([[pad_idx] * maxlen_sent] * maxlen_doc).astype(\"int32\")\n",
        "        res[:len(seq)] = seq\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EuR5WENx3W_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB = 5000\n",
        "MIN_FREQ = 5\n",
        "PERCENTILE = 0.8\n",
        "BATCH_SIZE = 32\n",
        "processor = HANPreprocessor(MAX_VOCAB, MIN_FREQ, PERCENTILE, tokenizer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeVkNuk4x6i_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "945ac3f7-416b-4834-e4f8-800ac53042db"
      },
      "source": [
        "train_seq = processor.transform(train_data)\n",
        "valid_seq = processor.transform(valid_data)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing 8208 documents\n",
            "Sample sentences: ['this mxl studio 24 usb microphone is a decent choice for those needing a portable or a good quality studio microphone.', 'we professionally produce instructional videos, both at our shop and at customer locations, and often need to voice over footage.', 'this mic will replace theblue microphones snowball usb microphone (textured white)we use in studio and also themxl mics mxl-studio 1 usb condenser microphone, cardioidwe had previously used for off-site recording.pros -* good voice quality; cleanly captures spoken voices* good physical quality; this is beefy mic that feels very well made* looks professional; while not important to the functionality, it makes a good impression on our customers* long usb cord; allows for placement away from pc* headphone jack; \"real time monitor\", handy to listen to live recording and playback* travels well; includes a protective travel case that fits all of componentscons -* no pop-filter included; normally i wouldn\\'t mention this but the instructions include a warning the condenser mics are susceptible to water damage (like from breath moisture) and a pop-filter is \"essential\".', \"we boughtnady mpf-6 6-inch clamp on microphone pop filter* left channel; only brings in audio on the left channel, had to compensate in my editing softwaremisc -* since we do not do music recording of any type, i can't comment on the mic's performance in that area* software; studio control gui (downloadable) can be launched to control some of the finer functions of the microphone, but the interface is not intuitive and my editing software (sony vegas pro) gives me better control for what we dothe mxl studio 24 mic more than meets our needs (which may be different from yours).recommended!cfh\"]\n",
            "Tokenizing 42003 sentences\n",
            "sentence: this mxl studio 24 usb microphone is a decent choice for those needing a portable or a good quality studio microphone.\n",
            "tokens: ['this', 'mxl', 'studio', '24', 'usb', 'microphone', 'is', 'a', 'decent', 'choice', 'for', 'those', 'needing', 'a', 'portable', 'or', 'a', 'good', 'quality', 'studio', 'microphone', '.']\n",
            "Padding sentences and documents...\n",
            "Processing 2053 documents\n",
            "Sample sentences: [\"i'm very pleased with this purchase.\", 'the cables are flexible, quiet and no pops or loss of signal.', 'not much else to say.', 'buy them.']\n",
            "Tokenizing 10294 sentences\n",
            "sentence: i'm very pleased with this purchase.\n",
            "tokens: ['i', \"'m\", 'very', 'pleased', 'with', 'this', 'purchase', '.']\n",
            "Padding sentences and documents...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjo9Ad5Zx8Bj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = TensorDataset(\n",
        "    torch.from_numpy(train_seq).long(),\n",
        "    torch.from_numpy(train_label).long()\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cig7ELox-hl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_set = TensorDataset(\n",
        "    torch.from_numpy(valid_seq).long(),\n",
        "    torch.from_numpy(valid_label).long()\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    dataset=valid_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L75bOIrUyKOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp => [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        energy = torch.tanh(self.attn(inp))\n",
        "        # energy => [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        attention = F.softmax(self.v(energy), dim=1)\n",
        "        # attention => [batch_size, seq_len, 1]\n",
        "\n",
        "        return attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiVdaVtgyPHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class WordAttention(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, pad_idx, hidden_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
        "        self.rnn = nn.GRU(emb_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.word_attn = Attention(hidden_dim * 2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inp, hidden):\n",
        "        # inp => [batch_size, seq_len]\n",
        "        # hidden => [num_dir * num_layers, batch_size, hidden_dim]\n",
        "\n",
        "        embedded = self.embedding(inp)\n",
        "        embedded = self.dropout(embedded)\n",
        "        # embedded => [batch_size, seq_len, emb_dim]\n",
        "\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        # output => [batch_size, seq_len, hidden_dim * num_dir]\n",
        "        #        => [batch_size, seq_len, hidden_dim * 2]\n",
        "        # hidden => [num_dir * n_layers, batch_size, hidden_dim]\n",
        "\n",
        "        attention = self.word_attn(output)\n",
        "        # attention => [batch_size, seq_len, 1]\n",
        "\n",
        "        # output => [batch_size, seq_len, hid_dim * 2]\n",
        "        weighted = torch.sum(attention * output, dim=1)\n",
        "        # weighted => [batch_size, hidden_dim * 2]\n",
        "\n",
        "        attention = attention.permute(0, 2, 1)\n",
        "        # attention => [batch_size, 1, seq_len]\n",
        "\n",
        "        weighted = weighted.unsqueeze(1)\n",
        "        # weighted => [batch_size, 1, hidden_dim * 2]\n",
        "\n",
        "        return attention, weighted, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3spmb7HyySRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentenceAttention(nn.Module):\n",
        "    def __init__(self, word_hidden_dim, sent_hidden_dim, pad_idx, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn = nn.GRU(word_hidden_dim * 2, sent_hidden_dim, bidirectional=True)\n",
        "        self.sent_attn = Attention(sent_hidden_dim * 2)\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp => [batch_size, seq_len, word_hid_dim * 2]\n",
        "\n",
        "        output, hidden = self.rnn(inp)\n",
        "        # output => [batch_size, seq_len, sent_hid_dim * 2]\n",
        "        # hidden => [num_layers * num_dir, batch_size, sent_hid_dim]\n",
        "\n",
        "        attention = self.sent_attn(output)\n",
        "        # attention => [batch_size, seq_len, 1]\n",
        "\n",
        "        # output => [batch_size, seq_len, hid_dim * 2]\n",
        "        weighted = torch.sum(attention * output, dim=1)\n",
        "        # weighted => [batch_size, hidden_dim * 2]\n",
        "\n",
        "        attention = attention.permute(0, 2, 1)\n",
        "        # attention => [batch_size, 1, seq_len]\n",
        "\n",
        "        return attention, weighted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A33I7fGpyVLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class HierarchicalAttention(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, word_hid_dim, sent_hid_dim, pad_idx, output_dim, dropout, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.word_hid_dim = word_hid_dim\n",
        "        self.device = device\n",
        "        self.word_attention = WordAttention(input_dim, emb_dim, pad_idx, word_hid_dim, dropout)\n",
        "        self.sent_attention = SentenceAttention(word_hid_dim, sent_hid_dim, pad_idx, dropout)\n",
        "\n",
        "        self.fc = nn.Linear(sent_hid_dim * 2, output_dim)\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp => [batch_size, max_sents, max_words]\n",
        "        \n",
        "        batch_size = inp.shape[0]\n",
        "        inp = inp.permute(1, 0 , 2)\n",
        "        # inp => [max_sents, batch_size, max_words]\n",
        "\n",
        "        # initialize word rnn hiddens state\n",
        "        hidden = torch.nn.Parameter(torch.zeros(2, batch_size, self.word_hid_dim)).to(self.device)\n",
        "\n",
        "        word_attentions, sents_reps = [], []\n",
        "\n",
        "        for sent in inp:\n",
        "            word_attn, sent_rep, hidden = self.word_attention(sent, hidden)\n",
        "            word_attentions.append(word_attn)\n",
        "            sents_reps.append(sent_rep)\n",
        "        \n",
        "        sents = torch.cat(sents_reps, 1)\n",
        "        # sents => [batch_size, max_sents, word_hid_dim * 2]\n",
        "\n",
        "        word_attns = torch.cat(word_attentions, 1)\n",
        "        # word_attns => [batch_size, max_sents, max_words]\n",
        "\n",
        "        sent_attn, doc_rep = self.sent_attention(sents)\n",
        "        sent_attn = sent_attn.squeeze(1)\n",
        "        # sent_attn => [batch_size, max_sents]\n",
        "        # doc_rep => [batch_size, sent_hid_dim * 2]\n",
        "\n",
        "        logits = self.fc(doc_rep)\n",
        "        # logits => [batch_size, 5]\n",
        "\n",
        "        return logits, sent_attn, word_attns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCDA3ZrUyY2d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "b1b330a6-00dc-4ecd-e75e-6042ee91815b"
      },
      "source": [
        "input_dim = len(processor.vocab.itos)\n",
        "emb_dim = 50\n",
        "word_hid_dim = 32\n",
        "sent_hid_dim = 32\n",
        "pad_idx = processor.pad_token\n",
        "output_dim = 5\n",
        "dropout = 0.5\n",
        "model = HierarchicalAttention(input_dim, emb_dim, word_hid_dim, sent_hid_dim, pad_idx, output_dim, dropout, device)\n",
        "model = model.to(device)\n",
        "print(model)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HierarchicalAttention(\n",
            "  (word_attention): WordAttention(\n",
            "    (embedding): Embedding(5000, 50, padding_idx=1)\n",
            "    (rnn): GRU(50, 32, batch_first=True, bidirectional=True)\n",
            "    (word_attn): Attention(\n",
            "      (attn): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (v): Linear(in_features=64, out_features=1, bias=False)\n",
            "    )\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (sent_attention): SentenceAttention(\n",
            "    (rnn): GRU(64, 32, bidirectional=True)\n",
            "    (sent_attn): Attention(\n",
            "      (attn): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (v): Linear(in_features=64, out_features=1, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qHTvA0eyvb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXUnn77iyyiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, criterion, optimizer, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        inp, labels = batch\n",
        "        inp = inp.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits, _, _ = model(inp)\n",
        "        # logits => [batch_size, 5]\n",
        "        # labels => [batch_size]\n",
        "\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        _, predictions = torch.max(logits, 1)\n",
        "        correct = torch.sum((predictions == labels))\n",
        "        epoch_accuracy = (correct / len(labels)).item()\n",
        "    \n",
        "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQVQ42C-y8ON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            inp, labels = batch\n",
        "            inp = inp.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits, _, _ = model(inp)\n",
        "            # logits => [batch_size, 5]\n",
        "            # labels => [batch_size]\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            _, predictions = torch.max(logits, 1)\n",
        "            correct = torch.sum((predictions == labels))\n",
        "            epoch_accuracy = (correct / len(labels)).item()\n",
        "    \n",
        "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xDOmqqNy-fi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF3UJw7hzAKh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24a8a846-5e11-4fda-8537-fd718c0cd949"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "CLIP = 1\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, CLIP)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.870 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.880 |  Val. Acc: 1.54%\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.830 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.863 |  Val. Acc: 1.54%\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.793 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.875 |  Val. Acc: 1.54%\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.765 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.875 |  Val. Acc: 1.54%\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.746 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.921 |  Val. Acc: 1.54%\n",
            "Epoch: 06 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.722 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.876 |  Val. Acc: 1.54%\n",
            "Epoch: 07 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.696 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.888 |  Val. Acc: 0.00%\n",
            "Epoch: 08 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.678 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.987 |  Val. Acc: 1.54%\n",
            "Epoch: 09 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.659 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.938 |  Val. Acc: 1.54%\n",
            "Epoch: 10 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.647 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.965 |  Val. Acc: 1.54%\n",
            "Epoch: 11 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.632 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.924 |  Val. Acc: 1.54%\n",
            "Epoch: 12 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.618 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.998 |  Val. Acc: 1.54%\n",
            "Epoch: 13 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.608 | Train Acc: 0.00%\n",
            "\t Val. Loss: 0.989 |  Val. Acc: 1.54%\n",
            "Epoch: 14 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.603 | Train Acc: 0.00%\n",
            "\t Val. Loss: 1.007 |  Val. Acc: 1.54%\n",
            "Epoch: 15 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.573 | Train Acc: 0.00%\n",
            "\t Val. Loss: 1.076 |  Val. Acc: 1.54%\n",
            "Epoch: 16 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.577 | Train Acc: 0.00%\n",
            "\t Val. Loss: 1.019 |  Val. Acc: 1.54%\n",
            "Epoch: 17 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.555 | Train Acc: 0.00%\n",
            "\t Val. Loss: 1.116 |  Val. Acc: 1.54%\n",
            "Epoch: 18 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.541 | Train Acc: 0.00%\n",
            "\t Val. Loss: 1.094 |  Val. Acc: 1.54%\n",
            "Epoch: 19 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.540 | Train Acc: 0.00%\n",
            "\t Val. Loss: 1.114 |  Val. Acc: 0.00%\n",
            "Epoch: 20 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.525 | Train Acc: 0.00%\n",
            "\t Val. Loss: 1.115 |  Val. Acc: 0.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE44fTiGz16R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}