{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of QQP Classification with BERT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyORevfAfFP7wTtcTImCgMsR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/100-Days-of-NLP/blob/applications%2Fclassification/applications/classification/QQP%20Classification%20with%20BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_Jx6JNV7T0X",
        "colab_type": "text"
      },
      "source": [
        "# Quora Duplicate Question Pair detection with BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9o-a4V87aeS",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "538wRmdrndIZ",
        "colab_type": "code",
        "outputId": "6f743a06-51ef-4f8e-ec0a-3fff8d34186c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install sh\n",
        "!pip install nlp\n",
        "!pip install transformers\n",
        "!pip install pytorch_lightning"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sh\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/9c/796934ee6d990d504c600056aa435e31bd49dbfba37e81d2045d37c8bdaf/sh-1.13.1-py2.py3-none-any.whl (40kB)\n",
            "\r\u001b[K     |████████▏                       | 10kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 20kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 1.7MB/s \n",
            "\u001b[?25hInstalling collected packages: sh\n",
            "Successfully installed sh-1.13.1\n",
            "Collecting nlp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/80/05b452119eb2213fc1b3c7647f39bd231b5804edc065168f4e43dce8026d/nlp-0.2.1-py3-none-any.whl (869kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 4.3MB/s \n",
            "\u001b[?25hCollecting pyarrow>=0.16.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/3f/6cac1714fff444664603f92cb9fbe91c7ae25375880158b9e9691c4584c8/pyarrow-0.17.1-cp36-cp36m-manylinux2014_x86_64.whl (63.8MB)\n",
            "\u001b[K     |████████████████████████████████| 63.8MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from nlp) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from nlp) (0.7)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from nlp) (0.3.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nlp) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from nlp) (4.41.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from nlp) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
            "Installing collected packages: pyarrow, nlp\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed nlp-0.2.1 pyarrow-0.17.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyarrow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\r\u001b[K     |▌                               | 10kB 11.0MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 2.8MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 3.5MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 3.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 3.6MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71kB 4.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81kB 4.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92kB 4.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 112kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 143kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 174kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 194kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 204kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 225kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 235kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 256kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 266kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 286kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 307kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 327kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 348kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 358kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 368kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 378kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 389kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 399kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 409kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 419kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 430kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 440kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 450kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 460kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 471kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 481kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 491kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 501kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 512kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 522kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 532kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 542kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 552kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 563kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 573kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 583kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 593kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 604kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 614kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 624kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 634kB 4.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 645kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 655kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 665kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 675kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 11.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 19.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 30.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=7a2d777684508f65ad79b0ae23df6d6e840f26e5583896e84aa18bce78555f73\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n",
            "Collecting pytorch_lightning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/ab/561d1fa6e5af30b2fd7cb4001f93eb08531e1b72976f13eebf7f7cdc021c/pytorch_lightning-0.7.6-py3-none-any.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (4.41.1)\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.18.5)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.5.0+cu101)\n",
            "Requirement already satisfied: pyyaml>=3.13 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (3.13)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (0.4.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (3.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (3.2.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (0.34.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.7.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (0.9.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.29.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.6.0.post3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (47.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch_lightning) (1.12.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch_lightning) (2020.4.5.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning) (1.6.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->pytorch_lightning) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch_lightning) (0.4.8)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=0b83bc08d1b24f613a835ee4ae8dde17e0c65ee840fb5c22df52bcc34eddc7da\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built future\n",
            "Installing collected packages: future, pytorch-lightning\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed future-0.18.2 pytorch-lightning-0.7.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXFVR9IrCuV-",
        "colab_type": "text"
      },
      "source": [
        "I have taken the reference from [hugging-face-nlp demo](https://github.com/yk/huggingface-nlp-demo/blob/master/demo.py) and modified accordingly to the quora question pair task.\n",
        "\n",
        "As the dataset is pretty huge (4,00,000) pairs, running BERT on it will take 6-8hours of time. I took a subset of it and ran the code. If you want, you can modify the `percent` value accordingly.\n",
        "\n",
        "### References:\n",
        "\n",
        "- [Hugging-face-nlp-demo code](https://github.com/yk/huggingface-nlp-demo/blob/master/demo.py)\n",
        "- [Hugging-face-nlp-demo video tutorial](https://www.youtube.com/watch?v=G3pOvrKkFuk&feature=youtu.be)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKMP2BB7fXQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sh\n",
        "\n",
        "import nlp\n",
        "import transformers\n",
        "import torch as th\n",
        "import pytorch_lightning as pl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xs_7wSzv-hY",
        "colab_type": "code",
        "outputId": "60cf952b-f445-410a-a0ef-a013075aa486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1zusrJjnVlf",
        "colab_type": "code",
        "outputId": "378b9367-7d6c-47ff-c470-eb24aa69f603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "debug = False\n",
        "epochs = 10\n",
        "batch_size = 8\n",
        "lr = 1e-2\n",
        "momentum = 0.9\n",
        "model_type = 'bert-base-uncased'\n",
        "seq_length = 100\n",
        "percent = 20\n",
        "\n",
        "\n",
        "sh.rm('-r', '-f', 'logs')\n",
        "sh.mkdir('logs')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh-5sBW_nZXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class QQPClassifier(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = transformers.BertForSequenceClassification.from_pretrained(model_type)\n",
        "        self.loss = th.nn.CrossEntropyLoss(reduction='none')\n",
        "    \n",
        "    def prepare_data(self, split=\"train\"):\n",
        "        tokenizer = transformers.BertTokenizer.from_pretrained(model_type)\n",
        "\n",
        "        def _tokenize(x):\n",
        "            encoded = tokenizer.batch_encode_plus(\n",
        "                    x['question1'],\n",
        "                    x['question2'],\n",
        "                    max_length=seq_length, \n",
        "                    pad_to_max_length=True)\n",
        "            x['input_ids'] = encoded['input_ids']\n",
        "            x['token_type_ids'] = encoded['token_type_ids']\n",
        "            return x\n",
        "\n",
        "        def _prepare_ds(split):\n",
        "            ds = nlp.load_dataset('glue', 'qqp', split=f'{split}[:{batch_size if debug else f\"{percent}%\"}]')\n",
        "            ds = ds.map(_tokenize, batched=True)\n",
        "            ds.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'label'])\n",
        "            return ds\n",
        "\n",
        "        self.train_ds, self.val_ds = map(_prepare_ds, ('train', 'validation'))\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids):\n",
        "        mask = (input_ids != 0).float()\n",
        "        logits, = self.model(input_ids, mask, token_type_ids)\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        logits = self.forward(batch['input_ids'], batch['token_type_ids'])\n",
        "        loss = self.loss(logits, batch['label']).mean()\n",
        "        return {'loss': loss, 'log': {'train_loss': loss}}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        logits = self.forward(batch['input_ids'], batch['token_type_ids'])\n",
        "        loss = self.loss(logits, batch['label'])\n",
        "        acc = (logits.argmax(-1) == batch['label']).float()\n",
        "        return {'loss': loss, 'acc': acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        loss = th.cat([o['loss'] for o in outputs], 0).mean()\n",
        "        acc = th.cat([o['acc'] for o in outputs], 0).mean()\n",
        "        out = {'val_loss': loss, 'val_acc': acc}\n",
        "        return {**out, 'log': out}\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return th.utils.data.DataLoader(\n",
        "                self.train_ds,\n",
        "                batch_size=batch_size,\n",
        "                drop_last=True,\n",
        "                shuffle=True,\n",
        "                )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return th.utils.data.DataLoader(\n",
        "                self.val_ds,\n",
        "                batch_size=batch_size,\n",
        "                drop_last=False,\n",
        "                shuffle=False,\n",
        "                )\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return th.optim.SGD(\n",
        "            self.parameters(),\n",
        "            lr=lr,\n",
        "            momentum=momentum,\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rsG2PoVoT-G",
        "colab_type": "code",
        "outputId": "2ccc7fea-f984-4800-dd94-efc65241c8e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "731f42c4f41c45eea780e453bd2cf4b6",
            "f7650beeab504ef092dd2ced51f9ebb7",
            "a9081b1578da468e8c8ba7f5948eed6e",
            "fa2e6716d8d14f3faf67d69d300f220a",
            "659606f5f26549f4b9c7337b842c25b8",
            "2efd151907f646c0b1bd1c4dca661e27",
            "1ad60e9a047b4de5acd53d9b46d9c40d",
            "ad4c4cda792c42db8ce14d9b2b750eb7",
            "185ce5e3307a4db7918423e805607962",
            "2a1008313041416c858cb9c5ea72c970",
            "0cb42e82f6d143b5b4f9d0dbfa98e33d"
          ]
        }
      },
      "source": [
        "model = QQPClassifier()\n",
        "trainer = pl.Trainer(\n",
        "    default_root_dir='logs',\n",
        "    gpus=(1 if th.cuda.is_available() else 0),\n",
        "    max_epochs=epochs,\n",
        "    fast_dev_run=debug,\n",
        "    logger=pl.loggers.TensorBoardLogger('logs/', name='qqp', version=0),\n",
        ")\n",
        "trainer.fit(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "731f42c4f41c45eea780e453bd2cf4b6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f7650beeab504ef092dd2ced51f9ebb7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "No environment variable for node rank defined. Set as 0.\n",
            "CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9081b1578da468e8c8ba7f5948eed6e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa2e6716d8d14f3faf67d69d300f220a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29015.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "659606f5f26549f4b9c7337b842c25b8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=30329.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading and preparing dataset glue/qqp (download: 57.73 MiB, generated: 107.02 MiB, total: 164.75 MiB) to /root/.cache/huggingface/datasets/glue/qqp/1.0.0...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2efd151907f646c0b1bd1c4dca661e27",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=60534884.0, style=ProgressStyle(descrip…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ad60e9a047b4de5acd53d9b46d9c40d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad4c4cda792c42db8ce14d9b2b750eb7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "185ce5e3307a4db7918423e805607962",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\rDataset glue downloaded and prepared to /root/.cache/huggingface/datasets/glue/qqp/1.0.0. Subsequent calls will reuse this data.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 73/73 [00:24<00:00,  3.00it/s]\n",
            "100%|██████████| 9/9 [00:02<00:00,  3.38it/s]\n",
            "\n",
            "    | Name                                                   | Type                          | Params\n",
            "-----------------------------------------------------------------------------------------------------\n",
            "0   | model                                                  | BertForSequenceClassification | 109 M \n",
            "1   | model.bert                                             | BertModel                     | 109 M \n",
            "2   | model.bert.embeddings                                  | BertEmbeddings                | 23 M  \n",
            "3   | model.bert.embeddings.word_embeddings                  | Embedding                     | 23 M  \n",
            "4   | model.bert.embeddings.position_embeddings              | Embedding                     | 393 K \n",
            "5   | model.bert.embeddings.token_type_embeddings            | Embedding                     | 1 K   \n",
            "6   | model.bert.embeddings.LayerNorm                        | LayerNorm                     | 1 K   \n",
            "7   | model.bert.embeddings.dropout                          | Dropout                       | 0     \n",
            "8   | model.bert.encoder                                     | BertEncoder                   | 85 M  \n",
            "9   | model.bert.encoder.layer                               | ModuleList                    | 85 M  \n",
            "10  | model.bert.encoder.layer.0                             | BertLayer                     | 7 M   \n",
            "11  | model.bert.encoder.layer.0.attention                   | BertAttention                 | 2 M   \n",
            "12  | model.bert.encoder.layer.0.attention.self              | BertSelfAttention             | 1 M   \n",
            "13  | model.bert.encoder.layer.0.attention.self.query        | Linear                        | 590 K \n",
            "14  | model.bert.encoder.layer.0.attention.self.key          | Linear                        | 590 K \n",
            "15  | model.bert.encoder.layer.0.attention.self.value        | Linear                        | 590 K \n",
            "16  | model.bert.encoder.layer.0.attention.self.dropout      | Dropout                       | 0     \n",
            "17  | model.bert.encoder.layer.0.attention.output            | BertSelfOutput                | 592 K \n",
            "18  | model.bert.encoder.layer.0.attention.output.dense      | Linear                        | 590 K \n",
            "19  | model.bert.encoder.layer.0.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
            "20  | model.bert.encoder.layer.0.attention.output.dropout    | Dropout                       | 0     \n",
            "21  | model.bert.encoder.layer.0.intermediate                | BertIntermediate              | 2 M   \n",
            "22  | model.bert.encoder.layer.0.intermediate.dense          | Linear                        | 2 M   \n",
            "23  | model.bert.encoder.layer.0.output                      | BertOutput                    | 2 M   \n",
            "24  | model.bert.encoder.layer.0.output.dense                | Linear                        | 2 M   \n",
            "25  | model.bert.encoder.layer.0.output.LayerNorm            | LayerNorm                     | 1 K   \n",
            "26  | model.bert.encoder.layer.0.output.dropout              | Dropout                       | 0     \n",
            "27  | model.bert.encoder.layer.1                             | BertLayer                     | 7 M   \n",
            "28  | model.bert.encoder.layer.1.attention                   | BertAttention                 | 2 M   \n",
            "29  | model.bert.encoder.layer.1.attention.self              | BertSelfAttention             | 1 M   \n",
            "30  | model.bert.encoder.layer.1.attention.self.query        | Linear                        | 590 K \n",
            "31  | model.bert.encoder.layer.1.attention.self.key          | Linear                        | 590 K \n",
            "32  | model.bert.encoder.layer.1.attention.self.value        | Linear                        | 590 K \n",
            "33  | model.bert.encoder.layer.1.attention.self.dropout      | Dropout                       | 0     \n",
            "34  | model.bert.encoder.layer.1.attention.output            | BertSelfOutput                | 592 K \n",
            "35  | model.bert.encoder.layer.1.attention.output.dense      | Linear                        | 590 K \n",
            "36  | model.bert.encoder.layer.1.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
            "37  | model.bert.encoder.layer.1.attention.output.dropout    | Dropout                       | 0     \n",
            "38  | model.bert.encoder.layer.1.intermediate                | BertIntermediate              | 2 M   \n",
            "39  | model.bert.encoder.layer.1.intermediate.dense          | Linear                        | 2 M   \n",
            "40  | model.bert.encoder.layer.1.output                      | BertOutput                    | 2 M   \n",
            "41  | model.bert.encoder.layer.1.output.dense                | Linear                        | 2 M   \n",
            "42  | model.bert.encoder.layer.1.output.LayerNorm            | LayerNorm                     | 1 K   \n",
            "43  | model.bert.encoder.layer.1.output.dropout              | Dropout                       | 0     \n",
            "44  | model.bert.encoder.layer.2                             | BertLayer                     | 7 M   \n",
            "45  | model.bert.encoder.layer.2.attention                   | BertAttention                 | 2 M   \n",
            "46  | model.bert.encoder.layer.2.attention.self              | BertSelfAttention             | 1 M   \n",
            "47  | model.bert.encoder.layer.2.attention.self.query        | Linear                        | 590 K \n",
            "48  | model.bert.encoder.layer.2.attention.self.key          | Linear                        | 590 K \n",
            "49  | model.bert.encoder.layer.2.attention.self.value        | Linear                        | 590 K \n",
            "50  | model.bert.encoder.layer.2.attention.self.dropout      | Dropout                       | 0     \n",
            "51  | model.bert.encoder.layer.2.attention.output            | BertSelfOutput                | 592 K \n",
            "52  | model.bert.encoder.layer.2.attention.output.dense      | Linear                        | 590 K \n",
            "53  | model.bert.encoder.layer.2.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
            "54  | model.bert.encoder.layer.2.attention.output.dropout    | Dropout                       | 0     \n",
            "55  | model.bert.encoder.layer.2.intermediate                | BertIntermediate              | 2 M   \n",
            "56  | model.bert.encoder.layer.2.intermediate.dense          | Linear                        | 2 M   \n",
            "57  | model.bert.encoder.layer.2.output                      | BertOutput                    | 2 M   \n",
            "58  | model.bert.encoder.layer.2.output.dense                | Linear                        | 2 M   \n",
            "59  | model.bert.encoder.layer.2.output.LayerNorm            | LayerNorm                     | 1 K   \n",
            "60  | model.bert.encoder.layer.2.output.dropout              | Dropout                       | 0     \n",
            "61  | model.bert.encoder.layer.3                             | BertLayer                     | 7 M   \n",
            "62  | model.bert.encoder.layer.3.attention                   | BertAttention                 | 2 M   \n",
            "63  | model.bert.encoder.layer.3.attention.self              | BertSelfAttention             | 1 M   \n",
            "64  | model.bert.encoder.layer.3.attention.self.query        | Linear                        | 590 K \n",
            "65  | model.bert.encoder.layer.3.attention.self.key          | Linear                        | 590 K \n",
            "66  | model.bert.encoder.layer.3.attention.self.value        | Linear                        | 590 K \n",
            "67  | model.bert.encoder.layer.3.attention.self.dropout      | Dropout                       | 0     \n",
            "68  | model.bert.encoder.layer.3.attention.output            | BertSelfOutput                | 592 K \n",
            "69  | model.bert.encoder.layer.3.attention.output.dense      | Linear                        | 590 K \n",
            "70  | model.bert.encoder.layer.3.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
            "71  | model.bert.encoder.layer.3.attention.output.dropout    | Dropout                       | 0     \n",
            "72  | model.bert.encoder.layer.3.intermediate                | BertIntermediate              | 2 M   \n",
            "73  | model.bert.encoder.layer.3.intermediate.dense          | Linear                        | 2 M   \n",
            "74  | model.bert.encoder.layer.3.output                      | BertOutput                    | 2 M   \n",
            "75  | model.bert.encoder.layer.3.output.dense                | Linear                        | 2 M   \n",
            "76  | model.bert.encoder.layer.3.output.LayerNorm            | LayerNorm                     | 1 K   \n",
            "77  | model.bert.encoder.layer.3.output.dropout              | Dropout                       | 0     \n",
            "78  | model.bert.encoder.layer.4                             | BertLayer                     | 7 M   \n",
            "79  | model.bert.encoder.layer.4.attention                   | BertAttention                 | 2 M   \n",
            "80  | model.bert.encoder.layer.4.attention.self              | BertSelfAttention             | 1 M   \n",
            "81  | model.bert.encoder.layer.4.attention.self.query        | Linear                        | 590 K \n",
            "82  | model.bert.encoder.layer.4.attention.self.key          | Linear                        | 590 K \n",
            "83  | model.bert.encoder.layer.4.attention.self.value        | Linear                        | 590 K \n",
            "84  | model.bert.encoder.layer.4.attention.self.dropout      | Dropout                       | 0     \n",
            "85  | model.bert.encoder.layer.4.attention.output            | BertSelfOutput                | 592 K \n",
            "86  | model.bert.encoder.layer.4.attention.output.dense      | Linear                        | 590 K \n",
            "87  | model.bert.encoder.layer.4.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
            "88  | model.bert.encoder.layer.4.attention.output.dropout    | Dropout                       | 0     \n",
            "89  | model.bert.encoder.layer.4.intermediate                | BertIntermediate              | 2 M   \n",
            "90  | model.bert.encoder.layer.4.intermediate.dense          | Linear                        | 2 M   \n",
            "91  | model.bert.encoder.layer.4.output                      | BertOutput                    | 2 M   \n",
            "92  | model.bert.encoder.layer.4.output.dense                | Linear                        | 2 M   \n",
            "93  | model.bert.encoder.layer.4.output.LayerNorm            | LayerNorm                     | 1 K   \n",
            "94  | model.bert.encoder.layer.4.output.dropout              | Dropout                       | 0     \n",
            "95  | model.bert.encoder.layer.5                             | BertLayer                     | 7 M   \n",
            "96  | model.bert.encoder.layer.5.attention                   | BertAttention                 | 2 M   \n",
            "97  | model.bert.encoder.layer.5.attention.self              | BertSelfAttention             | 1 M   \n",
            "98  | model.bert.encoder.layer.5.attention.self.query        | Linear                        | 590 K \n",
            "99  | model.bert.encoder.layer.5.attention.self.key          | Linear                        | 590 K \n",
            "100 | model.bert.encoder.layer.5.attention.self.value        | Linear                        | 590 K \n",
            "101 | model.bert.encoder.layer.5.attention.self.dropout      | Dropout                       | 0     \n",
            "102 | model.bert.encoder.layer.5.attention.output            | BertSelfOutput                | 592 K \n",
            "103 | model.bert.encoder.layer.5.attention.output.dense      | Linear                        | 590 K \n",
            "104 | model.bert.encoder.layer.5.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
            "105 | model.bert.encoder.layer.5.attention.output.dropout    | Dropout                       | 0     \n",
            "106 | model.bert.encoder.layer.5.intermediate                | BertIntermediate              | 2 M   \n",
            "107 | model.bert.encoder.layer.5.intermediate.dense          | Linear                        | 2 M   \n",
            "108 | model.bert.encoder.layer.5.output                      | BertOutput                    | 2 M   \n",
            "109 | model.bert.encoder.layer.5.output.dense                | Linear                        | 2 M   \n",
            "110 | model.bert.encoder.layer.5.output.LayerNorm            | LayerNorm                     | 1 K   \n",
            "111 | model.bert.encoder.layer.5.output.dropout              | Dropout                       | 0     \n",
            "112 | model.bert.encoder.layer.6                             | BertLayer                     | 7 M   \n",
            "113 | model.bert.encoder.layer.6.attention                   | BertAttention                 | 2 M   \n",
            "114 | model.bert.encoder.layer.6.attention.self              | BertSelfAttention             | 1 M   \n",
            "115 | model.bert.encoder.layer.6.attention.self.query        | Linear                        | 590 K \n",
            "116 | model.bert.encoder.layer.6.attention.self.key          | Linear                        | 590 K \n",
            "117 | model.bert.encoder.layer.6.attention.self.value        | Linear                        | 590 K \n",
            "118 | model.bert.encoder.layer.6.attention.self.dropout      | Dropout                       | 0     \n",
            "119 | model.bert.encoder.layer.6.attention.output            | BertSelfOutput                | 592 K \n",
            "120 | model.bert.encoder.layer.6.attention.output.dense      | Linear                        | 590 K \n",
            "121 | model.bert.encoder.layer.6.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
            "122 | model.bert.encoder.layer.6.attention.output.dropout    | Dropout                       | 0     \n",
            "123 | model.bert.encoder.layer.6.intermediate                | BertIntermediate              | 2 M   \n",
            "124 | model.bert.encoder.layer.6.intermediate.dense          | Linear                        | 2 M   \n",
            "125 | model.bert.encoder.layer.6.output                      | BertOutput                    | 2 M   \n",
            "126 | model.bert.encoder.layer.6.output.dense                | Linear                        | 2 M   \n",
            "127 | model.bert.encoder.layer.6.output.LayerNorm            | LayerNorm                     | 1 K   \n",
            "128 | model.bert.encoder.layer.6.output.dropout              | Dropout                       | 0     \n",
            "129 | model.bert.encoder.layer.7                             | BertLayer                     | 7 M   \n",
            "130 | model.bert.encoder.layer.7.attention                   | BertAttention                 | 2 M   \n",
            "131 | model.bert.encoder.layer.7.attention.self              | BertSelfAttention             | 1 M   \n",
            "132 | model.bert.encoder.layer.7.attention.self.query        | Linear                        | 590 K \n",
            "133 | model.bert.encoder.layer.7.attention.self.key          | Linear                        | 590 K \n",
            "134 | model.bert.encoder.layer.7.attention.self.value        | Linear                        | 590 K \n",
            "135 | model.bert.encoder.layer.7.attention.self.dropout      | Dropout                       | 0     \n",
            "136 | model.bert.encoder.layer.7.attention.output            | BertSelfOutput                | 592 K \n",
            "137 | model.bert.encoder.layer.7.attention.output.dense      | Linear                        | 590 K \n",
            "138 | model.bert.encoder.layer.7.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
            "139 | model.bert.encoder.layer.7.attention.output.dropout    | Dropout                       | 0     \n",
            "140 | model.bert.encoder.layer.7.intermediate                | BertIntermediate              | 2 M   \n",
            "141 | model.bert.encoder.layer.7.intermediate.dense          | Linear                        | 2 M   \n",
            "142 | model.bert.encoder.layer.7.output                      | BertOutput                    | 2 M   \n",
            "143 | model.bert.encoder.layer.7.output.dense                | Linear                        | 2 M   \n",
            "144 | model.bert.encoder.layer.7.output.LayerNorm            | LayerNorm                     | 1 K   \n",
            "145 | model.bert.encoder.layer.7.output.dropout              | Dropout                       | 0     \n",
            "146 | model.bert.encoder.layer.8                             | BertLayer                     | 7 M   \n",
            "147 | model.bert.encoder.layer.8.attention                   | BertAttention                 | 2 M   \n",
            "148 | model.bert.encoder.layer.8.attention.self              | BertSelfAttention             | 1 M   \n",
            "149 | model.bert.encoder.layer.8.attention.self.query        | Linear                        | 590 K \n",
            "150 | model.bert.encoder.layer.8.attention.self.key          | Linear                        | 590 K \n",
            "151 | model.bert.encoder.layer.8.attention.self.value        | Linear                        | 590 K \n",
            "152 | model.bert.encoder.layer.8.attention.self.dropout      | Dropout                       | 0     \n",
            "153 | model.bert.encoder.layer.8.attention.output            | BertSelfOutput                | 592 K \n",
            "154 | model.bert.encoder.layer.8.attention.output.dense      | Linear                        | 590 K \n",
            "155 | model.bert.encoder.layer.8.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
            "156 | model.bert.encoder.layer.8.attention.output.dropout    | Dropout                       | 0     \n",
            "157 | model.bert.encoder.layer.8.intermediate                | BertIntermediate              | 2 M   \n",
            "158 | model.bert.encoder.layer.8.intermediate.dense          | Linear                        | 2 M   \n",
            "159 | model.bert.encoder.layer.8.output                      | BertOutput                    | 2 M   \n",
            "160 | model.bert.encoder.layer.8.output.dense                | Linear                        | 2 M   \n",
            "161 | model.bert.encoder.layer.8.output.LayerNorm            | LayerNorm                     | 1 K   \n",
            "162 | model.bert.encoder.layer.8.output.dropout              | Dropout                       | 0     \n",
            "163 | model.bert.encoder.layer.9                             | BertLayer                     | 7 M   \n",
            "164 | model.bert.encoder.layer.9.attention                   | BertAttention                 | 2 M   \n",
            "165 | model.bert.encoder.layer.9.attention.self              | BertSelfAttention             | 1 M   \n",
            "166 | model.bert.encoder.layer.9.attention.self.query        | Linear                        | 590 K \n",
            "167 | model.bert.encoder.layer.9.attention.self.key          | Linear                        | 590 K \n",
            "168 | model.bert.encoder.layer.9.attention.self.value        | Linear                        | 590 K \n",
            "169 | model.bert.encoder.layer.9.attention.self.dropout      | Dropout                       | 0     \n",
            "170 | model.bert.encoder.layer.9.attention.output            | BertSelfOutput                | 592 K \n",
            "171 | model.bert.encoder.layer.9.attention.output.dense      | Linear                        | 590 K \n",
            "172 | model.bert.encoder.layer.9.attention.output.LayerNorm  | LayerNorm                     | 1 K   \n",
            "173 | model.bert.encoder.layer.9.attention.output.dropout    | Dropout                       | 0     \n",
            "174 | model.bert.encoder.layer.9.intermediate                | BertIntermediate              | 2 M   \n",
            "175 | model.bert.encoder.layer.9.intermediate.dense          | Linear                        | 2 M   \n",
            "176 | model.bert.encoder.layer.9.output                      | BertOutput                    | 2 M   \n",
            "177 | model.bert.encoder.layer.9.output.dense                | Linear                        | 2 M   \n",
            "178 | model.bert.encoder.layer.9.output.LayerNorm            | LayerNorm                     | 1 K   \n",
            "179 | model.bert.encoder.layer.9.output.dropout              | Dropout                       | 0     \n",
            "180 | model.bert.encoder.layer.10                            | BertLayer                     | 7 M   \n",
            "181 | model.bert.encoder.layer.10.attention                  | BertAttention                 | 2 M   \n",
            "182 | model.bert.encoder.layer.10.attention.self             | BertSelfAttention             | 1 M   \n",
            "183 | model.bert.encoder.layer.10.attention.self.query       | Linear                        | 590 K \n",
            "184 | model.bert.encoder.layer.10.attention.self.key         | Linear                        | 590 K \n",
            "185 | model.bert.encoder.layer.10.attention.self.value       | Linear                        | 590 K \n",
            "186 | model.bert.encoder.layer.10.attention.self.dropout     | Dropout                       | 0     \n",
            "187 | model.bert.encoder.layer.10.attention.output           | BertSelfOutput                | 592 K \n",
            "188 | model.bert.encoder.layer.10.attention.output.dense     | Linear                        | 590 K \n",
            "189 | model.bert.encoder.layer.10.attention.output.LayerNorm | LayerNorm                     | 1 K   \n",
            "190 | model.bert.encoder.layer.10.attention.output.dropout   | Dropout                       | 0     \n",
            "191 | model.bert.encoder.layer.10.intermediate               | BertIntermediate              | 2 M   \n",
            "192 | model.bert.encoder.layer.10.intermediate.dense         | Linear                        | 2 M   \n",
            "193 | model.bert.encoder.layer.10.output                     | BertOutput                    | 2 M   \n",
            "194 | model.bert.encoder.layer.10.output.dense               | Linear                        | 2 M   \n",
            "195 | model.bert.encoder.layer.10.output.LayerNorm           | LayerNorm                     | 1 K   \n",
            "196 | model.bert.encoder.layer.10.output.dropout             | Dropout                       | 0     \n",
            "197 | model.bert.encoder.layer.11                            | BertLayer                     | 7 M   \n",
            "198 | model.bert.encoder.layer.11.attention                  | BertAttention                 | 2 M   \n",
            "199 | model.bert.encoder.layer.11.attention.self             | BertSelfAttention             | 1 M   \n",
            "200 | model.bert.encoder.layer.11.attention.self.query       | Linear                        | 590 K \n",
            "201 | model.bert.encoder.layer.11.attention.self.key         | Linear                        | 590 K \n",
            "202 | model.bert.encoder.layer.11.attention.self.value       | Linear                        | 590 K \n",
            "203 | model.bert.encoder.layer.11.attention.self.dropout     | Dropout                       | 0     \n",
            "204 | model.bert.encoder.layer.11.attention.output           | BertSelfOutput                | 592 K \n",
            "205 | model.bert.encoder.layer.11.attention.output.dense     | Linear                        | 590 K \n",
            "206 | model.bert.encoder.layer.11.attention.output.LayerNorm | LayerNorm                     | 1 K   \n",
            "207 | model.bert.encoder.layer.11.attention.output.dropout   | Dropout                       | 0     \n",
            "208 | model.bert.encoder.layer.11.intermediate               | BertIntermediate              | 2 M   \n",
            "209 | model.bert.encoder.layer.11.intermediate.dense         | Linear                        | 2 M   \n",
            "210 | model.bert.encoder.layer.11.output                     | BertOutput                    | 2 M   \n",
            "211 | model.bert.encoder.layer.11.output.dense               | Linear                        | 2 M   \n",
            "212 | model.bert.encoder.layer.11.output.LayerNorm           | LayerNorm                     | 1 K   \n",
            "213 | model.bert.encoder.layer.11.output.dropout             | Dropout                       | 0     \n",
            "214 | model.bert.pooler                                      | BertPooler                    | 590 K \n",
            "215 | model.bert.pooler.dense                                | Linear                        | 590 K \n",
            "216 | model.bert.pooler.activation                           | Tanh                          | 0     \n",
            "217 | model.dropout                                          | Dropout                       | 0     \n",
            "218 | model.classifier                                       | Linear                        | 1 K   \n",
            "219 | loss                                                   | CrossEntropyLoss              | 0     \n",
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a1008313041416c858cb9c5ea72c970",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:23: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` in the `DataLoader` init to improve performance.\n",
            "  warnings.warn(*args, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cb42e82f6d143b5b4f9d0dbfa98e33d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPu4WMEkobDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}