{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Improved Sentiment Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2oOtID0J9r0Wt2EuAdGiO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/100-Days-of-NLP/blob/applications%2Fsentiment/applications/classification/Improved%20Sentiment%20Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PKvXBhIokwF",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "Sentiment analysis refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NfbT1NFowOX",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## IMDB - Dataset of 50K Movie Reviews\n",
        "\n",
        "This is a dataset for binary sentiment classification containing a set of 25,000 highly polar movie reviews for training and 25,000 for testing.\n",
        "\n",
        "In the [previous notebook](https://github.com/graviraja/100-Days-of-NLP/blob/master/applications/classification/Simple%20Sentiment%20Analysis.ipynb), we explored the basic sentiment analysis using RNN. The `test_accuracy` is less than **`50%`**.\n",
        "\n",
        "Here we will explore few techniques that can help in increasing the accuracy of the model.\n",
        "\n",
        "We will try the following:\n",
        "\n",
        "- packed padded sequences\n",
        "- pre-trained word embeddings\n",
        "- different RNN architecture\n",
        "- bidirectional RNN\n",
        "- multi-layer RNN\n",
        "- regularization\n",
        "- a different optimizer\n",
        "\n",
        "If you are not aware of what:\n",
        " - packed padded sequences are, then checkout this [snippet](https://github.com/graviraja/pytorch-sample-codes/blob/master/pad_sequences.py)\n",
        "\n",
        " - embeddings are, then have a look into [this](https://github.com/graviraja/100-Days-of-NLP/tree/master/embeddings)\n",
        "\n",
        " - how multi-layer RNN (or) bidirectional RNN works, checkout my explaniation [here](https://github.com/graviraja/100-Days-of-NLP/blob/master/architectures/RNN.ipynb)\n",
        "\n",
        "- Dropout - [Andrew NG tutorial](https://www.youtube.com/watch?v=ARq74QuavAo)\n",
        "\n",
        "Reference:\n",
        "- [Ben Trevett Notebook](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb)\n",
        "\n",
        "This is the overview of the model:\n",
        "\n",
        "![arch](https://drive.google.com/uc?id=1yevVkm4nVt19aW36YQiehZWbJJwb7-7D)\n",
        "\n",
        "With that said let's get into code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmq3VSGdqNNv",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1auuB0fqPAX",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otrck-eWPRMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import torch\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TclYL24ZqRsU",
        "colab_type": "text"
      },
      "source": [
        "## Data Processing\n",
        "\n",
        "For padding sequences to work, RNN needs the sequence_lengths along with the sequences. By default the `FIELD` does not return lengths. We have to mention explicitly. Set the parameter **`include_lengths = True`** while declaring the FIELD. This will cause batch.text to now be a tuple with the first element being our sentence (a numericalized tensor that has been padded) and the second element being the actual lengths of our sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyBkQORdR7rc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize = 'spacy', include_lengths = True)\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVWi1EihrHJ8",
        "colab_type": "text"
      },
      "source": [
        "## IMDB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl3ywFguSak_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "318dd7e7-010d-4270-9df6-abf6d21cdabd"
      },
      "source": [
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:06<00:00, 12.1MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnLIpnrhSd-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQa6wjWY3Kiq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9f319608-30eb-43d7-8f22-f7c907c6bc83"
      },
      "source": [
        "\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 17500\n",
            "Number of validation examples: 7500\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFdCveSR3L3b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cfcd4460-d7cf-49fe-fb12-8d1d3a8de6de"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['Before', 'the', 'release', 'of', 'George', 'Romero', \"'s\", 'genre', '-', 'defining', 'Night', 'of', 'the', 'Living', 'Dead', ',', 'zombies', 'were', 'relatively', 'well', '-', 'behaved', 'creatures', '.', 'They', 'certainly', 'had', 'much', 'better', 'table', '-', 'manners', 'in', 'the', 'old', 'days', '.', 'But', 'social', 'etiquette', 'aside', 'what', 'thrills', 'did', 'these', 'early', 'zombies', 'offer', 'to', 'the', 'movie', '-', 'going', 'public', '?', 'Judging', 'by', 'this', 'film', ',', 'none', 'whatsoever.<br', '/><br', '/>The', 'story', 'is', 'about', 'an', 'expedition', 'to', 'Cambodia', ',', 'whose', 'purpose', 'is', 'to', 'find', 'and', 'destroy', 'the', 'secret', 'of', 'zombiefication', '.', 'One', 'of', 'the', 'party', 'discovers', 'the', 'secrets', 'on', 'his', 'own', 'and', 'sets', 'about', 'building', 'his', 'zombie', 'army.<br', '/><br', '/>This', 'film', 'is', 'basically', 'a', 'love', 'triangle', 'with', 'zombies', '.', 'But', 'seeing', 'as', 'this', 'is', 'a', '30', \"'s\", 'movie', ',', 'the', 'said', 'zombies', 'are', 'more', 'like', 'somnambulists', 'than', 'the', 'flesh', '-', 'eating', 'variety', 'we', 'think', 'of', 'today', '.', 'They', 'seem', 'to', 'respond', 'to', 'mind', '-', 'control', ',', 'rather', 'than', 'insatiable', 'appetites', '.', 'And', ',', 'quite', 'frankly', ',', 'the', \"'\", 'revolt', \"'\", 'is', 'somewhat', 'underwhelming', 'too', '.', 'The', 'whole', 'thing', 'is', 'really', 'very', 'dull', '.', 'Aside', 'from', 'the', 'lack', 'of', 'horror', ',', 'there', 'is', \"n't\", 'any', 'over', '-', 'the', '-', 'top', 'melodramatic', 'theatrics', 'to', 'keep', 'us', 'entertained', '.', 'It', 'seems', 'unlikely', 'that', 'this', 'could', \"'ve\", 'provided', 'much', 'entertainment', 'even', '70', 'years', 'ago', '.', 'See', 'it', 'if', 'you', 'have', 'to', 'see', 'everything', 'with', \"'\", 'zombie', \"'\", 'in', 'the', 'title', 'but', 'otherwise', 'I', 'would', 'advise', 'skipping', 'this', 'one', '.'], 'label': 'neg'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O45DSyNorLSi",
        "colab_type": "text"
      },
      "source": [
        "## Vocabulary\n",
        "\n",
        "\n",
        "Using of pre-trained word embeddings is one of the most important step in many SOTA models.\n",
        "\n",
        "#### Loading the Pre-trained Word Embeddings\n",
        "We get these vectors simply by specifying which vectors we want and passing it as an argument to build_vocab. TorchText handles downloading the vectors and associating them with the correct words in our vocabulary.\n",
        "\n",
        "By default, TorchText will initialize words in your vocabulary but not in your pre-trained embeddings to zero. We don't want this, and instead initialize them randomly by setting unk_init to torch.Tensor.normal_. This will now initialize those words via a Gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57xt34asSexs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "24c5b42e-98ad-4802-db44-707eb8eb9f9a"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                           \n",
            "100%|█████████▉| 399853/400000 [00:21<00:00, 18813.42it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZIlVArEtELk",
        "colab_type": "text"
      },
      "source": [
        "## Iterators\n",
        "\n",
        "Let’s create the iterators for our data.\n",
        "\n",
        "These can be iterated on to return a batch of data which will have a text attribute (the PyTorch tensors containing a batch of numericalized movie reviews) and a label attribute (the PyTorch tensors containing a batch of sentiment of movie reviews).\n",
        "\n",
        "We also need to replace the words by it’s indexes, since any model takes only numbers as input using the vocabulary.\n",
        "\n",
        "We use a BucketIterator instead of the standard Iterator as it creates batches in such a way that it minimizes the amount of padding.\n",
        "\n",
        "Another thing for packed padded sequences all of the tensors within a batch need to be sorted by their lengths. This is handled in the iterator by setting `sort_within_batch = True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RFYGyoLS0aO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrBXo1LTtQXN",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "\n",
        "The model used is a Bi-directional Multi Layer LSTM.\n",
        "\n",
        "![arch](https://drive.google.com/uc?id=1yevVkm4nVt19aW36YQiehZWbJJwb7-7D)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4hVE5EvS4Cn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.rnn = nn.LSTM(\n",
        "                        emb_dim,\n",
        "                        hidden_dim,\n",
        "                        num_layers=n_layers,\n",
        "                        bidirectional=bidirectional,\n",
        "                        dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, input, input_lengths):\n",
        "        # input => [seq_len, batch_size]\n",
        "        # input_lengths => [batch_size]\n",
        "        batch_size = input.shape[1]\n",
        "        embedded = self.embedding(input)\n",
        "        embedded = self.dropout(embedded)\n",
        "        # embedded => [seq_len, batch_size, emb_dim]\n",
        "\n",
        "        # packing the sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "\n",
        "        # unpacking the sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "        # output => [seq_len, batch_size, hidden_dim * num_dir]\n",
        "        # hidden => [num_layers * num_dir, batch_size, hidden_dim]\n",
        "        # cell => [num_layers * num_dir, batch_size, hidden_dim]\n",
        "\n",
        "        hidden = hidden.view(self.n_layers, -1 , batch_size, self.hidden_dim)\n",
        "        # [1, 2, b, h] => single layer bi dir   \n",
        "        #              => final layer forward hidden [-1][0][:] => [batch_size, hidden_dim]\n",
        "        #              => final layer backward hidden [-1][1][:] => [batch_size, hidden_dim]\n",
        "        # [2, 2, b, h] => multi layer bi dir    => [-1][0]  [-1][1]\n",
        "        #              => final layer forward hidden [-1][0][:] => [batch_size, hidden_dim]\n",
        "        #              => final layer backward hidden [-1][1][:] => [batch_size, hidden_dim]\n",
        "\n",
        "        # concatinating final forward and final backward hidden states\n",
        "        hidden = torch.cat((hidden[-1][0][:], hidden[-1][1][:]), dim=1)\n",
        "        # hidden => [batch_size, hidden_dim * num_dir(2)]\n",
        "\n",
        "        hidden = self.dropout(hidden)\n",
        "\n",
        "        logits = self.fc(hidden)\n",
        "        # logits => [batch_size, output_dim]\n",
        "        \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqWMES584obn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "674372c2-ac7f-4c35-c8e4-20ab1691d873"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "model"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(25002, 100, padding_idx=1)\n",
              "  (rnn): LSTM(100, 256, num_layers=2, dropout=0.5, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czR6ZgMr9N0J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48e68d13-c709-4f70-fe9f-d4e46902c466"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model)} trainable paramters')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 4810857 trainable paramters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7Px7cho9lUn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2eb5e2b-6e5d-475f-abb5-eb9a4a7dc119"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jKvY3cS97Q-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "75978d72-32ff-4f06-b061-d33f9be801de"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
              "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [-0.4414,  0.4593,  0.6607,  ..., -0.2775,  0.0806, -0.4131],\n",
              "        [-0.0608, -1.0925,  0.3700,  ..., -0.7628,  0.1420, -0.4692],\n",
              "        [ 0.1362, -0.9169,  0.1200,  ..., -0.0716,  0.8561, -0.9624]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUm97oF35WWB",
        "colab_type": "text"
      },
      "source": [
        "As our $<unk>$ and $<pad>$ token aren't in the pre-trained vocabulary they have been initialized using unk_init (an $\\mathcal{N}(0,1)$ distribution) when building our vocab. \n",
        "\n",
        "It is preferable to initialize them both to all zeros to explicitly tell our model that, initially, they are irrelevant for determining sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB9SAbcT-Ofi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3b5cfbc4-40b6-427b-de33-c9e3c8bddf9e"
      },
      "source": [
        "\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [-0.4414,  0.4593,  0.6607,  ..., -0.2775,  0.0806, -0.4131],\n",
            "        [-0.0608, -1.0925,  0.3700,  ..., -0.7628,  0.1420, -0.4692],\n",
            "        [ 0.1362, -0.9169,  0.1200,  ..., -0.0716,  0.8561, -0.9624]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj1EFXYy0pqU",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer & Criterion\n",
        "\n",
        "Optimizer will be used to update the parameters of the module. Here, we'll use **`Adam`**. The first argument is the parameters will be updated by the optimizer, the second is the learning rate, i.e. how much we'll change the parameters by when we do a parameter update. We will use the default learning rate\n",
        "\n",
        "Next, we'll define our loss function. In PyTorch this is commonly called a criterion.\n",
        "\n",
        "The loss function here is binary cross entropy with logits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUdhE-b4-Pnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXuSO2bs-gGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bnq0IO104m9",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy\n",
        "\n",
        "Since the labels are binary 0 and 1. Applying sigmoid on logits will convert the values to 0-1 scale. Then rounding it will give the value 0 or 1. Comparing with the ground truth lables will give the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn47woDc-q8o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    predicted = torch.round(torch.sigmoid(preds))\n",
        "    correct = (predicted == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0Ju1W-O0_QZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Training\n",
        "\n",
        "In training the following steps are performed:\n",
        "\n",
        "- keep the model in training mode\n",
        "- Iterate over batches\n",
        "- In each batch do the following:\n",
        "\n",
        "    - PyTorch does not automatically remove (or \"zero\") the gradients calculated from the last gradient calculation, so they must be manually zeroed. Use the method optimizer.zero_grad()\n",
        "    - Do the forward pass\n",
        "    - Calculate loss using the criterion\n",
        "    - Calculate the accuracy\n",
        "    - Perform the backward pass\n",
        "    - Update the parameters of the model\n",
        "    - Update the loss, accuracy of the epoch\n",
        "- Return the loss and accuracy of the epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-fv-3w1_C73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input, input_lengths = batch.text\n",
        "        preds = model(input, input_lengths)\n",
        "        preds = preds.squeeze(1)\n",
        "\n",
        "        loss = criterion(preds, batch.label)\n",
        "        acc = binary_accuracy(preds, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnDctfRp1PAK",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating\n",
        "\n",
        "In evaluation the following steps are performed:\n",
        "\n",
        "- keep the model in evaluation mode\n",
        "- we don't require to calculate the gradients in the evaluation mode.\n",
        "- Iterate over batches\n",
        "- In each batch do the following:\n",
        "    - Do the forward pass\n",
        "    - Calculate loss using the criterion\n",
        "    - Calculate the accuracy\n",
        "    - Update the loss, accuracy of the epoch\n",
        "- Return the loss and accuracy of the epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUZK8nJa_7iM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        input, input_lengths = batch.text\n",
        "        preds = model(input, input_lengths)\n",
        "        preds = preds.squeeze(1)\n",
        "        loss = criterion(preds, batch.label)\n",
        "        acc = binary_accuracy(preds, batch.label)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_95zI2EApKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzu6J_dB1huU",
        "colab_type": "text"
      },
      "source": [
        "## Running\n",
        "\n",
        "`N_EPOCHS`: num of epochs (iterations over the complete training dataset) to run over the model\n",
        "\n",
        "Save the model which has less validation loss.\n",
        "\n",
        "Caluclate the accuracy on the test_data using the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOSquWK5EH7o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "2aeb1e39-76aa-4294-9fce-8523994b37f5"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
        "    "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 1m 38s\n",
            "\tTrain Loss: 0.258 | Train Acc: 89.76%\n",
            "\t Val. Loss: 0.508 |  Val. Acc: 82.96%\n",
            "Epoch: 02 | Epoch Time: 1m 38s\n",
            "\tTrain Loss: 0.262 | Train Acc: 89.24%\n",
            "\t Val. Loss: 0.288 |  Val. Acc: 88.87%\n",
            "Epoch: 03 | Epoch Time: 1m 38s\n",
            "\tTrain Loss: 0.183 | Train Acc: 93.09%\n",
            "\t Val. Loss: 0.327 |  Val. Acc: 88.55%\n",
            "Epoch: 04 | Epoch Time: 1m 38s\n",
            "\tTrain Loss: 0.166 | Train Acc: 93.91%\n",
            "\t Val. Loss: 0.283 |  Val. Acc: 89.81%\n",
            "Epoch: 05 | Epoch Time: 1m 37s\n",
            "\tTrain Loss: 0.139 | Train Acc: 94.83%\n",
            "\t Val. Loss: 0.281 |  Val. Acc: 90.32%\n",
            "Test Loss: 0.300 | Test Acc: 89.22%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2ELF-Vz1mK1",
        "colab_type": "text"
      },
      "source": [
        "## Inference\n",
        "\n",
        "We can now use our model to predict the sentiment of any sentence we give it. As it has been trained on movie reviews, the sentences provided should also be movie reviews.\n",
        "\n",
        "Our `inference` function does a few things:\n",
        "\n",
        "- sets the model to evaluation mode\n",
        "- tokenizes the sentence, i.e. splits it from a raw string into a list of tokens\n",
        "- indexes the tokens by converting them into their integer representation from our vocabulary\n",
        "- gets the length of our sequence\n",
        "- converts the indexes, which are a Python list into a PyTorch tensor\n",
        "- add a batch dimension by unsqueezeing\n",
        "- converts the length into a tensor\n",
        "- squashes the output prediction from a real number between 0 and 1 with the sigmoid function\n",
        "- converts the tensor holding a single value into an integer with the item() method\n",
        "\n",
        "We are expecting reviews with a negative sentiment to return a value close to 0 and positive reviews to return a value close to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft2wfBhhFOzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def inference(model, sentence):\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Eh298h2Pjx",
        "colab_type": "text"
      },
      "source": [
        "Example of negative review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9y6cvbX2LtB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a58ef4a3-1341-4f03-8120-bcf5e9429938"
      },
      "source": [
        "inference(model, \"This film is terrible\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.012477142736315727"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8yiEBG_2SPq",
        "colab_type": "text"
      },
      "source": [
        "Example of positive review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfqt_icA2Uu2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d39f3cf-f654-4f8a-c85b-1ae5d918cd97"
      },
      "source": [
        "inference(model, \"I love this film\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7550132274627686"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}