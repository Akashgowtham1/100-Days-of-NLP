{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Sentiment Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOGqMaappGT7sHM1QmdVY6n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/100-Days-of-NLP/blob/applications%2Fsentiment/applications/classification/Simple%20Sentiment%20Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUUGA1qg10OS",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "Sentiment analysis refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.\n",
        "\n",
        "## IMDB - Dataset of 50K Movie Reviews\n",
        "\n",
        "This is a dataset for binary sentiment classification containing a set of 25,000 highly polar movie reviews for training and 25,000 for testing. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNMi5IVN1o82",
        "colab_type": "text"
      },
      "source": [
        "This notebook covers the basic workflow. We'll learn how to: load data, create train/test/validation splits, build a vocabulary, create data iterators, define a model and implement the train/evaluate/test loop.\n",
        "\n",
        "The model used is a simple RNN network\n",
        "\n",
        "![arch](https://drive.google.com/uc?id=1LcoUUyg3S7JBII4buOHVVj46LU7nLCPt)\n",
        "\n",
        "Resources:\n",
        "\n",
        "- [IMDB dataset](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
        "- [Ben Trevett Sentiment analysis](https://github.com/bentrevett/pytorch-sentiment-analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFI6SNZ42S7H",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "We'll be using a recurrent neural network (RNN) for analysing the text. If you are not aware of what an RNN is then check out my notebook on [RNN](https://github.com/graviraja/100-Days-of-NLP/blob/master/architectures/RNN.ipynb)\n",
        "\n",
        "Let's first see the basic equations of RNN,\n",
        "\n",
        "The input to the RNN is a Sequence $X = \\{x_1, x_2,...., x_t\\}$ and the hidden states, $$H = \\{h_1, h_2,...., h_t\\}$$ are calcualted using the following equation:\n",
        "\n",
        "$$h_t = RNN(x_t, h_{t-1})$$ \n",
        "\n",
        "Once the final hidden state $h_t$ is calculated, it will be fed through a linear layer, $f$ for predicting the sentiment $\\hat{y} = f(h_t)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX2t6IWeQCVU",
        "colab_type": "text"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzDJZ0B5QDnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import random\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext import data, datasets\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOpISZNgQX1s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04748d3a-36dd-4c07-8da7-e35920b10caf"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke5zkOhIL2wM",
        "colab_type": "text"
      },
      "source": [
        "## Field\n",
        "Field define how the data should be processed.\n",
        "The parameters of a Field specify how the data should be processed.\n",
        "\n",
        "We use the TEXT field to define how the review should be processed, and the LABEL field to process the sentiment.\n",
        "\n",
        "Our TEXT field has tokenize='spacy' as an argument. This defines that the \"tokenization\" (the act of splitting the string into discrete \"tokens\") should be done using the spaCy tokenizer.\n",
        "If no tokenize argument is passed, the default is simply splitting the string on spaces.\n",
        "\n",
        "LABEL is defined by a LabelField, a special subset of the Field class specifically used for handling labels.\n",
        "\n",
        "For more on Fields, go [here](https://pytorch.org/text/data.html#field)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBFwxFVRMbaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check the code documentation for more parameters\n",
        "data.Field??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvlvlFLeQcw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize = 'spacy')\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKLBZNGVNE2U",
        "colab_type": "text"
      },
      "source": [
        "## IMDB dataset\n",
        "\n",
        "`TorchText` supports various datasets used in NLP. Checkout all the datasets supported [here](https://pytorch.org/text/datasets.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SCqLGZwRCWm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b01f2de0-a9c9-475a-fcfb-a661411569b6"
      },
      "source": [
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\raclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:03<00:00, 25.3MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0jWqctzROLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d60bd872-f02e-4516-8154-c2aa93d29b8c"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 25000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_UomjNgRTjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXq20DpFRWc_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "24d9d8b3-b24e-4566-cb68-c95903dc0669"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 17500\n",
            "Number of validation examples: 7500\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEueBrqSNbUZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1ea8899f-05cf-4f50-c9b5-c5f90b07fc9b"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['Before', 'the', 'release', 'of', 'George', 'Romero', \"'s\", 'genre', '-', 'defining', 'Night', 'of', 'the', 'Living', 'Dead', ',', 'zombies', 'were', 'relatively', 'well', '-', 'behaved', 'creatures', '.', 'They', 'certainly', 'had', 'much', 'better', 'table', '-', 'manners', 'in', 'the', 'old', 'days', '.', 'But', 'social', 'etiquette', 'aside', 'what', 'thrills', 'did', 'these', 'early', 'zombies', 'offer', 'to', 'the', 'movie', '-', 'going', 'public', '?', 'Judging', 'by', 'this', 'film', ',', 'none', 'whatsoever.<br', '/><br', '/>The', 'story', 'is', 'about', 'an', 'expedition', 'to', 'Cambodia', ',', 'whose', 'purpose', 'is', 'to', 'find', 'and', 'destroy', 'the', 'secret', 'of', 'zombiefication', '.', 'One', 'of', 'the', 'party', 'discovers', 'the', 'secrets', 'on', 'his', 'own', 'and', 'sets', 'about', 'building', 'his', 'zombie', 'army.<br', '/><br', '/>This', 'film', 'is', 'basically', 'a', 'love', 'triangle', 'with', 'zombies', '.', 'But', 'seeing', 'as', 'this', 'is', 'a', '30', \"'s\", 'movie', ',', 'the', 'said', 'zombies', 'are', 'more', 'like', 'somnambulists', 'than', 'the', 'flesh', '-', 'eating', 'variety', 'we', 'think', 'of', 'today', '.', 'They', 'seem', 'to', 'respond', 'to', 'mind', '-', 'control', ',', 'rather', 'than', 'insatiable', 'appetites', '.', 'And', ',', 'quite', 'frankly', ',', 'the', \"'\", 'revolt', \"'\", 'is', 'somewhat', 'underwhelming', 'too', '.', 'The', 'whole', 'thing', 'is', 'really', 'very', 'dull', '.', 'Aside', 'from', 'the', 'lack', 'of', 'horror', ',', 'there', 'is', \"n't\", 'any', 'over', '-', 'the', '-', 'top', 'melodramatic', 'theatrics', 'to', 'keep', 'us', 'entertained', '.', 'It', 'seems', 'unlikely', 'that', 'this', 'could', \"'ve\", 'provided', 'much', 'entertainment', 'even', '70', 'years', 'ago', '.', 'See', 'it', 'if', 'you', 'have', 'to', 'see', 'everything', 'with', \"'\", 'zombie', \"'\", 'in', 'the', 'title', 'but', 'otherwise', 'I', 'would', 'advise', 'skipping', 'this', 'one', '.'], 'label': 'neg'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MxqQI_ZNYAZ",
        "colab_type": "text"
      },
      "source": [
        "## Vocabulary\n",
        "\n",
        "Next, we have to build a vocabulary. \n",
        "\n",
        "With Torchtext’s Field that is extremely simple. we don’t need to worry about creating dicts, mapping word to index, mapping index to word, counting the words etc. All these things are done by the Field for us.\n",
        "\n",
        "We can define the minimum frequency of the words by specifying the attribute min_freq in build_vocab method of Field. Tokens that appear less the min_freq are converted into an `<unk>` (unknown) token.\n",
        "\n",
        "*Note : We will use only training data for creating the vocabulary*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO5IEiTSSCcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWZ7cK1WSZ1c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3d7820cb-1450-48da-84e0-5cb0b8c3c642"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt4NKPrcON35",
        "colab_type": "text"
      },
      "source": [
        "Why is the vocab size 25002 and not 25000? One of the addition tokens is the `<unk>` token and the other is a `<pad>` token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p777rPTuOXwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fa9d1739-2659-4c59-b6bd-a630bd89fe2e"
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(10))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 202394), (',', 192899), ('.', 165582), ('and', 109516), ('a', 109451), ('of', 100959), ('to', 93453), ('is', 76282), ('in', 61204), ('I', 54045)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5kReGsfTffR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d028135-9ce2-4dc9-a12c-7a9e686bd783"
      },
      "source": [
        "print(TEXT.vocab.itos[:10])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX-b_NP4TDxp",
        "colab_type": "text"
      },
      "source": [
        "## Iterators\n",
        "\n",
        "Let’s create the iterators for our data.\n",
        "\n",
        "These can be iterated on to return a batch of data which will have a text attribute (the PyTorch tensors containing a batch of numericalized movie reviews) and a label attribute (the PyTorch tensors containing a batch of sentiment of movie reviews).\n",
        "\n",
        "We also need to replace the words by it’s indexes, since any model takes only numbers as input using the vocabulary.\n",
        "\n",
        "We use a BucketIterator instead of the standard Iterator as it creates batches in such a way that it minimizes the amount of padding.\n",
        "\n",
        "This can be done as following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kb6jX76SgIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqrfTgTcW2uR",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "\n",
        "We will be using simple RNN model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5vTflkdSlrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim,  output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # embedding layer\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        # rnn layer\n",
        "        self.rnn = nn.RNN(emb_dim, hidden_dim)\n",
        "\n",
        "        # prediction layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # data => [seq_len, batch_size]\n",
        "\n",
        "        embedded = self.embedding(data)\n",
        "        # embedded => [seq_len, batch_size, emb_dim]\n",
        "\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        # output => [seq_len, batch_size, hid_dim]\n",
        "        # hidden => [1, batch_size, hid_dim]\n",
        "\n",
        "        logits = self.fc(hidden.squeeze(0))\n",
        "        # logits => [batch_size, output_dim]\n",
        "        # logits => [batch_size, 1]\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPO5X-xqTANL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9uPQxtmTC_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62bd8548-451b-4436-da46-14e09c2972bb"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,592,105 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTaXv1i5W-KI",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer & Criterion\n",
        "\n",
        "Optimizer will be used to update the parameters of the module. Here, we'll use stochastic gradient descent (SGD). The first argument is the parameters will be updated by the optimizer, the second is the learning rate, i.e. how much we'll change the parameters by when we do a parameter update.\n",
        "\n",
        "Next, we'll define our loss function. In PyTorch this is commonly called a criterion.\n",
        "\n",
        "The loss function here is binary cross entropy with logits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC_47v7lTHMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YwhDZAEXhSf",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy\n",
        "\n",
        "Since the labels are binary `0` and `1`. Applying sigmoid on `logits` will convert the values to `0-1` scale. Then rounding it will give the value `0` or `1`. Comparing with the ground truth lables will give the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLOlPwZYTI5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z7YyIpQYjkI",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "In training the following steps are performed:\n",
        "\n",
        "- keep the model in training mode (not useful here but a good practice)\n",
        "- Iterate over batches\n",
        "- In each batch do the following:\n",
        "    - PyTorch does not automatically remove (or \"zero\") the gradients calculated from the last gradient calculation, so they must be manually zeroed. Use the method `optimizer.zero_grad()`\n",
        "    - Do the forward pass \n",
        "    - Calculate loss using the `criterion`\n",
        "    - Calculate the accuracy \n",
        "    - Perform the `backward` pass\n",
        "    - Update the parameters of the model\n",
        "    - Update the loss, accuracy of the epoch\n",
        "\n",
        "- Return the loss and accuracy of the epoch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHNGDGF7TKwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_Pu5YWJYoSm",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating\n",
        "\n",
        "In evaluation the following steps are performed:\n",
        "\n",
        "- keep the model in evaluation mode (not useful here but a good practice)\n",
        "- we don't require to calculate the gradients in the evaluation mode. \n",
        "- Iterate over batches\n",
        "- In each batch do the following:\n",
        "    - Do the forward pass \n",
        "    - Calculate loss using the `criterion`\n",
        "    - Calculate the accuracy \n",
        "    - Update the loss, accuracy of the epoch\n",
        "\n",
        "- Return the loss and accuracy of the epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBv5RdIjTN0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--jSIryTTQJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for caluclating the time take to run each epoch\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyoN6HZZaPax",
        "colab_type": "text"
      },
      "source": [
        "## Running\n",
        "\n",
        "`N_EPOCHS`: num of epochs (iterations over the complete training dataset) to run over the model\n",
        "\n",
        "Save the model which has less validation loss. \n",
        "\n",
        "Caluclate the accuracy on the `test_data` using the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znN04slATSV5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "a664a830-5535-4f3e-fd71-d2e3b8008dfc"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
        "\n",
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.694 | Train Acc: 50.41%\n",
            "\t Val. Loss: 0.697 |  Val. Acc: 50.21%\n",
            "Epoch: 02 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.04%\n",
            "\t Val. Loss: 0.697 |  Val. Acc: 49.95%\n",
            "Epoch: 03 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.93%\n",
            "\t Val. Loss: 0.697 |  Val. Acc: 50.72%\n",
            "Epoch: 04 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.693 | Train Acc: 49.84%\n",
            "\t Val. Loss: 0.697 |  Val. Acc: 50.16%\n",
            "Epoch: 05 | Epoch Time: 0m 18s\n",
            "\tTrain Loss: 0.693 | Train Acc: 50.13%\n",
            "\t Val. Loss: 0.697 |  Val. Acc: 50.59%\n",
            "Test Loss: 0.710 | Test Acc: 47.48%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpK_MDyvardy",
        "colab_type": "text"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "As we can see the `accuracy` of the model on test_data is less than `50%`. Since the model we used is pretty basic there are a lot of tweeks we can do. In the later notebooks we will try the following:\n",
        "\n",
        "- packed padded sequences\n",
        "- pre-trained word embeddings\n",
        "- different RNN architecture\n",
        "- bidirectional RNN\n",
        "- multi-layer RNN\n",
        "- regularization\n",
        "- a different optimizer"
      ]
    }
  ]
}