{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLI with BiLSTM.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2R/Oxxl9uoLs9UTNWa7i+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/100-Days-of-NLP/blob/applications%2Fclassification/applications/classification/natural_language_inference/NLI%20with%20BiLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bChD0BpGIMwF",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Inference\n",
        "\n",
        "The goal of natural language inference (NLI), a widely-studied natural language processing task, is to determine if one given statement (a premise) semantically entails another given statement (a hypothesis).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXRju3STIRdA",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br_3ppk7qdRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext import data, datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7oz1RNmq5Pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j52oeQQcITPj",
        "colab_type": "text"
      },
      "source": [
        "## Fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obkiW8JTq8mA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize = 'spacy', lower = True)\n",
        "LABEL = data.LabelField()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohmZRkVrIUpf",
        "colab_type": "text"
      },
      "source": [
        "## SNLI (Stanford Natural Language Inference) Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp_5khtprDGW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "90ecac0e-0691-4d2c-b69b-09e719261b39"
      },
      "source": [
        "train_data, valid_data, test_data = datasets.SNLI.splits(TEXT, LABEL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading snli_1.0.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "snli_1.0.zip: 100%|██████████| 94.6M/94.6M [00:44<00:00, 2.11MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "extracting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izph38KHrEvG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "ffb8b56b-9b40-4c3c-fb6c-1a1f8dd0831c"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 549367\n",
            "Number of validation examples: 9842\n",
            "Number of testing examples: 9824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4GRwSiMrGtd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "d4c4937f-8b01-4a95-be7d-900eb9604f35"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'premise': ['a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.'], 'hypothesis': ['a', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition', '.'], 'label': 'neutral'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8d5lFWxIc7H",
        "colab_type": "text"
      },
      "source": [
        "## Building Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4rIP6hRrIgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MIN_FREQ = 10\n",
        "\n",
        "TEXT.build_vocab(train_data, min_freq = MIN_FREQ)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pStFCZ0rNFK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b206f58a-dd7b-4f7b-9172-b3bda41f0eac"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 12193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXNLkVCtrO5M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c98aef80-bd98-4811-f544-8eff460b9a6e"
      },
      "source": [
        "print(LABEL.vocab.itos)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['entailment', 'contradiction', 'neutral']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c64xop44rR1E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "012f51a4-04bd-4e86-c79a-942291e672cf"
      },
      "source": [
        "print(LABEL.vocab.freqs.most_common())"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('entailment', 183416), ('contradiction', 183187), ('neutral', 182764)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLUqRgO3IgAq",
        "colab_type": "text"
      },
      "source": [
        "## Data Iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMDCqErSrT58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nHooPDMIhuS",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "![](https://drive.google.com/uc?id=1DfMfzLXSpbTRITVt3yuOPeIhHBo4-lxg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Wr0riQjrXow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, n_linear_layers, output_dim, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, bidirectional=True, dropout=dropout)\n",
        "\n",
        "        d_model = hid_dim * 4\n",
        "        self.fcs = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(n_linear_layers)])\n",
        "        self.layer_norms = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(n_linear_layers)])\n",
        "        self.out = nn.Linear(d_model, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, premise, hypothesis):\n",
        "        # premise => [prem_seq_len, batch_size]\n",
        "        # hypothesis => [hypo_seq_len, batch_size]\n",
        "\n",
        "        embedded_prem = self.dropout(self.embedding(premise))\n",
        "        # embedded_prem => [prem_seq_len, batch_size, emb_dim]\n",
        "\n",
        "        embedded_hypo = self.dropout(self.embedding(hypothesis))\n",
        "        # embedded_hypo => [hypo_seq_len, batch_size, emb_dim]\n",
        "\n",
        "        outputs_prem, (hidden_prem, cell_prem) = self.rnn(embedded_prem)\n",
        "        # outputs_prem => [prem_seq_len, batch_size, hid_dim * 2]\n",
        "        # hidden_prem, cell_prem => [n_layers * num_dir, batch_size, hidden_dim]\n",
        "\n",
        "        outputs_hypo, (hidden_hypo, cell_hypo) = self.rnn(embedded_hypo)\n",
        "        # outputs_hypo => [hypo_seq_len, batch_size, hid_dim * 2]\n",
        "        # hidden_hypo, cell_hypo => [n_layers * num_dir, batch_size, hidden_dim]\n",
        "        \n",
        "        # combine the final hidden states\n",
        "        prem_representation = torch.cat((hidden_prem[-1], hidden_prem[-2]), dim=-1)\n",
        "        hypo_representation = torch.cat((hidden_hypo[-1], hidden_hypo[-2]), dim=-1)\n",
        "        # representation => [batch_size, hid_dim * 2]\n",
        "\n",
        "        hidden = torch.cat((prem_representation, hypo_representation), dim=-1)\n",
        "        # hidden => [batch_size, hidden_dim * 4]\n",
        "        #        => [batch_size, d_model]\n",
        "\n",
        "        for fc, norm in zip(self.fcs, self.layer_norms):\n",
        "            hidden_ = fc(hidden)\n",
        "            hidden_ = self.dropout(hidden_)\n",
        "            # residual connection\n",
        "            hidden = hidden + F.relu(hidden_)\n",
        "            # layer normalization\n",
        "            hidden = norm(hidden)\n",
        "        \n",
        "        logits = self.out(hidden)\n",
        "        # logits => [batch_size, output_dim]\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAVQerriyVJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 50\n",
        "HIDDEN_DIM = 100\n",
        "N_LSTM_LAYERS = 2\n",
        "N_FC_LAYERS = 3\n",
        "OUTPUT_DIM = len(LABEL.vocab)\n",
        "DROPOUT = 0.3\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = BiLSTM(\n",
        "    INPUT_DIM,\n",
        "    EMBEDDING_DIM,\n",
        "    HIDDEN_DIM,\n",
        "    N_LSTM_LAYERS,\n",
        "    N_FC_LAYERS,\n",
        "    OUTPUT_DIM,\n",
        "    DROPOUT,\n",
        "    PAD_IDX).to(device)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V82yTCJR54Y8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "3aa4cfae-8dae-4ca3-b534-0288227f8607"
      },
      "source": [
        "def init_weights(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM(\n",
              "  (embedding): Embedding(12193, 50, padding_idx=1)\n",
              "  (rnn): LSTM(50, 100, num_layers=2, dropout=0.3, bidirectional=True)\n",
              "  (fcs): ModuleList(\n",
              "    (0): Linear(in_features=400, out_features=400, bias=True)\n",
              "    (1): Linear(in_features=400, out_features=400, bias=True)\n",
              "    (2): Linear(in_features=400, out_features=400, bias=True)\n",
              "  )\n",
              "  (layer_norms): ModuleList(\n",
              "    (0): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (out): Linear(in_features=400, out_features=3, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1V2iYymw5BeD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "64d8019e-c708-4be1-b4dc-3a1e99f2f45b"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 1,457,653 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xU-1_ufImzd",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer & Loss Criterion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_CBFECq5ELz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hj3DBlXIsem",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsiQ8E-c58nJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7BmVMOpIu12",
        "colab_type": "text"
      },
      "source": [
        "## Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvZlF5IL6CGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        prem = batch.premise\n",
        "        hypo = batch.hypothesis\n",
        "        labels = batch.label\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(prem, hypo)\n",
        "        \n",
        "        # predictions => [batch size, output dim]\n",
        "        # labels => [batch size]\n",
        "    \n",
        "        loss = criterion(predictions, labels)            \n",
        "        acc = categorical_accuracy(predictions, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcsZeyJFIwuq",
        "colab_type": "text"
      },
      "source": [
        "## Validation Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liLiDHMQ6NJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            prem = batch.premise\n",
        "            hypo = batch.hypothesis\n",
        "            labels = batch.label\n",
        "                        \n",
        "            predictions = model(prem, hypo)\n",
        "            \n",
        "            loss = criterion(predictions, labels)\n",
        "                \n",
        "            acc = categorical_accuracy(predictions, labels)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YniTStu6P46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOIF6KqaIzPk",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejXIgRQe6R6N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "3b103b40-8ef1-49d0-a0f5-af3274514bae"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 4m 19s\n",
            "\tTrain Loss: 0.812 | Train Acc: 62.66%\n",
            "\t Val. Loss: 0.734 |  Val. Acc: 68.38%\n",
            "Epoch: 02 | Epoch Time: 4m 20s\n",
            "\tTrain Loss: 0.701 | Train Acc: 70.09%\n",
            "\t Val. Loss: 0.662 |  Val. Acc: 72.34%\n",
            "Epoch: 03 | Epoch Time: 4m 20s\n",
            "\tTrain Loss: 0.651 | Train Acc: 72.70%\n",
            "\t Val. Loss: 0.630 |  Val. Acc: 74.13%\n",
            "Epoch: 04 | Epoch Time: 4m 20s\n",
            "\tTrain Loss: 0.620 | Train Acc: 74.31%\n",
            "\t Val. Loss: 0.632 |  Val. Acc: 74.74%\n",
            "Epoch: 05 | Epoch Time: 4m 21s\n",
            "\tTrain Loss: 0.598 | Train Acc: 75.30%\n",
            "\t Val. Loss: 0.616 |  Val. Acc: 75.72%\n",
            "Epoch: 06 | Epoch Time: 4m 21s\n",
            "\tTrain Loss: 0.582 | Train Acc: 76.12%\n",
            "\t Val. Loss: 0.602 |  Val. Acc: 75.94%\n",
            "Epoch: 07 | Epoch Time: 4m 20s\n",
            "\tTrain Loss: 0.567 | Train Acc: 76.84%\n",
            "\t Val. Loss: 0.600 |  Val. Acc: 76.40%\n",
            "Epoch: 08 | Epoch Time: 4m 19s\n",
            "\tTrain Loss: 0.555 | Train Acc: 77.39%\n",
            "\t Val. Loss: 0.601 |  Val. Acc: 76.82%\n",
            "Epoch: 09 | Epoch Time: 4m 20s\n",
            "\tTrain Loss: 0.544 | Train Acc: 77.90%\n",
            "\t Val. Loss: 0.589 |  Val. Acc: 76.99%\n",
            "Epoch: 10 | Epoch Time: 4m 20s\n",
            "\tTrain Loss: 0.534 | Train Acc: 78.37%\n",
            "\t Val. Loss: 0.601 |  Val. Acc: 77.06%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N47RTdF6I1Mh",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrF2ajUt6WwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aa052357-257a-43c9-ce25-8f43d8bcc90a"
      },
      "source": [
        "model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.584 |  Test Acc: 76.84%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-uaXBsdI3f4",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njr-MMuL6ZRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference(premise, hypothesis, text_field, label_field, model, device):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    if isinstance(premise, str):\n",
        "        premise = text_field.tokenize(premise)\n",
        "    \n",
        "    if isinstance(hypothesis, str):\n",
        "        hypothesis = text_field.tokenize(hypothesis)\n",
        "    \n",
        "    if text_field.lower:\n",
        "        premise = [t.lower() for t in premise]\n",
        "        hypothesis = [t.lower() for t in hypothesis]\n",
        "\n",
        "    # numericalize  \n",
        "    premise = [text_field.vocab.stoi[t] for t in premise]\n",
        "    hypothesis = [text_field.vocab.stoi[t] for t in hypothesis]\n",
        "    \n",
        "    # convert into tensors\n",
        "    premise = torch.LongTensor(premise).unsqueeze(1).to(device)\n",
        "    # premise => [prem_len, 1]\n",
        "    hypothesis = torch.LongTensor(hypothesis).unsqueeze(1).to(device)\n",
        "    # hypothesis => [hypo_len, 1]\n",
        "\n",
        "    prediction = model(premise, hypothesis)\n",
        "    prediction = prediction.argmax(dim=-1).item()\n",
        "\n",
        "    return label_field.vocab.itos[prediction]"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdvGdOs86uSb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "380b78b9-31b9-4af4-fd8f-162ec7af126b"
      },
      "source": [
        "premise = 'A woman selling bamboo sticks talking to two men on a loading dock.'\n",
        "hypothesis = 'There are at least three people on a loading dock.'\n",
        "\n",
        "inference(premise, hypothesis, TEXT, LABEL, model, device)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'entailment'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x0aRzqWGyy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59678959-092a-4579-c661-f2c70198737e"
      },
      "source": [
        "premise = 'A woman selling bamboo sticks talking to two men on a loading dock.'\n",
        "hypothesis = 'A woman is selling bamboo sticks to help provide for her family.'\n",
        "\n",
        "inference(premise, hypothesis, TEXT, LABEL, model, device)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'neutral'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8FhxZwvGzXO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e6e8df48-9502-48b3-c5e2-231fb772268c"
      },
      "source": [
        "premise = 'A woman selling bamboo sticks talking to two men on a loading dock.'\n",
        "hypothesis = ' A woman is not taking money for any of her sticks.'\n",
        "\n",
        "inference(premise, hypothesis, TEXT, LABEL, model, device)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'contradiction'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    }
  ]
}