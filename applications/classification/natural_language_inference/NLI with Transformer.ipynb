{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLI with Transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOOVgsHE0zqPG/lldzupL3L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/100-Days-of-NLP/blob/applications%2Fclassification/applications/classification/natural_language_inference/NLI%20with%20Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bChD0BpGIMwF",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Inference\n",
        "\n",
        "The goal of natural language inference (NLI), a widely-studied natural language processing task, is to determine if one given statement (a premise) semantically entails another given statement (a hypothesis).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXRju3STIRdA",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br_3ppk7qdRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "\n",
        "from torchtext import data, datasets, vocab"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7oz1RNmq5Pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j52oeQQcITPj",
        "colab_type": "text"
      },
      "source": [
        "## Fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obkiW8JTq8mA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize = 'spacy', lower = True)\n",
        "LABEL = data.LabelField()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohmZRkVrIUpf",
        "colab_type": "text"
      },
      "source": [
        "## SNLI (Stanford Natural Language Inference) Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp_5khtprDGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data, test_data = datasets.SNLI.splits(TEXT, LABEL)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izph38KHrEvG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d4713d0d-e479-4555-cde5-1a98d97dcca1"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 549367\n",
            "Number of validation examples: 9842\n",
            "Number of testing examples: 9824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4GRwSiMrGtd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "2d4aa169-c4cd-43b7-d05b-eed4d36ac1f0"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'premise': ['a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.'], 'hypothesis': ['a', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition', '.'], 'label': 'neutral'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8d5lFWxIc7H",
        "colab_type": "text"
      },
      "source": [
        "## Building Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4rIP6hRrIgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MIN_FREQ = 10\n",
        "\n",
        "TEXT.build_vocab(train_data, min_freq = MIN_FREQ)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pStFCZ0rNFK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d093b80e-6268-4132-8756-f0e8c1c3ffdf"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 12193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXNLkVCtrO5M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a892c90e-95b8-480f-a282-6ad2d9a25054"
      },
      "source": [
        "print(LABEL.vocab.itos)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['entailment', 'contradiction', 'neutral']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c64xop44rR1E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "19b960a8-24f4-43f6-f611-97eefde9f59c"
      },
      "source": [
        "print(LABEL.vocab.freqs.most_common())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('entailment', 183416), ('contradiction', 183187), ('neutral', 182764)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLUqRgO3IgAq",
        "colab_type": "text"
      },
      "source": [
        "## Data Iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMDCqErSrT58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-p90c-w2o9I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "488b746e-40dc-4df9-b0ce-1ed9efef812f"
      },
      "source": [
        "# sample check\n",
        "sample = next(iter(valid_iterator))\n",
        "sample.premise.shape, sample.hypothesis.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([7, 128]), torch.Size([7, 128]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nHooPDMIhuS",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "![](https://drive.google.com/uc?id=1vc_Bg0WSMEBZhdNx7JdJXbhYTbaRDbke)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh0DNUWp8MRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Wr0riQjrXow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, n_head, hid_dim, n_layers, n_linear_layers, output_dim, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pad_idx = pad_idx\n",
        "        self.d_model = d_model\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, d_model, padding_idx=pad_idx)\n",
        "        \n",
        "        encoder_layers = TransformerEncoderLayer(d_model, n_head, hid_dim, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers)\n",
        "\n",
        "        self.fcs = nn.ModuleList([nn.Linear(d_model * 2, d_model * 2) for _ in range(n_linear_layers)])\n",
        "        self.layer_norms = nn.ModuleList([nn.LayerNorm(d_model * 2) for _ in range(n_linear_layers)])\n",
        "        \n",
        "        self.out = nn.Linear(d_model * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def create_mask(self, seq):\n",
        "        # seq => [seq_len, batch_size]\n",
        "        \n",
        "        mask = (seq == self.pad_idx)\n",
        "        mask = mask.permute(1, 0)\n",
        "        # mask => [batch_size, seq_len]\n",
        "\n",
        "    def forward(self, premise, hypothesis):\n",
        "        # premise => [prem_seq_len, batch_size]\n",
        "        # hypothesis => [hypo_seq_len, batch_size]\n",
        "\n",
        "        # create input masks\n",
        "        prem_mask = self.create_mask(premise)\n",
        "        # prem_mask => [batch_size, prem_seq_len]\n",
        "        hypo_mask = self.create_mask(hypothesis)\n",
        "        # hypo_mask => [batch_size, hypo_seq_len]\n",
        "\n",
        "        embedded_prem = self.dropout(self.embedding(premise)) * math.sqrt(self.d_model)\n",
        "        # embedded_prem => [prem_seq_len, batch_size, emb_dim]\n",
        "\n",
        "        embedded_hypo = self.dropout(self.embedding(hypothesis)) * math.sqrt(self.d_model)\n",
        "        # embedded_hypo => [hypo_seq_len, batch_size, emb_dim]\n",
        "        \n",
        "        embedded_prem = self.pos_encoder(embedded_prem)\n",
        "        embedded_hypo = self.pos_encoder(embedded_hypo)\n",
        "\n",
        "        outputs_prem = self.transformer_encoder(embedded_prem, src_key_padding_mask=prem_mask)\n",
        "        # outputs_prem => [prem_seq_len, batch_size, d_model]\n",
        "\n",
        "        outputs_hypo = self.transformer_encoder(embedded_hypo, src_key_padding_mask=hypo_mask)\n",
        "        # outputs_hypo => [hypo_seq_len, batch_size, d_model]\n",
        "        \n",
        "        # add the representation through attention\n",
        "        prem_representation = self.dropout(torch.sum(outputs_prem, dim=0))\n",
        "        hypo_representation = self.dropout(torch.sum(outputs_hypo, dim=0))\n",
        "        # representation => [batch_size, d_model]\n",
        "\n",
        "        hidden = torch.cat((prem_representation, hypo_representation), dim=-1)\n",
        "        # hidden => [batch_size, d_model * 2]\n",
        "\n",
        "        for fc, norm in zip(self.fcs, self.layer_norms):\n",
        "            hidden_ = fc(hidden)\n",
        "            hidden_ = self.dropout(hidden_)\n",
        "            # residual connection\n",
        "            hidden = hidden + F.relu(hidden_)\n",
        "            # layer normalization\n",
        "            hidden = norm(hidden)\n",
        "        \n",
        "        logits = self.out(hidden)\n",
        "        # logits => [batch_size, output_dim]\n",
        "\n",
        "        return logits\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAVQerriyVJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "D_MODEL = 128\n",
        "N_HEAD = 8\n",
        "HIDDEN_DIM = 200\n",
        "N_LAYERS = 3\n",
        "N_FC_LAYERS = 3\n",
        "OUTPUT_DIM = len(LABEL.vocab)\n",
        "DROPOUT = 0.3\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = TransformerModel(\n",
        "    INPUT_DIM,\n",
        "    D_MODEL,\n",
        "    N_HEAD,\n",
        "    HIDDEN_DIM,\n",
        "    N_LAYERS,\n",
        "    N_FC_LAYERS,\n",
        "    OUTPUT_DIM,\n",
        "    DROPOUT,\n",
        "    PAD_IDX).to(device)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5o676I3NJ4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3ad3c21-f37b-44b1-b7e0-85d8a51367a3"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 2,114,651 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3XtRyT5Ed4p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "54e5a10b-ae4f-494c-ae38-d31fd7651999"
      },
      "source": [
        "def init_weights(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)\n",
        "\n",
        "# def init_weights(m):\n",
        "#     for name, param in m.named_parameters():\n",
        "#         nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
        "# model.apply(init_weights)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerModel(\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (embedding): Embedding(12193, 128, padding_idx=1)\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=200, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=200, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (1): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=200, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=200, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "      (2): TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=200, bias=True)\n",
              "        (dropout): Dropout(p=0.3, inplace=False)\n",
              "        (linear2): Linear(in_features=200, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.3, inplace=False)\n",
              "        (dropout2): Dropout(p=0.3, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fcs): ModuleList(\n",
              "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "  )\n",
              "  (layer_norms): ModuleList(\n",
              "    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (out): Linear(in_features=256, out_features=3, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xU-1_ufImzd",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer & Loss Criterion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_CBFECq5ELz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hj3DBlXIsem",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsiQ8E-c58nJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7BmVMOpIu12",
        "colab_type": "text"
      },
      "source": [
        "## Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvZlF5IL6CGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        prem = batch.premise\n",
        "        hypo = batch.hypothesis\n",
        "        labels = batch.label\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(prem, hypo)\n",
        "        \n",
        "        # predictions => [batch size, output dim]\n",
        "        # labels => [batch size]\n",
        "    \n",
        "        loss = criterion(predictions, labels)            \n",
        "        acc = categorical_accuracy(predictions, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcsZeyJFIwuq",
        "colab_type": "text"
      },
      "source": [
        "## Validation Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liLiDHMQ6NJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            prem = batch.premise\n",
        "            hypo = batch.hypothesis\n",
        "            labels = batch.label\n",
        "                        \n",
        "            predictions = model(prem, hypo)\n",
        "            \n",
        "            loss = criterion(predictions, labels)\n",
        "                \n",
        "            acc = categorical_accuracy(predictions, labels)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YniTStu6P46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOIF6KqaIzPk",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejXIgRQe6R6N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f542e14-e2df-444f-8b27-0d9a3b7d316e"
      },
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 4m 32s\n",
            "\tTrain Loss: 0.905 | Train Acc: 57.45%\n",
            "\t Val. Loss: 0.802 |  Val. Acc: 64.25%\n",
            "Epoch: 02 | Epoch Time: 4m 33s\n",
            "\tTrain Loss: 0.797 | Train Acc: 64.58%\n",
            "\t Val. Loss: 0.771 |  Val. Acc: 66.86%\n",
            "Epoch: 03 | Epoch Time: 4m 33s\n",
            "\tTrain Loss: 0.772 | Train Acc: 66.05%\n",
            "\t Val. Loss: 0.776 |  Val. Acc: 66.96%\n",
            "Epoch: 04 | Epoch Time: 4m 32s\n",
            "\tTrain Loss: 0.759 | Train Acc: 66.82%\n",
            "\t Val. Loss: 0.771 |  Val. Acc: 68.07%\n",
            "Epoch: 05 | Epoch Time: 4m 32s\n",
            "\tTrain Loss: 0.749 | Train Acc: 67.39%\n",
            "\t Val. Loss: 0.766 |  Val. Acc: 68.45%\n",
            "Epoch: 06 | Epoch Time: 4m 33s\n",
            "\tTrain Loss: 0.741 | Train Acc: 67.84%\n",
            "\t Val. Loss: 0.766 |  Val. Acc: 68.75%\n",
            "Epoch: 07 | Epoch Time: 4m 33s\n",
            "\tTrain Loss: 0.734 | Train Acc: 68.15%\n",
            "\t Val. Loss: 0.775 |  Val. Acc: 69.00%\n",
            "Epoch: 08 | Epoch Time: 4m 33s\n",
            "\tTrain Loss: 0.729 | Train Acc: 68.49%\n",
            "\t Val. Loss: 0.780 |  Val. Acc: 68.98%\n",
            "Epoch: 09 | Epoch Time: 4m 33s\n",
            "\tTrain Loss: 0.724 | Train Acc: 68.74%\n",
            "\t Val. Loss: 0.765 |  Val. Acc: 69.01%\n",
            "Epoch: 10 | Epoch Time: 4m 34s\n",
            "\tTrain Loss: 0.721 | Train Acc: 68.89%\n",
            "\t Val. Loss: 0.763 |  Val. Acc: 68.84%\n",
            "Epoch: 11 | Epoch Time: 4m 33s\n",
            "\tTrain Loss: 0.716 | Train Acc: 69.14%\n",
            "\t Val. Loss: 0.752 |  Val. Acc: 69.63%\n",
            "Epoch: 12 | Epoch Time: 4m 33s\n",
            "\tTrain Loss: 0.714 | Train Acc: 69.24%\n",
            "\t Val. Loss: 0.774 |  Val. Acc: 69.46%\n",
            "Epoch: 13 | Epoch Time: 4m 33s\n",
            "\tTrain Loss: 0.711 | Train Acc: 69.44%\n",
            "\t Val. Loss: 0.789 |  Val. Acc: 69.52%\n",
            "Epoch: 14 | Epoch Time: 4m 33s\n",
            "\tTrain Loss: 0.708 | Train Acc: 69.64%\n",
            "\t Val. Loss: 0.775 |  Val. Acc: 69.90%\n",
            "Epoch: 15 | Epoch Time: 4m 33s\n",
            "\tTrain Loss: 0.707 | Train Acc: 69.64%\n",
            "\t Val. Loss: 0.790 |  Val. Acc: 70.01%\n",
            "Epoch: 16 | Epoch Time: 4m 33s\n",
            "\tTrain Loss: 0.705 | Train Acc: 69.74%\n",
            "\t Val. Loss: 0.778 |  Val. Acc: 69.54%\n",
            "Epoch: 17 | Epoch Time: 4m 33s\n",
            "\tTrain Loss: 0.703 | Train Acc: 69.81%\n",
            "\t Val. Loss: 0.784 |  Val. Acc: 70.16%\n",
            "Epoch: 18 | Epoch Time: 4m 33s\n",
            "\tTrain Loss: 0.701 | Train Acc: 69.99%\n",
            "\t Val. Loss: 0.771 |  Val. Acc: 70.36%\n",
            "Epoch: 19 | Epoch Time: 4m 33s\n",
            "\tTrain Loss: 0.699 | Train Acc: 70.07%\n",
            "\t Val. Loss: 0.753 |  Val. Acc: 70.16%\n",
            "Epoch: 20 | Epoch Time: 4m 33s\n",
            "\tTrain Loss: 0.698 | Train Acc: 70.13%\n",
            "\t Val. Loss: 0.766 |  Val. Acc: 69.71%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N47RTdF6I1Mh",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrF2ajUt6WwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "db0b2d5c-5f53-4324-a9bc-c6766d17164f"
      },
      "source": [
        "model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.746 |  Test Acc: 69.61%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-uaXBsdI3f4",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njr-MMuL6ZRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference(premise, hypothesis, text_field, label_field, model, device):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    if isinstance(premise, str):\n",
        "        premise = text_field.tokenize(premise)\n",
        "    \n",
        "    if isinstance(hypothesis, str):\n",
        "        hypothesis = text_field.tokenize(hypothesis)\n",
        "    \n",
        "    if text_field.lower:\n",
        "        premise = [t.lower() for t in premise]\n",
        "        hypothesis = [t.lower() for t in hypothesis]\n",
        "\n",
        "    # numericalize  \n",
        "    premise = [text_field.vocab.stoi[t] for t in premise]\n",
        "    hypothesis = [text_field.vocab.stoi[t] for t in hypothesis]\n",
        "    \n",
        "    # convert into tensors\n",
        "    premise = torch.LongTensor(premise).unsqueeze(1).to(device)\n",
        "    # premise => [prem_len, 1]\n",
        "    hypothesis = torch.LongTensor(hypothesis).unsqueeze(1).to(device)\n",
        "    # hypothesis => [hypo_len, 1]\n",
        "\n",
        "    prediction = model(premise, hypothesis)\n",
        "    prediction = prediction.argmax(dim=-1).item()\n",
        "\n",
        "    return label_field.vocab.itos[prediction]"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdvGdOs86uSb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "34e66e19-fac0-4eb6-d521-f6b85e5f21f2"
      },
      "source": [
        "premise = 'A woman selling bamboo sticks talking to two men on a loading dock.'\n",
        "hypothesis = 'There are at least three people on a loading dock.'\n",
        "\n",
        "inference(premise, hypothesis, TEXT, LABEL, model, device)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'entailment'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x0aRzqWGyy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d2f654f0-74a6-4cd0-c21c-f7394055dc00"
      },
      "source": [
        "premise = 'A woman selling bamboo sticks talking to two men on a loading dock.'\n",
        "hypothesis = 'A woman is selling bamboo sticks to help provide for her family.'\n",
        "\n",
        "inference(premise, hypothesis, TEXT, LABEL, model, device)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'neutral'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8FhxZwvGzXO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f3631bfe-da56-4df9-a7c0-00a228bd5c16"
      },
      "source": [
        "premise = 'A woman selling bamboo sticks talking to two men on a loading dock.'\n",
        "hypothesis = ' A woman is not taking money for any of her sticks.'\n",
        "\n",
        "inference(premise, hypothesis, TEXT, LABEL, model, device)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'contradiction'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47ZgmuCqv-p_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}