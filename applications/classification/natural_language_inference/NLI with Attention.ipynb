{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLI with Attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNDBrYVWwmiWZCHV1aTf0IJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/100-Days-of-NLP/blob/applications%2Fclassification/applications/classification/natural_language_inference/NLI%20with%20Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bChD0BpGIMwF",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Inference\n",
        "\n",
        "The goal of natural language inference (NLI), a widely-studied natural language processing task, is to determine if one given statement (a premise) semantically entails another given statement (a hypothesis).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXRju3STIRdA",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br_3ppk7qdRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext import data, datasets, vocab"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7oz1RNmq5Pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6HS9eAsKH_g",
        "colab_type": "text"
      },
      "source": [
        "## Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OONKSVWnKHQg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "691ed892-5fce-42fc-80e6-61285bd29751"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb9rKAsjKu9y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "abe86482-ed56-438e-94a1-33f322d8e3f8"
      },
      "source": [
        "!unzip \"./drive/My Drive/glove.6B.zip\""
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./drive/My Drive/glove.6B.zip\n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "  inflating: glove.6B.50d.txt        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blf0ksB-Kzs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_file_path = \"./glove.6B.100d.txt\""
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5ot8J7RL8Pz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1e1d8160-36e3-4643-e81d-68f1c27b2c6e"
      },
      "source": [
        "vectors = vocab.Vectors(glove_file_path, unk_init = torch.Tensor.normal_)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 397436/400001 [00:15<00:00, 24832.19it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j52oeQQcITPj",
        "colab_type": "text"
      },
      "source": [
        "## Fields"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obkiW8JTq8mA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize = 'spacy', lower = True, include_lengths=True)\n",
        "LABEL = data.LabelField()"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohmZRkVrIUpf",
        "colab_type": "text"
      },
      "source": [
        "## SNLI (Stanford Natural Language Inference) Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp_5khtprDGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data, test_data = datasets.SNLI.splits(TEXT, LABEL)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izph38KHrEvG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3c49d8dc-3716-47b9-bbd8-2cb653b4a41c"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 549367\n",
            "Number of validation examples: 9842\n",
            "Number of testing examples: 9824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4GRwSiMrGtd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "d7f8eb4e-6f29-4184-803e-a1cb6eed9737"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'premise': ['a', 'person', 'on', 'a', 'horse', 'jumps', 'over', 'a', 'broken', 'down', 'airplane', '.'], 'hypothesis': ['a', 'person', 'is', 'training', 'his', 'horse', 'for', 'a', 'competition', '.'], 'label': 'neutral'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8d5lFWxIc7H",
        "colab_type": "text"
      },
      "source": [
        "## Building Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4rIP6hRrIgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MIN_FREQ = 10\n",
        "\n",
        "TEXT.build_vocab(train_data, min_freq = MIN_FREQ, vectors=vectors)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pStFCZ0rNFK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9191100a-538e-430c-8b13-cba87f04d1b9"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 12193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXNLkVCtrO5M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b3580b1c-1e3f-401e-d409-a82913b9bb80"
      },
      "source": [
        "print(LABEL.vocab.itos)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['entailment', 'contradiction', 'neutral']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c64xop44rR1E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "55ccf926-88ad-47ff-daab-b7776e8b83f4"
      },
      "source": [
        "print(LABEL.vocab.freqs.most_common())"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('entailment', 183416), ('contradiction', 183187), ('neutral', 182764)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLUqRgO3IgAq",
        "colab_type": "text"
      },
      "source": [
        "## Data Iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMDCqErSrT58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key=lambda x: len(x.premise),\n",
        "    sort_within_batch=True,\n",
        "    device = device)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-p90c-w2o9I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "0e202df6-88fc-4a39-9d46-d37523579a98"
      },
      "source": [
        "# sample check\n",
        "sample = next(iter(valid_iterator))\n",
        "prem, prem_lengths = sample.premise\n",
        "hypo, hypo_lengths = sample.hypothesis\n",
        "print(prem.shape, prem_lengths.shape)\n",
        "print(hypo.shape, hypo_lengths.shape)\n",
        "print(sample.label.shape)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([6, 128]) torch.Size([128])\n",
            "torch.Size([14, 128]) torch.Size([128])\n",
            "torch.Size([128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcA_Q5EcF2_k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "dee2dc10-59c2-488c-bdc6-099c0e796058"
      },
      "source": [
        "print(prem_lengths[:100])\n",
        "print(hypo_lengths[:100])"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
            "        6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
            "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4], device='cuda:0')\n",
            "tensor([10, 10,  8,  6,  4, 11,  7, 14,  9,  8,  8, 10,  7,  7,  7,  5,  8, 12,\n",
            "         5,  7,  5,  4,  5,  8, 11,  8,  8,  9,  6,  7,  8,  7,  5,  6,  6,  5,\n",
            "         9,  8,  7, 10, 12,  4,  7,  7,  5,  5, 11,  6,  7,  7,  7,  7, 10,  4,\n",
            "         6,  4,  5,  5, 10,  7,  8,  6,  6,  5,  5,  6,  3,  3,  7, 10,  7,  8,\n",
            "         7,  5,  7,  7, 13,  7,  7,  6,  8,  9, 13,  6,  5,  8,  9,  9,  8,  5,\n",
            "         8,  9,  5,  4,  2, 11,  5, 11,  6,  5], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nHooPDMIhuS",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "![](https://drive.google.com/uc?id=1vc_Bg0WSMEBZhdNx7JdJXbhYTbaRDbke)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Wr0riQjrXow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTMWithAttention(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, n_linear_layers, output_dim, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        self.pad_idx = pad_idx\n",
        "    \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, bidirectional=True, dropout=dropout)\n",
        "\n",
        "        self.attn = nn.Linear(hid_dim * 2, hid_dim * 2)\n",
        "        self.v = nn.Linear(hid_dim * 2, 1, bias=False)\n",
        "\n",
        "        d_model = hid_dim * 4\n",
        "        self.fcs = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(n_linear_layers)])\n",
        "        self.layer_norms = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(n_linear_layers)])\n",
        "        \n",
        "        self.out = nn.Linear(d_model, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def create_mask(self, seq):\n",
        "        # seq => [seq_len, batch_size]\n",
        "        \n",
        "        mask = (seq != self.pad_idx)\n",
        "        mask = mask.permute(1, 0)\n",
        "        # mask => [batch_size, seq_len]\n",
        "\n",
        "    def forward(self, premise, hypothesis, premise_lengths, hypothesis_lengths):\n",
        "        # premise => [prem_seq_len, batch_size]\n",
        "        # hypothesis => [hypo_seq_len, batch_size]\n",
        "        # premise_lengths => [batch_size]\n",
        "        # hypothesis_lengths => [batch_size]\n",
        "\n",
        "        # create input masks\n",
        "        prem_mask = self.create_mask(premise)\n",
        "        # prem_mask => [batch_size, prem_seq_len]\n",
        "        hypo_mask = self.create_mask(hypothesis)\n",
        "        # hypo_mask => [batch_size, hypo_seq_len]\n",
        "\n",
        "        embedded_prem = self.dropout(self.embedding(premise))\n",
        "        # embedded_prem => [prem_seq_len, batch_size, emb_dim]\n",
        "\n",
        "        embedded_hypo = self.dropout(self.embedding(hypothesis))\n",
        "        # embedded_hypo => [hypo_seq_len, batch_size, emb_dim]\n",
        "        \n",
        "        packed_prem = nn.utils.rnn.pack_padded_sequence(embedded_prem, premise_lengths)\n",
        "        packed_out_prem, (hidden_prem, cell_prem) = self.rnn(packed_prem)\n",
        "        outputs_prem, _ = nn.utils.rnn.pad_packed_sequence(packed_out_prem)\n",
        "        # outputs_prem => [prem_seq_len, batch_size, hid_dim * 2]\n",
        "        # hidden_prem, cell_prem => [n_layers * num_dir, batch_size, hidden_dim]\n",
        "\n",
        "        outputs_hypo, (hidden_hypo, cell_hypo) = self.rnn(embedded_hypo)\n",
        "        # outputs_hypo => [hypo_seq_len, batch_size, hid_dim * 2]\n",
        "        # hidden_hypo, cell_hypo => [n_layers * num_dir, batch_size, hidden_dim]\n",
        "        \n",
        "        # weighted representation through attention\n",
        "        weighted_prem = self.dropout(self.attention(outputs_prem, prem_mask))\n",
        "        weighted_hypo = self.dropout(self.attention(outputs_hypo, hypo_mask))\n",
        "        # weighted => [batch_size, hid_dim * 2]\n",
        "\n",
        "        hidden = torch.cat((weighted_prem, weighted_hypo), dim=-1)\n",
        "        # hidden => [batch_size, hidden_dim * 4]\n",
        "        #        => [batch_size, d_model]\n",
        "\n",
        "        for fc, norm in zip(self.fcs, self.layer_norms):\n",
        "            hidden_ = fc(hidden)\n",
        "            hidden_ = self.dropout(hidden_)\n",
        "            # residual connection\n",
        "            hidden = hidden + F.relu(hidden_)\n",
        "            # layer normalization\n",
        "            hidden = norm(hidden)\n",
        "        \n",
        "        logits = self.out(hidden)\n",
        "        # logits => [batch_size, output_dim]\n",
        "\n",
        "        return logits\n",
        "    \n",
        "    def attention(self, outputs, mask=None):\n",
        "        # outputs => [seq_len, batch_size, hidden_dim * 2]\n",
        "        # mask => [batch_size, seq_len]\n",
        "\n",
        "        batch_size, seq_len, _ = outputs.shape\n",
        "\n",
        "        outputs_ = outputs.permute(1, 0, 2)\n",
        "        # outputs_ => [batch_size, seq_len, hidden_dim * 2]\n",
        "\n",
        "        energy = torch.tanh(self.attn(outputs_))\n",
        "        # energy => [batch_size, seq_len, hidden_dim * 2]\n",
        "\n",
        "        attention_energy = self.v(energy).squeeze(2)\n",
        "        # attention_energy => [batch_size, seq_len]\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_energy = attention_energy.masked_fill(mask == 0, -1e10)\n",
        "            # attention_energy => [batch_size, seq_len]\n",
        "\n",
        "        scores = F.softmax(attention_energy, dim=-1)\n",
        "        # scores => [batch_size, seq_len]\n",
        "\n",
        "        scores = scores.unsqueeze(1)\n",
        "        # scores => [batch_size, 1, seq_len]\n",
        "\n",
        "        outputs = outputs.permute(1, 0, 2)\n",
        "        # outputs => [batch_size, seq_len, hidden_dim * 2]\n",
        "\n",
        "        weighted = torch.bmm(scores, outputs)\n",
        "        # weighted => [batch_size, 1, hidden_dim * 2]\n",
        "\n",
        "        weighted = weighted.squeeze(1)\n",
        "        # weighted => [batch_size, hidden_dim * 2]\n",
        "\n",
        "        return weighted"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAVQerriyVJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 200\n",
        "N_LSTM_LAYERS = 2\n",
        "N_FC_LAYERS = 3\n",
        "OUTPUT_DIM = len(LABEL.vocab)\n",
        "DROPOUT = 0.3\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = BiLSTMWithAttention(\n",
        "    INPUT_DIM,\n",
        "    EMBEDDING_DIM,\n",
        "    HIDDEN_DIM,\n",
        "    N_LSTM_LAYERS,\n",
        "    N_FC_LAYERS,\n",
        "    OUTPUT_DIM,\n",
        "    DROPOUT,\n",
        "    PAD_IDX).to(device)"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5o676I3NJ4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9377ac44-3ce5-4f7d-c2d0-1d28842046b2"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 4,756,103 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvinSJ0OM8gX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "384185be-c904-46e6-925f-70342de574cd"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([12193, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9twua_rJM-ee",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "2e65aee3-9f6c-4724-b749-c58f338c27db"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2837, -0.6263, -0.4435,  ...,  0.4368, -0.8261, -0.1570],\n",
              "        [ 1.4773,  1.2373, -0.3034,  ..., -1.5434,  0.0221, -0.3314],\n",
              "        [-0.2709,  0.0440, -0.0203,  ..., -0.4923,  0.6369,  0.2364],\n",
              "        ...,\n",
              "        [-0.3962, -0.0070,  0.4369,  ..., -0.3295,  0.1764,  0.0092],\n",
              "        [ 0.0882, -0.3188,  0.4663,  ...,  0.8881,  0.5180, -0.1170],\n",
              "        [-0.6644, -0.3045,  0.6151,  ...,  0.1404,  0.5788, -0.0333]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULf61Z2CNDVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgr2WcePNeqA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "592fbe8d-fb86-438e-c3fb-b04e352e2c6e"
      },
      "source": [
        "print(model.embedding.weight.data)"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.2837, -0.6263, -0.4435,  ...,  0.4368, -0.8261, -0.1570],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.2709,  0.0440, -0.0203,  ..., -0.4923,  0.6369,  0.2364],\n",
            "        ...,\n",
            "        [-0.3962, -0.0070,  0.4369,  ..., -0.3295,  0.1764,  0.0092],\n",
            "        [ 0.0882, -0.3188,  0.4663,  ...,  0.8881,  0.5180, -0.1170],\n",
            "        [-0.6644, -0.3045,  0.6151,  ...,  0.1404,  0.5788, -0.0333]],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLpFEhJdTWJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "1f10fd16-38b0-4c83-a8d4-4aa6659ae0b4"
      },
      "source": [
        "model"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTMWithAttention(\n",
              "  (embedding): Embedding(12193, 100, padding_idx=1)\n",
              "  (rnn): LSTM(100, 200, num_layers=2, dropout=0.3, bidirectional=True)\n",
              "  (attn): Linear(in_features=400, out_features=400, bias=True)\n",
              "  (v): Linear(in_features=400, out_features=1, bias=False)\n",
              "  (fcs): ModuleList(\n",
              "    (0): Linear(in_features=800, out_features=800, bias=True)\n",
              "    (1): Linear(in_features=800, out_features=800, bias=True)\n",
              "    (2): Linear(in_features=800, out_features=800, bias=True)\n",
              "  )\n",
              "  (layer_norms): ModuleList(\n",
              "    (0): LayerNorm((800,), eps=1e-05, elementwise_affine=True)\n",
              "    (1): LayerNorm((800,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): LayerNorm((800,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (out): Linear(in_features=800, out_features=3, bias=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xU-1_ufImzd",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer & Loss Criterion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_CBFECq5ELz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hj3DBlXIsem",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsiQ8E-c58nJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def categorical_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    correct = max_preds.squeeze(1).eq(y)\n",
        "    return correct.sum() / torch.FloatTensor([y.shape[0]])"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7BmVMOpIu12",
        "colab_type": "text"
      },
      "source": [
        "## Train Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvZlF5IL6CGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        prem, prem_lengths = batch.premise\n",
        "        hypo, hypo_lengths = batch.hypothesis\n",
        "        labels = batch.label\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(prem, hypo, prem_lengths, hypo_lengths)\n",
        "        \n",
        "        # predictions => [batch size, output dim]\n",
        "        # labels => [batch size]\n",
        "    \n",
        "        loss = criterion(predictions, labels)            \n",
        "        acc = categorical_accuracy(predictions, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcsZeyJFIwuq",
        "colab_type": "text"
      },
      "source": [
        "## Validation Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liLiDHMQ6NJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            prem, prem_lengths = batch.premise\n",
        "            hypo, hypo_lengths = batch.hypothesis\n",
        "            labels = batch.label\n",
        "                        \n",
        "            predictions = model(prem, hypo, prem_lengths, hypo_lengths)\n",
        "            \n",
        "            loss = criterion(predictions, labels)\n",
        "                \n",
        "            acc = categorical_accuracy(predictions, labels)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YniTStu6P46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOIF6KqaIzPk",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejXIgRQe6R6N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "db1d8bc3-f6a1-4ca1-f033-c841843b3c89"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 1m 55s\n",
            "\tTrain Loss: 0.804 | Train Acc: 63.64%\n",
            "\t Val. Loss: 0.666 |  Val. Acc: 72.31%\n",
            "Epoch: 02 | Epoch Time: 1m 55s\n",
            "\tTrain Loss: 0.682 | Train Acc: 71.00%\n",
            "\t Val. Loss: 0.596 |  Val. Acc: 75.56%\n",
            "Epoch: 03 | Epoch Time: 1m 55s\n",
            "\tTrain Loss: 0.618 | Train Acc: 74.49%\n",
            "\t Val. Loss: 0.553 |  Val. Acc: 77.66%\n",
            "Epoch: 04 | Epoch Time: 1m 55s\n",
            "\tTrain Loss: 0.577 | Train Acc: 76.48%\n",
            "\t Val. Loss: 0.546 |  Val. Acc: 78.58%\n",
            "Epoch: 05 | Epoch Time: 1m 55s\n",
            "\tTrain Loss: 0.549 | Train Acc: 77.75%\n",
            "\t Val. Loss: 0.542 |  Val. Acc: 79.12%\n",
            "Epoch: 06 | Epoch Time: 1m 55s\n",
            "\tTrain Loss: 0.528 | Train Acc: 78.77%\n",
            "\t Val. Loss: 0.524 |  Val. Acc: 79.77%\n",
            "Epoch: 07 | Epoch Time: 1m 55s\n",
            "\tTrain Loss: 0.511 | Train Acc: 79.57%\n",
            "\t Val. Loss: 0.530 |  Val. Acc: 80.44%\n",
            "Epoch: 08 | Epoch Time: 1m 55s\n",
            "\tTrain Loss: 0.497 | Train Acc: 80.22%\n",
            "\t Val. Loss: 0.531 |  Val. Acc: 80.42%\n",
            "Epoch: 09 | Epoch Time: 1m 55s\n",
            "\tTrain Loss: 0.485 | Train Acc: 80.74%\n",
            "\t Val. Loss: 0.525 |  Val. Acc: 80.73%\n",
            "Epoch: 10 | Epoch Time: 1m 55s\n",
            "\tTrain Loss: 0.475 | Train Acc: 81.22%\n",
            "\t Val. Loss: 0.525 |  Val. Acc: 80.69%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N47RTdF6I1Mh",
        "colab_type": "text"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrF2ajUt6WwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dea937e1-1905-4a96-897c-34c0bcc5bd03"
      },
      "source": [
        "model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.532 |  Test Acc: 79.51%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-uaXBsdI3f4",
        "colab_type": "text"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njr-MMuL6ZRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference(premise, hypothesis, text_field, label_field, model, device):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    if isinstance(premise, str):\n",
        "        premise = text_field.tokenize(premise)\n",
        "    \n",
        "    if isinstance(hypothesis, str):\n",
        "        hypothesis = text_field.tokenize(hypothesis)\n",
        "    \n",
        "    if text_field.lower:\n",
        "        premise = [t.lower() for t in premise]\n",
        "        hypothesis = [t.lower() for t in hypothesis]\n",
        "\n",
        "    # numericalize  \n",
        "    premise = [text_field.vocab.stoi[t] for t in premise]\n",
        "    premise_lengths = [len(premise)]\n",
        "    hypothesis = [text_field.vocab.stoi[t] for t in hypothesis]\n",
        "    hypothesis_lengths = [len(hypothesis)]\n",
        "    \n",
        "    # convert into tensors\n",
        "    premise = torch.LongTensor(premise).unsqueeze(1).to(device)\n",
        "    # premise => [prem_len, 1]\n",
        "    premise_lengths = torch.LongTensor(premise_lengths).to(device)\n",
        "    # premise_lengths => [1]\n",
        "    hypothesis = torch.LongTensor(hypothesis).unsqueeze(1).to(device)\n",
        "    # hypothesis => [hypo_len, 1]\n",
        "    hypothesis_lengths = torch.LongTensor(hypothesis_lengths).to(device)\n",
        "    # hypothesis_lengths => [1]\n",
        "\n",
        "    prediction = model(premise, hypothesis, premise_lengths, hypothesis_lengths)\n",
        "    prediction = prediction.argmax(dim=-1).item()\n",
        "\n",
        "    return label_field.vocab.itos[prediction]"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdvGdOs86uSb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "039738db-b868-40fc-8511-ad8ee3bb710a"
      },
      "source": [
        "premise = 'A woman selling bamboo sticks talking to two men on a loading dock.'\n",
        "hypothesis = 'There are at least three people on a loading dock.'\n",
        "\n",
        "inference(premise, hypothesis, TEXT, LABEL, model, device)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'entailment'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x0aRzqWGyy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "abc32af1-e743-4c3c-f787-429f5b5ae28d"
      },
      "source": [
        "premise = 'A woman selling bamboo sticks talking to two men on a loading dock.'\n",
        "hypothesis = 'A woman is selling bamboo sticks to help provide for her family.'\n",
        "\n",
        "inference(premise, hypothesis, TEXT, LABEL, model, device)"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'neutral'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8FhxZwvGzXO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "751e2b29-a750-4fd0-d3f5-8cad5162d58d"
      },
      "source": [
        "premise = 'A woman selling bamboo sticks talking to two men on a loading dock.'\n",
        "hypothesis = ' A woman is not taking money for any of her sticks.'\n",
        "\n",
        "inference(premise, hypothesis, TEXT, LABEL, model, device)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'contradiction'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    }
  ]
}