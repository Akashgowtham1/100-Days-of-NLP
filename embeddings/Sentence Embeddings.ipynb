{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence Embeddings",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhUVuWZexbd2kvmo0xdm1o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/100-Days-of-NLP/blob/embeddings/embeddings/Sentence%20Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QfE2Hubv5_f",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYvOyf1Xah7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 815
        },
        "outputId": "c68cfd50-9332-474d-db63-01b9866b70ce"
      },
      "source": [
        "!pip install sentence-transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/46/b7d6c37d92d1bd65319220beabe4df845434930e3f30e42d3cfaecb74dc4/sentence-transformers-0.2.6.1.tar.gz (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 1.8MB/s \n",
            "\u001b[?25hCollecting transformers>=2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 25.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 31.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 32.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.8.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.1->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.15.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.8.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.8.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.8.0->sentence-transformers) (2.4.7)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.2.6.1-cp36-none-any.whl size=74031 sha256=e5bf9d335e923f3325f32bccfae36770243ce1a8b9315f7c31f1da8fa04a6583\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/fa/17/2b081a8cd8b0a86753fb0e9826b3cc19f0207062c0b2da7008\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=6c55b71f89365e3ad5c6dbcb010045bc11e2b084f99d55ec037c92a46ba6557e\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.2.6.1 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlnpwy2yv9BI",
        "colab_type": "text"
      },
      "source": [
        "## Sentence Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89nHdLOCvaDQ",
        "colab_type": "text"
      },
      "source": [
        "BERT has set a new state-of-the-art\n",
        "performance on sentence-pair regression tasks\n",
        "like semantic textual similarity (STS). However, it requires that both sentences are fed\n",
        "into the network, which causes a massive computational overhead.\n",
        "\n",
        "Finding in a collection of n = 10 000 sentences the pair with the highest similarity requires with BERT $n*(n−1) / 2$ = 49 995 000 inference computations. On a modern V100 GPU, this requires about 65 hours. Similar, finding which of the over 40 million existent questions of Quora is the most similar for a new question could be modeled as a pair-wise comparison with BERT, however, answering a single query would require over 50 hours.\n",
        "\n",
        "The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks\n",
        "like clustering.\n",
        "\n",
        "A common method to address clustering and semantic search is to map each sentence to a vector space such that semantically similar sentences are close. Researchers have started to input individual sentences into BERT and to derive fixedsize sentence embeddings. The most commonly used approach is to average the BERT output layer (known as BERT embeddings) or by using the output of the first token (the [CLS] token). This common practice yields rather bad\n",
        "sentence embeddings, often worse than averaging\n",
        "GloVe embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk8H5pI_wfKg",
        "colab_type": "text"
      },
      "source": [
        "To alleviate this issue, a new architecture called **SBERT** was used. The siamese network architecture enables that\n",
        "fixed-sized vectors for input sentences can be derived. Using a similarity measure like cosinesimilarity or Manhatten / Euclidean distance, semantically similar sentences can be found. These\n",
        "similarity measures can be performed extremely\n",
        "efficient on modern hardware, allowing SBERT\n",
        "to be used for semantic similarity search as well\n",
        "as for clustering.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1TH-juL431ykPCe_415ezcwf4Vo_KVGw1)\n",
        "\n",
        "\n",
        "The complexity for finding the most similar sentence pair in a collection of 10,000 sentences is reduced from 65 hours with BERT to the computation of 10,000 sentence embeddings\n",
        "(\\~5 seconds with SBERT) and computing cosinesimilarity (~0.01 seconds). By using optimized index structures, finding the most similar Quora question can be reduced from 50 hours to a few milliseconds.\n",
        "\n",
        "- [Sentence BERT Paper](https://arxiv.org/pdf/1908.10084.pdf)\n",
        "- [Sentence Transformers Repo](https://github.com/UKPLab/sentence-transformers)\n",
        "\n",
        "There are many ways in which sentence-transformers could be used. With pre-trained models, training on custom dataset, with LSTM, CNN, Word Embeddings (Glove) etc. We will explore a few of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chEcpbOIy-zq",
        "colab_type": "text"
      },
      "source": [
        "### Using Pre-trained Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRE5Hqlb14fn",
        "colab_type": "text"
      },
      "source": [
        "There are many pre-trained models available. We can use our own custom trained model also. Some of the pre-trained models available are:\n",
        "\n",
        "Natural Language Inference (NLI): Given two sentences, the model should classify if these two sentence entail, contradict, or are neutral to each other. \n",
        "\n",
        "These models were trained on SNLI and MultiNLI dataset to create universal sentence embeddings.\n",
        "\n",
        "- **`bert-base-nli-mean-tokens`**: BERT-base model with mean-tokens pooling. Performance: STSbenchmark: 77.12\n",
        "- **`bert-large-nli-mean-tokens`**: BERT-large with mean-tokens pooling. Performance: STSbenchmark: 79.19\n",
        "- **`roberta-base-nli-mean-tokens`**: RoBERTa-base with mean-tokens pooling. Performance: STSbenchmark: 77.49\n",
        "- **`roberta-large-nli-mean-tokens`**: RoBERTa-base with mean-tokens pooling. Performance: STSbenchmark: 78.69\n",
        "- **`distilbert-base-nli-mean-tokens`**: DistilBERT-base with mean-tokens pooling. Performance: STSbenchmark: 76.97\n",
        "\n",
        "These models were first fine-tuned on the AllNLI datasent, then on train set of STS benchmark. They are specifically well suited for semantic textual similarity.\n",
        "\n",
        "- **`bert-base-nli-stsb-mean-tokens`**: Performance: STSbenchmark: 85.14\n",
        "- **`bert-large-nli-stsb-mean-tokens`**: Performance: STSbenchmark: 85.29\n",
        "- **`roberta-base-nli-stsb-mean-tokens`**: Performance: STSbenchmark: 85.44\n",
        "- **`roberta-large-nli-stsb-mean-tokens`**: Performance: STSbenchmark: 86.39\n",
        "- **`distilbert-base-nli-stsb-mean-tokens`**: Performance: STSbenchmark: 84.38\n",
        "\n",
        "For more information refer to this [link](https://github.com/UKPLab/sentence-transformers#english-pre-trained-models)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnWR6IxoueHW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "917142c0-350f-4c8a-d12f-4cc1e4007bec"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405M/405M [00:55<00:00, 7.29MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YAQw4L_0n4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'This is a hot sunny day', \n",
        "    'Machine learning is awesome. It is necessary for each and everyone to learn about it.']\n",
        "sentence_embeddings = model.encode(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KNGWr990t4J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "2992a651-fa65-424a-fd57-c4abdc09ee14"
      },
      "source": [
        "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding.shape)\n",
        "    print(\"\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: This framework generates embeddings for each input sentence\n",
            "Embedding: (768,)\n",
            "\n",
            "Sentence: This is a hot sunny day\n",
            "Embedding: (768,)\n",
            "\n",
            "Sentence: Machine learning is awesome. It is necessary for each and everyone to learn about it.\n",
            "Embedding: (768,)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h99C0VQVAdvc",
        "colab_type": "text"
      },
      "source": [
        "### Training the SBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaJGDgOMAj9d",
        "colab_type": "text"
      },
      "source": [
        "First, the BERT model (instantiated from bert-base-uncased) to map tokens in a sentence to the output embeddings from BERT is used. The next layer of the model is a Pooling model: In this case, mean-pooling is used. You can also perform max-pooling or use the embedding from the CLS token. You can also combine multiple poolings together.\n",
        "\n",
        "These two modules (word_embedding_model and pooling_model) form our SentenceTransformer. Each sentence is now passed first through the word_embedding_model and then through the pooling_model to give fixed sized sentence vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRCBtqmG1FEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code won't run. It is for sample purpose to show the flow of training\n",
        "# For actual running code, refer here: https://github.com/UKPLab/sentence-transformers#training\n",
        "nli_reader = NLIDataReader('datasets/AllNLI')\n",
        "\n",
        "train_data = SentencesDataset(nli_reader.get_examples('train.gz'), model=model)\n",
        "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "train_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=train_num_labels)\n",
        "\n",
        "# evaluation on different dataset\n",
        "sts_reader = STSBenchmarkDataReader('datasets/stsbenchmark')\n",
        "dev_data = SentencesDataset(examples=sts_reader.get_examples('sts-dev.csv'), model=model)\n",
        "dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=train_batch_size)\n",
        "evaluator = EmbeddingSimilarityEvaluator(dev_dataloader)\n",
        "\n",
        "# training pipeline\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "         evaluator=evaluator,\n",
        "         epochs=num_epochs,\n",
        "         evaluation_steps=1000,\n",
        "         warmup_steps=warmup_steps,\n",
        "         output_path=model_save_path\n",
        "         )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z47TZpWrCxRG",
        "colab_type": "text"
      },
      "source": [
        "### Using Custom BERT models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPYaWaTdCamD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use BERT for mapping tokens to embeddings\n",
        "word_embedding_model = models.Transformer('path/to/your/BERT/model')\n",
        "\n",
        "# Apply mean pooling to get one fixed sized sentence vector\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "                               pooling_mode_mean_tokens=True,\n",
        "                               pooling_mode_cls_token=False,\n",
        "                               pooling_mode_max_tokens=False)\n",
        "\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9AACVG8FUA4",
        "colab_type": "text"
      },
      "source": [
        "### Using Glove embeddings instead of BERT for tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRTet0fnFcXN",
        "colab_type": "text"
      },
      "source": [
        "We need to change the word_embedding_model. Instead of BERT, we have to specify the glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x-B5_eKDQHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Map tokens to traditional word embeddings like GloVe\n",
        "word_embedding_model = models.WordEmbeddings.from_text_file('glove.6B.300d.txt.gz')\n",
        "\n",
        "# Apply mean pooling to get one fixed sized sentence vector\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "                               pooling_mode_mean_tokens=True,\n",
        "                               pooling_mode_cls_token=False,\n",
        "                               pooling_mode_max_tokens=False)\n",
        "\n",
        "# Add two trainable feed-forward networks (DAN)\n",
        "sent_embeddings_dimension = pooling_model.get_sentence_embedding_dimension()\n",
        "dan1 = models.Dense(in_features=sent_embeddings_dimension, out_features=sent_embeddings_dimension)\n",
        "dan2 = models.Dense(in_features=sent_embeddings_dimension, out_features=sent_embeddings_dimension)\n",
        "\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dan1, dan2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsH6naKZFw3r",
        "colab_type": "text"
      },
      "source": [
        "### Using LSTM model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-riuZJxF8t9",
        "colab_type": "text"
      },
      "source": [
        "We just need to add LSTM model before the pooling layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWx4xsZdFviX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Map tokens to traditional word embeddings like GloVe\n",
        "word_embedding_model = models.WordEmbeddings.from_text_file('glove.6B.300d.txt.gz')\n",
        "\n",
        "lstm = models.LSTM(word_embedding_dimension=word_embedding_model.get_word_embedding_dimension(), hidden_dim=1024)\n",
        "\n",
        "# Apply mean pooling to get one fixed sized sentence vector\n",
        "pooling_model = models.Pooling(lstm.get_word_embedding_dimension(),\n",
        "                               pooling_mode_mean_tokens=False,\n",
        "                               pooling_mode_cls_token=False,\n",
        "                               pooling_mode_max_tokens=True)\n",
        "\n",
        "\n",
        "model = SentenceTransformer(modules=[word_embedding_model, lstm, pooling_model])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTLM8flGGCSj",
        "colab_type": "text"
      },
      "source": [
        "### Using CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbOfQiQ2GIIe",
        "colab_type": "text"
      },
      "source": [
        "Similar to the above configuration. Replacing LSTM model with CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MD_pcOePGEDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Map tokens to vectors using BERT\n",
        "word_embedding_model = models.BERT('bert-base-uncased')\n",
        "\n",
        "cnn = models.CNN(in_word_embedding_dimension=word_embedding_model.get_word_embedding_dimension(), out_channels=256, kernel_sizes=[1,3,5])\n",
        "\n",
        "# Apply mean pooling to get one fixed sized sentence vector\n",
        "pooling_model = models.Pooling(cnn.get_word_embedding_dimension(),\n",
        "                               pooling_mode_mean_tokens=True,\n",
        "                               pooling_mode_cls_token=False,\n",
        "                               pooling_mode_max_tokens=False)\n",
        "\n",
        "\n",
        "model = SentenceTransformer(modules=[word_embedding_model, cnn, pooling_model])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Vm7hlYwGT6g",
        "colab_type": "text"
      },
      "source": [
        "**NOTE**: All the above mentioned snippets are taken from [sentence-transformers](https://github.com/UKPLab/sentence-transformers/)  repository. Please refer to that if you need more details."
      ]
    }
  ]
}