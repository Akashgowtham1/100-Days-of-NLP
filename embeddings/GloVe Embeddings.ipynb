{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVe Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ufgvJjM5e_cV",
        "q6R-3WaKfvhV",
        "JTIXSJBjj9fO",
        "uPVrbfguE_50"
      ],
      "authorship_tag": "ABX9TyM+PnVa4AiATtTc61AKTHJx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graviraja/100-Days-of-NLP/blob/embeddings/embeddings/GloVe%20Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufgvJjM5e_cV",
        "colab_type": "text"
      },
      "source": [
        "# GloVe Embeddings\n",
        "\n",
        "GloVe is another commonly used method of obtaining pre-trained embeddings. GloVe aims to achieve two goals:\n",
        "\n",
        "- Create word vectors that capture meaning in vector space\n",
        "- Takes advantage of global count statistics instead of only local information\n",
        "\n",
        "There are a lot of online material available to explain the concept about GloVe. So my focus here will be on, how to use pre-trained Glove word embeddings. I will provide relevant resources to look into more details.\n",
        "\n",
        "## Resources:\n",
        "\n",
        "- [Glove Paper Explaination](https://mlexplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/)\n",
        "- [Colyer blog on GloVe](https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/)\n",
        "- [Code and Pretrained Embeddings](https://nlp.stanford.edu/projects/glove/)\n",
        "- [Stanford Lecture](https://www.youtube.com/watch?v=ASn7ExxLZws)\n",
        "- [GloVe Paper](https://www-nlp.stanford.edu/pubs/glove.pdf) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bmyOlQePpEa",
        "colab_type": "text"
      },
      "source": [
        "## Difference between Word2Vec and GloVe\n",
        "\n",
        "**Global information:** word2vec does not have any explicit global information embedded in it by default. GloVe creates a global co-occurrence matrix by estimating the probability a given word will co-occur with other words. This presence of global information makes GloVe ideally work better. Although in a practical sense, they work almost similar and people have found similar performance with both.\n",
        "\n",
        "**Presence of Neural Networks:** GloVe does not use neural networks while word2vec does. In GloVe, the loss function is the difference between the product of word embeddings and the log of the probability of co-occurrence. We try to reduce that and use SGD but solve it as we would solve a linear regression. While in the case of word2vec, we either train the word on its context (skip-gram) or train the context on the word (continuous bag of words) using a 1-hidden layer neural network.\n",
        "\n",
        "[source 1](https://www.quora.com/How-is-GloVe-different-from-word2vec)\n",
        "\n",
        "[source 2](http://deeplearning.lipingyang.org/wp-content/uploads/2017/12/How-is-GloVe-different-from-word2vec_-Quora.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6R-3WaKfvhV",
        "colab_type": "text"
      },
      "source": [
        "## Download the pre-trained glove file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCvDfKj-xuRT",
        "colab_type": "text"
      },
      "source": [
        "I will be using glove.6B file which is trained on Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 300d vectors, 822 MB download). You can find the other files [here](https://nlp.stanford.edu/projects/glove/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY7RhZeKeMkQ",
        "colab_type": "code",
        "outputId": "4dd3baa7-0e91-4f3a-c518-6776fd477999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/wordvecs/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-18 01:13:05--  http://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/wordvecs/glove.6B.zip [following]\n",
            "--2020-05-18 01:13:05--  https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip [following]\n",
            "--2020-05-18 01:13:05--  http://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182753 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.93MB/s    in 6m 27s  \n",
            "\n",
            "2020-05-18 01:19:32 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182753/862182753]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLksNjoAj0Yr",
        "colab_type": "code",
        "outputId": "ef5d5481-981e-4200-a80d-2fcdf6df868b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.6B.zip  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTIXSJBjj9fO",
        "colab_type": "text"
      },
      "source": [
        "## Upload the Data to Google Drive (Optional)\n",
        "\n",
        "If the notebook shutdown means, the data which is present is also lost. Inorder to save the data we can sync it google drive by mounting the drive. Instead of downloading the glove file again we can simply mount the drive and get the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrIAGcC_lwst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHrMQ4xSl0XL",
        "colab_type": "code",
        "outputId": "11e19475-a171-445d-f2c0-9ed622692b85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px7gFR0umLaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run for the first time\n",
        "!mv glove.6B.zip \"/content/drive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bmO8C7LnD7r",
        "colab_type": "text"
      },
      "source": [
        "## Extract the contents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfLfbUtjmz6N",
        "colab_type": "code",
        "outputId": "d5d89ef3-2b5c-46aa-f814-cfafc5b44f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# update the path to zip file accordingly if not uploaded to drive means\n",
        "!unzip \"./drive/My Drive/glove.6B.zip\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./drive/My Drive/glove.6B.zip\n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "  inflating: glove.6B.50d.txt        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eElrAG3zI3UZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9e84a9a3-85b2-4194-93db-6412bbd10fdc"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t\t   glove.6B.200d.txt  glove.6B.50d.txt\n",
            "glove.6B.100d.txt  glove.6B.300d.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvyOXIQSqZOp",
        "colab_type": "text"
      },
      "source": [
        "I will be using **glove.6B.300d.txt**. The same logic applies for all the other versions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPVrbfguE_50",
        "colab_type": "text"
      },
      "source": [
        "# Using Gensim to load pre-trained Glove Embeddings\n",
        "\n",
        "Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning.\n",
        "\n",
        "Gensim includes streamed parallelized implementations of fastText,word2vec and doc2vec algorithms, as well as latent semantic analysis (LSA, LSI, SVD), non-negative matrix factorization (NMF), latent Dirichlet allocation (LDA), tf-idf and random projections. [source](https://en.wikipedia.org/wiki/Gensim)\n",
        "\n",
        "References:\n",
        "\n",
        "- [My code on Word2Vec](https://github.com/graviraja/100-Days-of-NLP/blob/master/embeddings/Word2Vec.ipynb)\n",
        "- [Machine Learning Mastery blog on using gensim](https://machinelearningmastery.com/develop-word-embeddings-python-gensim/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPyXmDvQFLZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jvqd4X6FHjo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_file_path = \"./glove.6B.300d.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb29ztLmFdp_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "0d800b2d-8b69-4397-e262-3c26022620e9"
      },
      "source": [
        "# converting glove file to word2vec format so that it can be loaded by gensim \n",
        "word2vec_output_file = 'glove.6B.300d.txt.word2vec'\n",
        "glove2word2vec(glove_file_path, word2vec_output_file)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400001, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1O-c6h8JLX8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b2ab72e5-ba26-41da-abbf-0844d1e90d21"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive\t\t   glove.6B.200d.txt  glove.6B.300d.txt.word2vec  sample_data\n",
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.50d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfT3VpzcFqcO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "3f7055fd-f6b1-429c-a916-daf838ce3e1b"
      },
      "source": [
        "# Note that the converted file is ASCII format, not binary, so we set binary=False when loading.\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eIVdFynGHhv",
        "colab_type": "text"
      },
      "source": [
        "## Word Similarities\n",
        "\n",
        "Here, we will see how similar are two words to each other"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLkwt67OGXrY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "8f4ef226-3ad6-4464-f0b5-6857f0100e92"
      },
      "source": [
        "print(f'Similarity between night and nights: {model.similarity(\"night\", \"nights\")}')\n",
        "print(f'Similarity between reb and blue: {model.similarity(\"red\", \"blue\")}')\n",
        "print(f'Similarity between hello and hi: {model.similarity(\"hello\", \"hi\")}')\n",
        "print(f'Similarity between king and queen: {model.similarity(\"king\", \"queen\")}')\n",
        "print(f'Similarity between london and moscow: {model.similarity(\"london\", \"moscow\")}')\n",
        "print(f'Similarity between car and bike: {model.similarity(\"car\", \"bike\")}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similarity between night and nights: 0.6768945455551147\n",
            "Similarity between reb and blue: 0.6736692786216736\n",
            "Similarity between hello and hi: 0.3302616477012634\n",
            "Similarity between king and queen: 0.6336469054222107\n",
            "Similarity between london and moscow: 0.39354825019836426\n",
            "Similarity between car and bike: 0.4672122299671173\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCFP4p6QG0P7",
        "colab_type": "text"
      },
      "source": [
        "##Most Similar Words\n",
        "\n",
        "Here, we will ask our model to find the words which are most similar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeVqySl2HQX-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "0c5aa484-20f5-4bf0-be52-119b61bba6ac"
      },
      "source": [
        "similar = model.most_similar(\"january\")\n",
        "for i in similar:\n",
        "    print(i)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('february', 0.9652106761932373)\n",
            "('december', 0.9620600938796997)\n",
            "('october', 0.9580933451652527)\n",
            "('november', 0.9528316855430603)\n",
            "('september', 0.9462947845458984)\n",
            "('august', 0.935489296913147)\n",
            "('april', 0.9315787553787231)\n",
            "('june', 0.928554356098175)\n",
            "('july', 0.9246786832809448)\n",
            "('march', 0.898531436920166)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYwhjHQZG0Nj",
        "colab_type": "text"
      },
      "source": [
        "## Odd-One-Out\n",
        "Here, we ask our model to give us the word that does not belong to the list!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnPgAjipHWUN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9db1ef45-f647-4667-f636-fbed56cf665e"
      },
      "source": [
        "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cereal\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHbX90BSHmbz",
        "colab_type": "text"
      },
      "source": [
        "## Analogy difference\n",
        "\n",
        "Which word is to women as king is to queen?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPSa9jflHorm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "2975248e-81b7-4dbb-cbc6-5d4be552944b"
      },
      "source": [
        "model.most_similar(positive=[\"women\", \"king\"], negative=[\"queen\"])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('men', 0.6591772437095642),\n",
              " ('people', 0.46714138984680176),\n",
              " ('who', 0.46234187483787537),\n",
              " ('americans', 0.4615159332752228),\n",
              " ('young', 0.45295244455337524),\n",
              " ('those', 0.4465915262699127),\n",
              " ('minorities', 0.44377851486206055),\n",
              " ('athletes', 0.42091041803359985),\n",
              " ('them', 0.4203292429447174),\n",
              " ('others', 0.4188333749771118)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGlowpFcHrHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def analogy(x1, x2, y1):\n",
        "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
        "    return result[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA3mzdhxH2vM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "af15d3dd-5338-4a9c-a7f4-be3bd80838e6"
      },
      "source": [
        "analogy('japan', 'japanese', 'china')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'chinese'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k18N5cRiLnMJ",
        "colab_type": "text"
      },
      "source": [
        "# Training GloVe Embeddings\n",
        "\n",
        "If the web datasets above don't match the semantics of your end use case, you can train word vectors on your own corpus.\n",
        "\n",
        "```code\n",
        "git clone http://github.com/stanfordnlp/glove\n",
        "cd glove && make\n",
        "./demo.sh\n",
        "```\n",
        "\n",
        "The demo.sh script downloads a small corpus, consisting of the first 100M characters of Wikipedia. It collects unigram counts, constructs and shuffles cooccurrence data, and trains a simple version of the GloVe model. It also runs a word analogy evaluation script in python to verify word vector quality. More details about training on your own corpus can be found by reading demo.sh  in [Offical Glove repo](https://github.com/stanfordnlp/GloVe)"
      ]
    }
  ]
}